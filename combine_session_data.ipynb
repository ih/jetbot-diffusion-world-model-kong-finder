{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4237926-8e41-4558-bb17-545bda787083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff98c00c-c30c-43c9-8004-3f849ae1719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sessions_append(session_base_dir, aggregate_image_dir, aggregate_csv_path):\n",
    "    \"\"\"\n",
    "    Combines data from session directories into an aggregate dataset.\n",
    "    - Uses session directory name as session_id.\n",
    "    - Renames images using session_id as a prefix.\n",
    "    - Appends data from new sessions to an existing CSV.\n",
    "    \"\"\"\n",
    "    os.makedirs(aggregate_image_dir, exist_ok=True) #\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    try:\n",
    "        session_dirs = [d for d in os.listdir(session_base_dir) if os.path.isdir(os.path.join(session_base_dir, d)) and d.startswith('session_')] #\n",
    "        session_dirs.sort() #\n",
    "    except FileNotFoundError: #\n",
    "        print(f\"Error: Base session directory not found: {session_base_dir}\") #\n",
    "        return #\n",
    "\n",
    "    print(f\"Found {len(session_dirs)} sessions to check from '{session_base_dir}'.\") #\n",
    "\n",
    "    # --- Determine which sessions are already processed (if CSV exists) ---\n",
    "    existing_sessions = set()\n",
    "    file_exists = os.path.exists(aggregate_csv_path) #\n",
    "    if file_exists:\n",
    "        try:\n",
    "            print(f\"Reading existing sessions from: {aggregate_csv_path}\") #\n",
    "            existing_df = pd.read_csv(aggregate_csv_path) #\n",
    "            if 'session_id' in existing_df.columns:\n",
    "                existing_sessions = set(existing_df['session_id'].unique()) #\n",
    "            print(f\"Found {len(existing_sessions)} existing sessions.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Existing CSV '{aggregate_csv_path}' is empty.\")\n",
    "            file_exists = False # Treat as if it doesn't exist for writing header\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing aggregate CSV: {e}. Will attempt to proceed, but caution advised.\")\n",
    "            # We might proceed but risk duplicates if we can't read existing IDs\n",
    "\n",
    "    # --- Process only new sessions ---\n",
    "    sessions_to_process = [s for s in session_dirs if s not in existing_sessions]\n",
    "    print(f\"Found {len(sessions_to_process)} new sessions to process.\")\n",
    "\n",
    "    if not sessions_to_process:\n",
    "        print(\"No new sessions to add. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for session_name in tqdm(sessions_to_process, desc=\"Processing New Sessions\"): #\n",
    "        session_path = os.path.join(session_base_dir, session_name) #\n",
    "        session_csv = os.path.join(session_path, 'data.csv') #\n",
    "        session_img_dir = os.path.join(session_path, 'images') #\n",
    "\n",
    "        if not os.path.exists(session_csv) or not os.path.exists(session_img_dir): #\n",
    "            print(f\"Warning: Skipping session {session_name}, missing data.csv or images directory.\") #\n",
    "            continue #\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(session_csv) #\n",
    "            if df.empty: #\n",
    "                 print(f\"Warning: Skipping session {session_name}, data.csv is empty.\") #\n",
    "                 continue #\n",
    "        except Exception as e: #\n",
    "            print(f\"Warning: Error reading {session_csv}, skipping session {session_name}. Error: {e}\") #\n",
    "            continue #\n",
    "\n",
    "        print(f\"Processing session: {session_name}, {len(df)} entries.\") #\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"  Processing {session_name}\", leave=False): #\n",
    "            original_relative_path = row['image_path'] #\n",
    "            original_absolute_path = os.path.join(session_path, original_relative_path) #\n",
    "            original_filename = os.path.basename(original_relative_path) #\n",
    "\n",
    "            if not os.path.exists(original_absolute_path): #\n",
    "                 print(f\"  Warning: Image not found, skipping: {original_absolute_path}\") #\n",
    "                 continue #\n",
    "\n",
    "            new_filename = f\"{session_name}_{original_filename}\" #\n",
    "            new_relative_path = os.path.join('images', new_filename) #\n",
    "            new_absolute_path = os.path.join(aggregate_image_dir, new_filename) #\n",
    "\n",
    "            try:\n",
    "                if not os.path.exists(new_absolute_path): #\n",
    "                    shutil.copy2(original_absolute_path, new_absolute_path) #\n",
    "            except Exception as e: #\n",
    "                print(f\"  Error copying image {original_absolute_path} to {new_absolute_path}. Skipping. Error: {e}\") #\n",
    "                continue #\n",
    "\n",
    "            all_data.append({ #\n",
    "                'session_id': session_name, #\n",
    "                'image_path': new_relative_path, #\n",
    "                'timestamp': row['timestamp'], #\n",
    "                'action': row['action'] #\n",
    "            })\n",
    "\n",
    "    # --- Write new data (if any) ---\n",
    "    if not all_data:\n",
    "         print(\"\\nNo new valid data found in session directories to add.\") #\n",
    "         return #\n",
    "\n",
    "    new_df_to_write = pd.DataFrame(all_data, columns=['session_id', 'image_path', 'timestamp', 'action']) #\n",
    "\n",
    "    try:\n",
    "        if file_exists:\n",
    "            # Append to existing file without header\n",
    "            print(f\"Appending {len(new_df_to_write)} new entries to {aggregate_csv_path}\")\n",
    "            new_df_to_write.to_csv(aggregate_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            # Write new file with header\n",
    "            print(f\"Creating new aggregate file {aggregate_csv_path} with {len(new_df_to_write)} entries.\")\n",
    "            new_df_to_write.to_csv(aggregate_csv_path, mode='w', header=True, index=False)\n",
    "\n",
    "        # Optional: Print total count after adding\n",
    "        final_df = pd.read_csv(aggregate_csv_path)\n",
    "        print(f\"\\nAggregate data saved. Total entries now: {len(final_df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"\\nError writing aggregated CSV file to {aggregate_csv_path}. Error: {e}\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bafcb0c-9e30-4c96-93f7-b3271b6701af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 sessions to check from 'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_laundry_session_data_two_actions'.\n",
      "Reading existing sessions from: C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions\\laundry_data.csv\n",
      "Found 6 existing sessions.\n",
      "Found 14 new sessions to process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c285800300dd4c38ada9c6c8f013ccf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing New Sessions:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250528_105352, 4241 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250528_105352:   0%|          | 0/4241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250528_105700, 4213 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250528_105700:   0%|          | 0/4213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250528_110003, 4136 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250528_110003:   0%|          | 0/4136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_170440, 4330 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_170440:   0%|          | 0/4330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_170850, 4547 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_170850:   0%|          | 0/4547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_172300, 4435 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_172300:   0%|          | 0/4435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_172731, 4419 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_172731:   0%|          | 0/4419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_175552, 4335 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_175552:   0%|          | 0/4335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_180646, 4634 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_180646:   0%|          | 0/4634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_182655, 4489 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_182655:   0%|          | 0/4489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_183407, 4227 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6f70e98eb3416796fb47d68b44a3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_183407:   0%|          | 0/4227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_184254, 4438 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_184254:   0%|          | 0/4438 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250529_184617, 4313 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250529_184617:   0%|          | 0/4313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending 61043 new entries to C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions\\laundry_data.csv\n",
      "\n",
      "Aggregate data saved. Total entries now: 87238\n"
     ]
    }
   ],
   "source": [
    "combine_sessions_append(config.SESSION_DATA_DIR, config.IMAGE_DIR, config.CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad09a7-3f57-4798-80be-d31549443483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
