{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4237926-8e41-4558-bb17-545bda787083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff98c00c-c30c-43c9-8004-3f849ae1719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sessions(session_base_dir, aggregate_image_dir, aggregate_csv_path):\n",
    "    \"\"\"\n",
    "    Combines data from session directories into an aggregate dataset.\n",
    "    - Uses session directory name as session_id.\n",
    "    - Renames images using session_id as a prefix for unique naming suitable for incremental runs.\n",
    "    \"\"\"\n",
    "    os.makedirs(aggregate_image_dir, exist_ok=True)\n",
    "\n",
    "    all_data = []\n",
    "    # global_image_counter = 0 # No longer needed for naming\n",
    "\n",
    "    try:\n",
    "        session_dirs = [d for d in os.listdir(session_base_dir) if os.path.isdir(os.path.join(session_base_dir, d)) and d.startswith('session_')]\n",
    "        session_dirs.sort()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Base session directory not found: {session_base_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(session_dirs)} sessions to combine from '{session_base_dir}'.\")\n",
    "\n",
    "    for session_name in tqdm(session_dirs, desc=\"Combining Sessions\"):\n",
    "        session_path = os.path.join(session_base_dir, session_name)\n",
    "        session_csv = os.path.join(session_path, 'data.csv')\n",
    "        session_img_dir = os.path.join(session_path, 'images')\n",
    "\n",
    "        # --- Basic session validity checks ---\n",
    "        if not os.path.exists(session_csv) or not os.path.exists(session_img_dir):\n",
    "            print(f\"Warning: Skipping session {session_name}, missing data.csv or images directory.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(session_csv)\n",
    "            if df.empty:\n",
    "                 print(f\"Warning: Skipping session {session_name}, data.csv is empty.\")\n",
    "                 continue\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error reading {session_csv}, skipping session {session_name}. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing session: {session_name}, {len(df)} entries.\")\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"  Processing {session_name}\", leave=False):\n",
    "            # original_relative_path is relative to session dir, e.g., 'images/image_00123.jpg'\n",
    "            original_relative_path = row['image_path']\n",
    "            original_absolute_path = os.path.join(session_path, original_relative_path)\n",
    "            original_filename = os.path.basename(original_relative_path) # e.g., 'image_00123.jpg'\n",
    "\n",
    "            if not os.path.exists(original_absolute_path):\n",
    "                 print(f\"  Warning: Image not found, skipping: {original_absolute_path}\")\n",
    "                 continue\n",
    "\n",
    "            # --- Create new unique filename using session_id prefix ---\n",
    "            new_filename = f\"{session_name}_{original_filename}\"\n",
    "            # new_relative_path is relative to AGGREGATE_DATA_DIR, e.g., 'images/session_XYZ_image_00123.jpg'\n",
    "            new_relative_path = os.path.join('images', new_filename)\n",
    "            new_absolute_path = os.path.join(aggregate_image_dir, new_filename)\n",
    "\n",
    "            # Copy and rename image\n",
    "            try:\n",
    "                # Check if target already exists - important if re-running/incremental\n",
    "                if not os.path.exists(new_absolute_path):\n",
    "                    shutil.copy2(original_absolute_path, new_absolute_path)\n",
    "                # else: # Optional: Add logic here if you want to handle existing files differently\n",
    "                #     print(f\"  Info: Target image already exists, skipping copy: {new_absolute_path}\")\n",
    "                pass # If it exists, assume it's from a previous run, do nothing\n",
    "            except Exception as e:\n",
    "                print(f\"  Error copying image {original_absolute_path} to {new_absolute_path}. Skipping. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Append data to master list\n",
    "            all_data.append({\n",
    "                'session_id': session_name,\n",
    "                'image_path': new_relative_path, # Store the new relative path with prefix\n",
    "                'timestamp': row['timestamp'],\n",
    "                'action': row['action']\n",
    "            })\n",
    "            # global_image_counter += 1 # No longer needed\n",
    "\n",
    "    # --- Combine with existing data if AGGREGATE_CSV_PATH exists (Basic Incremental Logic) ---\n",
    "    if os.path.exists(aggregate_csv_path):\n",
    "        print(f\"Found existing aggregate CSV: {aggregate_csv_path}. Appending new data.\")\n",
    "        try:\n",
    "            existing_df = pd.read_csv(aggregate_csv_path)\n",
    "            # Get session IDs already present\n",
    "            existing_sessions = set(existing_df['session_id'].unique())\n",
    "            # Filter new data to only include sessions not already present\n",
    "            new_data_df = pd.DataFrame(all_data)\n",
    "            new_data_to_add = new_data_df[~new_data_df['session_id'].isin(existing_sessions)]\n",
    "\n",
    "            if not new_data_to_add.empty:\n",
    "                print(f\"Adding data for {len(new_data_to_add['session_id'].unique())} new sessions.\")\n",
    "                combined_df = pd.concat([existing_df, new_data_to_add], ignore_index=True)\n",
    "            else:\n",
    "                print(\"No new sessions found to add.\")\n",
    "                combined_df = existing_df # No changes needed\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or processing existing aggregate CSV. Overwriting. Error: {e}\")\n",
    "            # Fallback to just writing the new data if reading fails\n",
    "            combined_df = pd.DataFrame(all_data, columns=['session_id', 'image_path', 'timestamp', 'action'])\n",
    "\n",
    "    elif all_data:\n",
    "         print(\"No existing aggregate CSV found. Creating new file.\")\n",
    "         combined_df = pd.DataFrame(all_data, columns=['session_id', 'image_path', 'timestamp', 'action'])\n",
    "    else:\n",
    "         print(\"\\nNo valid data found in session directories to combine.\")\n",
    "         return # Exit if no data\n",
    "\n",
    "\n",
    "    # Write the combined/updated aggregate CSV\n",
    "    try:\n",
    "        combined_df.to_csv(aggregate_csv_path, index=False)\n",
    "        print(f\"\\nAggregate data saved to {aggregate_csv_path}\")\n",
    "        print(f\"Total entries in aggregate CSV: {len(combined_df)}\")\n",
    "    except Exception as e:\n",
    "         print(f\"\\nError saving aggregated CSV file to {aggregate_csv_path}. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bafcb0c-9e30-4c96-93f7-b3271b6701af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 sessions to combine from 'jetbot_session_data_two_actions'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87844a46b532432fbb6eae9568bc4bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combining Sessions:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_170732, 2136 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_170732:   0%|          | 0/2136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_170951, 2091 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_170951:   0%|          | 0/2091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_171145, 1848 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_171145:   0%|          | 0/1848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_171325, 1926 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_171325:   0%|          | 0/1926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_171510, 2190 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_171510:   0%|          | 0/2190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_171710, 2096 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_171710:   0%|          | 0/2096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_172036, 2078 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_172036:   0%|          | 0/2078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_172838, 2262 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_172838:   0%|          | 0/2262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_173056, 2404 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_173056:   0%|          | 0/2404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_173308, 2128 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_173308:   0%|          | 0/2128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250325_174840, 1922 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250325_174840:   0%|          | 0/1922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing aggregate CSV found. Creating new file.\n",
      "\n",
      "Aggregate data saved to jetbot_data_two_actions\\data.csv\n",
      "Total entries in aggregate CSV: 23081\n"
     ]
    }
   ],
   "source": [
    "combine_sessions(config.SESSION_DATA_DIR, config.IMAGE_DIR, config.CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad09a7-3f57-4798-80be-d31549443483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
