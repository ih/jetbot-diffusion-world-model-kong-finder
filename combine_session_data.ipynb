{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4237926-8e41-4558-bb17-545bda787083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff98c00c-c30c-43c9-8004-3f849ae1719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sessions_append(session_base_dir, aggregate_image_dir, aggregate_csv_path):\n",
    "    \"\"\"\n",
    "    Combines data from session directories into an aggregate dataset.\n",
    "    - Uses session directory name as session_id.\n",
    "    - Renames images using session_id as a prefix.\n",
    "    - Appends data from new sessions to an existing CSV.\n",
    "    \"\"\"\n",
    "    os.makedirs(aggregate_image_dir, exist_ok=True) #\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    try:\n",
    "        session_dirs = [d for d in os.listdir(session_base_dir) if os.path.isdir(os.path.join(session_base_dir, d)) and d.startswith('session_')] #\n",
    "        session_dirs.sort() #\n",
    "    except FileNotFoundError: #\n",
    "        print(f\"Error: Base session directory not found: {session_base_dir}\") #\n",
    "        return #\n",
    "\n",
    "    print(f\"Found {len(session_dirs)} sessions to check from '{session_base_dir}'.\") #\n",
    "\n",
    "    # --- Determine which sessions are already processed (if CSV exists) ---\n",
    "    existing_sessions = set()\n",
    "    file_exists = os.path.exists(aggregate_csv_path) #\n",
    "    if file_exists:\n",
    "        try:\n",
    "            print(f\"Reading existing sessions from: {aggregate_csv_path}\") #\n",
    "            existing_df = pd.read_csv(aggregate_csv_path) #\n",
    "            if 'session_id' in existing_df.columns:\n",
    "                existing_sessions = set(existing_df['session_id'].unique()) #\n",
    "            print(f\"Found {len(existing_sessions)} existing sessions.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Existing CSV '{aggregate_csv_path}' is empty.\")\n",
    "            file_exists = False # Treat as if it doesn't exist for writing header\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing aggregate CSV: {e}. Will attempt to proceed, but caution advised.\")\n",
    "            # We might proceed but risk duplicates if we can't read existing IDs\n",
    "\n",
    "    # --- Process only new sessions ---\n",
    "    sessions_to_process = [s for s in session_dirs if s not in existing_sessions]\n",
    "    print(f\"Found {len(sessions_to_process)} new sessions to process.\")\n",
    "\n",
    "    if not sessions_to_process:\n",
    "        print(\"No new sessions to add. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for session_name in tqdm(sessions_to_process, desc=\"Processing New Sessions\"): #\n",
    "        session_path = os.path.join(session_base_dir, session_name) #\n",
    "        session_csv = os.path.join(session_path, 'data.csv') #\n",
    "        session_img_dir = os.path.join(session_path, 'images') #\n",
    "\n",
    "        if not os.path.exists(session_csv) or not os.path.exists(session_img_dir): #\n",
    "            print(f\"Warning: Skipping session {session_name}, missing data.csv or images directory.\") #\n",
    "            continue #\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(session_csv) #\n",
    "            if df.empty: #\n",
    "                 print(f\"Warning: Skipping session {session_name}, data.csv is empty.\") #\n",
    "                 continue #\n",
    "        except Exception as e: #\n",
    "            print(f\"Warning: Error reading {session_csv}, skipping session {session_name}. Error: {e}\") #\n",
    "            continue #\n",
    "\n",
    "        print(f\"Processing session: {session_name}, {len(df)} entries.\") #\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"  Processing {session_name}\", leave=False): #\n",
    "            original_relative_path = row['image_path'] #\n",
    "            original_absolute_path = os.path.join(session_path, original_relative_path) #\n",
    "            original_filename = os.path.basename(original_relative_path) #\n",
    "\n",
    "            if not os.path.exists(original_absolute_path): #\n",
    "                 print(f\"  Warning: Image not found, skipping: {original_absolute_path}\") #\n",
    "                 continue #\n",
    "\n",
    "            new_filename = f\"{session_name}_{original_filename}\" #\n",
    "            new_relative_path = os.path.join('images', new_filename) #\n",
    "            new_absolute_path = os.path.join(aggregate_image_dir, new_filename) #\n",
    "\n",
    "            try:\n",
    "                if not os.path.exists(new_absolute_path): #\n",
    "                    shutil.copy2(original_absolute_path, new_absolute_path) #\n",
    "            except Exception as e: #\n",
    "                print(f\"  Error copying image {original_absolute_path} to {new_absolute_path}. Skipping. Error: {e}\") #\n",
    "                continue #\n",
    "\n",
    "            all_data.append({ #\n",
    "                'session_id': session_name, #\n",
    "                'image_path': new_relative_path, #\n",
    "                'timestamp': row['timestamp'], #\n",
    "                'action': row['action'] #\n",
    "            })\n",
    "\n",
    "    # --- Write new data (if any) ---\n",
    "    if not all_data:\n",
    "         print(\"\\nNo new valid data found in session directories to add.\") #\n",
    "         return #\n",
    "\n",
    "    new_df_to_write = pd.DataFrame(all_data, columns=['session_id', 'image_path', 'timestamp', 'action']) #\n",
    "\n",
    "    try:\n",
    "        if file_exists:\n",
    "            # Append to existing file without header\n",
    "            print(f\"Appending {len(new_df_to_write)} new entries to {aggregate_csv_path}\")\n",
    "            new_df_to_write.to_csv(aggregate_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            # Write new file with header\n",
    "            print(f\"Creating new aggregate file {aggregate_csv_path} with {len(new_df_to_write)} entries.\")\n",
    "            new_df_to_write.to_csv(aggregate_csv_path, mode='w', header=True, index=False)\n",
    "\n",
    "        # Optional: Print total count after adding\n",
    "        final_df = pd.read_csv(aggregate_csv_path)\n",
    "        print(f\"\\nAggregate data saved. Total entries now: {len(final_df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"\\nError writing aggregated CSV file to {aggregate_csv_path}. Error: {e}\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45e8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_new_sessions_only(session_base_dir, processed_csv_path, new_image_dir, new_csv_path):\n",
    "    \"\"\"Collects only sessions not already present in processed_csv_path and\n",
    "    writes them to a separate aggregate located at ``new_image_dir`` and ``new_csv_path``.\n",
    "    This is useful for incremental training before permanently adding the\n",
    "    sessions to the full dataset.\"\"\"\n",
    "    os.makedirs(new_image_dir, exist_ok=True)\n",
    "    if os.path.exists(new_csv_path):\n",
    "        os.remove(new_csv_path)\n",
    "\n",
    "    existing_sessions = set()\n",
    "    if os.path.exists(processed_csv_path):\n",
    "        try:\n",
    "            df_existing = pd.read_csv(processed_csv_path)\n",
    "            if 'session_id' in df_existing.columns:\n",
    "                existing_sessions = set(df_existing['session_id'].unique())\n",
    "        except Exception as exc:\n",
    "            print(f\"Error reading processed CSV {processed_csv_path}: {exc}\")\n",
    "\n",
    "    try:\n",
    "        session_dirs = [d for d in os.listdir(session_base_dir)\n",
    "                        if os.path.isdir(os.path.join(session_base_dir, d)) and d.startswith('session_')]\n",
    "        session_dirs.sort()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Base session directory not found: {session_base_dir}\")\n",
    "        return []\n",
    "\n",
    "    sessions_to_process = [s for s in session_dirs if s not in existing_sessions]\n",
    "    print(f\"Found {len(sessions_to_process)} new sessions to collect.\")\n",
    "\n",
    "    all_rows = []\n",
    "    for session_name in tqdm(sessions_to_process, desc=\"Collecting New Sessions\"):\n",
    "        session_path = os.path.join(session_base_dir, session_name)\n",
    "        session_csv = os.path.join(session_path, 'data.csv')\n",
    "        session_img_dir = os.path.join(session_path, 'images')\n",
    "        if not os.path.exists(session_csv) or not os.path.exists(session_img_dir):\n",
    "            print(f\"Skipping {session_name}, missing data.csv or images\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(session_csv)\n",
    "        except Exception as exc:\n",
    "            print(f\"Error reading {session_csv}: {exc}\")\n",
    "            continue\n",
    "        for _, row in df.iterrows():\n",
    "            orig_rel = row['image_path']\n",
    "            orig_abs = os.path.join(session_path, orig_rel)\n",
    "            new_filename = f\"{session_name}_{os.path.basename(orig_rel)}\"\n",
    "            new_rel = os.path.join('images', new_filename)\n",
    "            new_abs = os.path.join(new_image_dir, new_filename)\n",
    "            if not os.path.exists(orig_abs):\n",
    "                continue\n",
    "            if not os.path.exists(new_abs):\n",
    "                try:\n",
    "                    shutil.copy2(orig_abs, new_abs)\n",
    "                except Exception as exc:\n",
    "                    print(f\"Could not copy {orig_abs}: {exc}\")\n",
    "                    continue\n",
    "            all_rows.append({'session_id': session_name,\n",
    "                             'image_path': new_rel,\n",
    "                             'timestamp': row.get('timestamp', ''),\n",
    "                             'action': row['action']})\n",
    "\n",
    "    if all_rows:\n",
    "        pd.DataFrame(all_rows).to_csv(new_csv_path, index=False)\n",
    "        print(f\"Wrote {len(all_rows)} entries to {new_csv_path}\")\n",
    "    else:\n",
    "        print(\"No new session data found.\")\n",
    "    return sessions_to_process\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075fe850-7563-4dd8-aded-1c1c438bca6a",
   "metadata": {},
   "source": [
    "combine_sessions_append(\n",
    "    r'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_session_data_two_actions_holdout_laundry', \n",
    "    r'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions_holdout\\images',\n",
    "    r'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions_holdout\\holdout.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfc084-4784-44bc-8e14-ebfe78622485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74 sessions to check from 'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_livingroom_session_data_single_position'.\n",
      "Found 74 new sessions to process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8299d3ad8e984000a06cd33133bbc071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing New Sessions:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_143822, 2170 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93369a92eeb44800b35d67a51f22c688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_143822:   0%|          | 0/2170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_144513, 2200 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd67bac0ef2a46729687b73f726c385a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_144513:   0%|          | 0/2200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_144645, 2119 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2432d2ae576d477ab86eb70fcc3cb6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_144645:   0%|          | 0/2119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_144818, 2311 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41f5332c6034ea8aef9f22d55df9338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_144818:   0%|          | 0/2311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_144957, 2234 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bfa663c6254ade98050fb0c26140c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_144957:   0%|          | 0/2234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_145139, 2180 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa38a829577447ca7b1eee6e8b34172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_145139:   0%|          | 0/2180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_145327, 2146 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b268273c5ea48f58aa7d86aa1bd5764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_145327:   0%|          | 0/2146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_145453, 2326 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8e5388b6b44dbda3528bb8a9d6d77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_145453:   0%|          | 0/2326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_145630, 2178 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad21eb5f12e4ed7b8cc403c8ce346d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_145630:   0%|          | 0/2178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250724_145804, 2123 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e64c05bad34c958beaff3d1bf2e79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250724_145804:   0%|          | 0/2123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    combine_sessions_append(config.SESSION_DATA_DIR, config.IMAGE_DIR, config.CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad09a7-3f57-4798-80be-d31549443483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
