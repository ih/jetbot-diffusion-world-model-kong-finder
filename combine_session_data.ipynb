{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4237926-8e41-4558-bb17-545bda787083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff98c00c-c30c-43c9-8004-3f849ae1719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sessions_append(session_base_dir, aggregate_image_dir, aggregate_csv_path):\n",
    "    \"\"\"\n",
    "    Combines data from session directories into an aggregate dataset.\n",
    "    - Uses session directory name as session_id.\n",
    "    - Renames images using session_id as a prefix.\n",
    "    - Appends data from new sessions to an existing CSV.\n",
    "    \"\"\"\n",
    "    os.makedirs(aggregate_image_dir, exist_ok=True) #\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    try:\n",
    "        session_dirs = [d for d in os.listdir(session_base_dir) if os.path.isdir(os.path.join(session_base_dir, d)) and d.startswith('session_')] #\n",
    "        session_dirs.sort() #\n",
    "    except FileNotFoundError: #\n",
    "        print(f\"Error: Base session directory not found: {session_base_dir}\") #\n",
    "        return #\n",
    "\n",
    "    print(f\"Found {len(session_dirs)} sessions to check from '{session_base_dir}'.\") #\n",
    "\n",
    "    # --- Determine which sessions are already processed (if CSV exists) ---\n",
    "    existing_sessions = set()\n",
    "    file_exists = os.path.exists(aggregate_csv_path) #\n",
    "    if file_exists:\n",
    "        try:\n",
    "            print(f\"Reading existing sessions from: {aggregate_csv_path}\") #\n",
    "            existing_df = pd.read_csv(aggregate_csv_path) #\n",
    "            if 'session_id' in existing_df.columns:\n",
    "                existing_sessions = set(existing_df['session_id'].unique()) #\n",
    "            print(f\"Found {len(existing_sessions)} existing sessions.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: Existing CSV '{aggregate_csv_path}' is empty.\")\n",
    "            file_exists = False # Treat as if it doesn't exist for writing header\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing aggregate CSV: {e}. Will attempt to proceed, but caution advised.\")\n",
    "            # We might proceed but risk duplicates if we can't read existing IDs\n",
    "\n",
    "    # --- Process only new sessions ---\n",
    "    sessions_to_process = [s for s in session_dirs if s not in existing_sessions]\n",
    "    print(f\"Found {len(sessions_to_process)} new sessions to process.\")\n",
    "\n",
    "    if not sessions_to_process:\n",
    "        print(\"No new sessions to add. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for session_name in tqdm(sessions_to_process, desc=\"Processing New Sessions\"): #\n",
    "        session_path = os.path.join(session_base_dir, session_name) #\n",
    "        session_csv = os.path.join(session_path, 'data.csv') #\n",
    "        session_img_dir = os.path.join(session_path, 'images') #\n",
    "\n",
    "        if not os.path.exists(session_csv) or not os.path.exists(session_img_dir): #\n",
    "            print(f\"Warning: Skipping session {session_name}, missing data.csv or images directory.\") #\n",
    "            continue #\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(session_csv) #\n",
    "            if df.empty: #\n",
    "                 print(f\"Warning: Skipping session {session_name}, data.csv is empty.\") #\n",
    "                 continue #\n",
    "        except Exception as e: #\n",
    "            print(f\"Warning: Error reading {session_csv}, skipping session {session_name}. Error: {e}\") #\n",
    "            continue #\n",
    "\n",
    "        print(f\"Processing session: {session_name}, {len(df)} entries.\") #\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"  Processing {session_name}\", leave=False): #\n",
    "            original_relative_path = row['image_path'] #\n",
    "            original_absolute_path = os.path.join(session_path, original_relative_path) #\n",
    "            original_filename = os.path.basename(original_relative_path) #\n",
    "\n",
    "            if not os.path.exists(original_absolute_path): #\n",
    "                 print(f\"  Warning: Image not found, skipping: {original_absolute_path}\") #\n",
    "                 continue #\n",
    "\n",
    "            new_filename = f\"{session_name}_{original_filename}\" #\n",
    "            new_relative_path = os.path.join('images', new_filename) #\n",
    "            new_absolute_path = os.path.join(aggregate_image_dir, new_filename) #\n",
    "\n",
    "            try:\n",
    "                if not os.path.exists(new_absolute_path): #\n",
    "                    shutil.copy2(original_absolute_path, new_absolute_path) #\n",
    "            except Exception as e: #\n",
    "                print(f\"  Error copying image {original_absolute_path} to {new_absolute_path}. Skipping. Error: {e}\") #\n",
    "                continue #\n",
    "\n",
    "            all_data.append({ #\n",
    "                'session_id': session_name, #\n",
    "                'image_path': new_relative_path, #\n",
    "                'timestamp': row['timestamp'], #\n",
    "                'action': row['action'] #\n",
    "            })\n",
    "\n",
    "    # --- Write new data (if any) ---\n",
    "    if not all_data:\n",
    "         print(\"\\nNo new valid data found in session directories to add.\") #\n",
    "         return #\n",
    "\n",
    "    new_df_to_write = pd.DataFrame(all_data, columns=['session_id', 'image_path', 'timestamp', 'action']) #\n",
    "\n",
    "    try:\n",
    "        if file_exists:\n",
    "            # Append to existing file without header\n",
    "            print(f\"Appending {len(new_df_to_write)} new entries to {aggregate_csv_path}\")\n",
    "            new_df_to_write.to_csv(aggregate_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            # Write new file with header\n",
    "            print(f\"Creating new aggregate file {aggregate_csv_path} with {len(new_df_to_write)} entries.\")\n",
    "            new_df_to_write.to_csv(aggregate_csv_path, mode='w', header=True, index=False)\n",
    "\n",
    "        # Optional: Print total count after adding\n",
    "        final_df = pd.read_csv(aggregate_csv_path)\n",
    "        print(f\"\\nAggregate data saved. Total entries now: {len(final_df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"\\nError writing aggregated CSV file to {aggregate_csv_path}. Error: {e}\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45e8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_new_sessions_only(session_base_dir, processed_csv_path, new_image_dir, new_csv_path):\n",
    "    \"\"\"Collects only sessions not already present in processed_csv_path and\n",
    "    writes them to a separate aggregate located at ``new_image_dir`` and ``new_csv_path``.\n",
    "    This is useful for incremental training before permanently adding the\n",
    "    sessions to the full dataset.\"\"\"\n",
    "    os.makedirs(new_image_dir, exist_ok=True)\n",
    "    if os.path.exists(new_csv_path):\n",
    "        os.remove(new_csv_path)\n",
    "\n",
    "    existing_sessions = set()\n",
    "    if os.path.exists(processed_csv_path):\n",
    "        try:\n",
    "            df_existing = pd.read_csv(processed_csv_path)\n",
    "            if 'session_id' in df_existing.columns:\n",
    "                existing_sessions = set(df_existing['session_id'].unique())\n",
    "        except Exception as exc:\n",
    "            print(f\"Error reading processed CSV {processed_csv_path}: {exc}\")\n",
    "\n",
    "    try:\n",
    "        session_dirs = [d for d in os.listdir(session_base_dir)\n",
    "                        if os.path.isdir(os.path.join(session_base_dir, d)) and d.startswith('session_')]\n",
    "        session_dirs.sort()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Base session directory not found: {session_base_dir}\")\n",
    "        return []\n",
    "\n",
    "    sessions_to_process = [s for s in session_dirs if s not in existing_sessions]\n",
    "    print(f\"Found {len(sessions_to_process)} new sessions to collect.\")\n",
    "\n",
    "    all_rows = []\n",
    "    for session_name in tqdm(sessions_to_process, desc=\"Collecting New Sessions\"):\n",
    "        session_path = os.path.join(session_base_dir, session_name)\n",
    "        session_csv = os.path.join(session_path, 'data.csv')\n",
    "        session_img_dir = os.path.join(session_path, 'images')\n",
    "        if not os.path.exists(session_csv) or not os.path.exists(session_img_dir):\n",
    "            print(f\"Skipping {session_name}, missing data.csv or images\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(session_csv)\n",
    "        except Exception as exc:\n",
    "            print(f\"Error reading {session_csv}: {exc}\")\n",
    "            continue\n",
    "        for _, row in df.iterrows():\n",
    "            orig_rel = row['image_path']\n",
    "            orig_abs = os.path.join(session_path, orig_rel)\n",
    "            new_filename = f\"{session_name}_{os.path.basename(orig_rel)}\"\n",
    "            new_rel = os.path.join('images', new_filename)\n",
    "            new_abs = os.path.join(new_image_dir, new_filename)\n",
    "            if not os.path.exists(orig_abs):\n",
    "                continue\n",
    "            if not os.path.exists(new_abs):\n",
    "                try:\n",
    "                    shutil.copy2(orig_abs, new_abs)\n",
    "                except Exception as exc:\n",
    "                    print(f\"Could not copy {orig_abs}: {exc}\")\n",
    "                    continue\n",
    "            all_rows.append({'session_id': session_name,\n",
    "                             'image_path': new_rel,\n",
    "                             'timestamp': row.get('timestamp', ''),\n",
    "                             'action': row['action']})\n",
    "\n",
    "    if all_rows:\n",
    "        pd.DataFrame(all_rows).to_csv(new_csv_path, index=False)\n",
    "        print(f\"Wrote {len(all_rows)} entries to {new_csv_path}\")\n",
    "    else:\n",
    "        print(\"No new session data found.\")\n",
    "    return sessions_to_process\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075fe850-7563-4dd8-aded-1c1c438bca6a",
   "metadata": {},
   "source": [
    "combine_sessions_append(\n",
    "    r'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_session_data_two_actions_holdout_laundry', \n",
    "    r'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions_holdout\\images',\n",
    "    r'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions_holdout\\holdout.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cdfc084-4784-44bc-8e14-ebfe78622485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165 sessions to check from 'C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_livingroom_session_data_single_position'.\n",
      "Reading existing sessions from: C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions_single_position\\livingroom_data_incremental_test.csv\n",
      "Found 150 existing sessions.\n",
      "Found 15 new sessions to process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b60d67720434b66a4da0e56184933d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing New Sessions:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250804_134305, 2206 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9771e552ad0a4bbba7dee6c88471040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250804_134305:   0%|          | 0/2206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250804_134558, 2218 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be4e377ad7b4970a56a929bbe59a419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250804_134558:   0%|          | 0/2218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250804_135959, 2071 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e60d9aed314f93bb3a140db46565be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250804_135959:   0%|          | 0/2071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_154831, 2168 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c14f33fb5d54523b4a9e8ff870eeb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_154831:   0%|          | 0/2168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_155235, 2224 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49c22a0152f4ba4a6c85d0502e6b6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_155235:   0%|          | 0/2224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_155416, 2137 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4895d542d4ac41f9a8e4a095de443578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_155416:   0%|          | 0/2137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_155827, 2214 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18979f4e9c634f549a9c04d8f4e289a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_155827:   0%|          | 0/2214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_160041, 2178 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8765c6888e5b470394b6f6bb8bcee94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_160041:   0%|          | 0/2178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_160215, 2283 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b4a44a2a4e4fe2ba5b8385cb6b91e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_160215:   0%|          | 0/2283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_160358, 2232 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59526a5842848e9aeaf325a97eeac7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_160358:   0%|          | 0/2232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_160554, 2199 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d982a12cd14628b082d50a286238a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_160554:   0%|          | 0/2199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_160736, 2271 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678e7c31b5de4ff08219d2c714aaf6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_160736:   0%|          | 0/2271 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_161033, 2161 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468b83835f0147fa9c1d42689fa64a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_161033:   0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_161438, 2223 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6e38c35464404aa702d7cf5386e3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_161438:   0%|          | 0/2223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session: session_20250805_161624, 2234 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e34dab17d744ceb086ba9c16d86baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Processing session_20250805_161624:   0%|          | 0/2234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending 33019 new entries to C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions_single_position\\livingroom_data_incremental_test.csv\n",
      "\n",
      "Aggregate data saved. Total entries now: 365153\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    combine_sessions_append(config.SESSION_DATA_DIR, config.IMAGE_DIR, config.CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad09a7-3f57-4798-80be-d31549443483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
