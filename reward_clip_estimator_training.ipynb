{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935ae1fc-def3-4509-be82-c71dc85d9b70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.39 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from transformers>=4.39) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from accelerate) (2.2.2+cu118)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from tqdm>=4.27->transformers>=4.39) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from requests->transformers>=4.39) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from requests->transformers>=4.39) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from requests->transformers>=4.39) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from requests->transformers>=4.39) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\projects\\pythonenv-deeprl\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.39\" accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175efe49-29df-4c72-bfde-1dad3b0b9e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded main data CSV: C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions\\data.csv (23081 rows)\n",
      "Loaded reward labels CSV: C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_data_two_actions\\interactive_reward_labels_subset.csv (3164 rows)\n",
      "Created labeled dataframe with 3164 entries.\n",
      "Dataset length: 3164\n",
      "train=2848, val=316\n",
      "Froze ViT & text backbones; training projection layers only.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11250/11250 2:06:46, Epoch 250/250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.657100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.648300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.650200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.635100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.647500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>3.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>3.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.642800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.654700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>3.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>3.627100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>3.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>3.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>3.635100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>3.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>3.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>3.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>3.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>3.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>3.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>3.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>3.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>3.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>3.638900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>3.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>3.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>3.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>3.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>3.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>3.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>3.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>3.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>3.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>3.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>3.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>3.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>3.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>3.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.631600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>3.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>3.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>3.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>3.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>3.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>3.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.644500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>3.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>3.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>3.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>3.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>3.635100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>3.622700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>3.643100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>3.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>3.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>3.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>3.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>3.637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>3.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.639900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>3.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>3.640500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>3.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>3.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>3.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>3.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>3.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>3.640500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>3.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>3.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>3.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>3.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>3.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>3.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>3.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>3.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>3.626200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>3.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>3.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>3.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>3.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>3.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>3.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>3.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>3.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>3.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>3.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>3.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>3.622200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  •  val top-1 = 0.032\n",
      "Epoch 2  •  val top-1 = 0.032\n",
      "Epoch 3  •  val top-1 = 0.032\n",
      "Epoch 4  •  val top-1 = 0.032\n",
      "Epoch 5  •  val top-1 = 0.032\n",
      "Epoch 6  •  val top-1 = 0.032\n",
      "Epoch 7  •  val top-1 = 0.032\n",
      "Epoch 8  •  val top-1 = 0.032\n",
      "Epoch 9  •  val top-1 = 0.032\n",
      "Epoch 10  •  val top-1 = 0.032\n",
      "Epoch 11  •  val top-1 = 0.032\n",
      "Epoch 12  •  val top-1 = 0.032\n",
      "Epoch 13  •  val top-1 = 0.032\n",
      "Epoch 14  •  val top-1 = 0.032\n",
      "Epoch 15  •  val top-1 = 0.032\n",
      "Epoch 16  •  val top-1 = 0.032\n",
      "Epoch 17  •  val top-1 = 0.032\n",
      "Epoch 18  •  val top-1 = 0.032\n",
      "Epoch 19  •  val top-1 = 0.032\n",
      "Epoch 20  •  val top-1 = 0.032\n",
      "Epoch 21  •  val top-1 = 0.032\n",
      "Epoch 22  •  val top-1 = 0.032\n",
      "Epoch 23  •  val top-1 = 0.032\n",
      "Epoch 24  •  val top-1 = 0.032\n",
      "Epoch 25  •  val top-1 = 0.032\n",
      "Epoch 26  •  val top-1 = 0.032\n",
      "Epoch 27  •  val top-1 = 0.032\n",
      "Epoch 28  •  val top-1 = 0.032\n",
      "Epoch 29  •  val top-1 = 0.032\n",
      "Epoch 30  •  val top-1 = 0.032\n",
      "Epoch 31  •  val top-1 = 0.032\n",
      "Epoch 32  •  val top-1 = 0.032\n",
      "Epoch 33  •  val top-1 = 0.032\n",
      "Epoch 34  •  val top-1 = 0.032\n",
      "Epoch 35  •  val top-1 = 0.032\n",
      "Epoch 36  •  val top-1 = 0.032\n",
      "Epoch 37  •  val top-1 = 0.032\n",
      "Epoch 38  •  val top-1 = 0.032\n",
      "Epoch 39  •  val top-1 = 0.032\n",
      "Epoch 40  •  val top-1 = 0.032\n",
      "Epoch 41  •  val top-1 = 0.032\n",
      "Epoch 42  •  val top-1 = 0.032\n",
      "Epoch 43  •  val top-1 = 0.032\n",
      "Epoch 44  •  val top-1 = 0.032\n",
      "Epoch 45  •  val top-1 = 0.032\n",
      "Epoch 46  •  val top-1 = 0.032\n",
      "Epoch 47  •  val top-1 = 0.032\n",
      "Epoch 48  •  val top-1 = 0.032\n",
      "Epoch 49  •  val top-1 = 0.032\n",
      "Epoch 50  •  val top-1 = 0.032\n",
      "Epoch 51  •  val top-1 = 0.032\n",
      "Epoch 52  •  val top-1 = 0.032\n",
      "Epoch 53  •  val top-1 = 0.032\n",
      "Epoch 54  •  val top-1 = 0.032\n",
      "Epoch 55  •  val top-1 = 0.032\n",
      "Epoch 56  •  val top-1 = 0.032\n",
      "Epoch 57  •  val top-1 = 0.032\n",
      "Epoch 58  •  val top-1 = 0.032\n",
      "Epoch 59  •  val top-1 = 0.032\n",
      "Epoch 60  •  val top-1 = 0.032\n",
      "Epoch 61  •  val top-1 = 0.032\n",
      "Epoch 62  •  val top-1 = 0.032\n",
      "Epoch 63  •  val top-1 = 0.032\n",
      "Epoch 64  •  val top-1 = 0.032\n",
      "Epoch 65  •  val top-1 = 0.032\n",
      "Epoch 66  •  val top-1 = 0.032\n",
      "Epoch 67  •  val top-1 = 0.032\n",
      "Epoch 68  •  val top-1 = 0.032\n",
      "Epoch 69  •  val top-1 = 0.032\n",
      "Epoch 70  •  val top-1 = 0.032\n",
      "Epoch 71  •  val top-1 = 0.032\n",
      "Epoch 72  •  val top-1 = 0.032\n",
      "Epoch 73  •  val top-1 = 0.032\n",
      "Epoch 74  •  val top-1 = 0.032\n",
      "Epoch 75  •  val top-1 = 0.032\n",
      "Epoch 76  •  val top-1 = 0.032\n",
      "Epoch 77  •  val top-1 = 0.032\n",
      "Epoch 78  •  val top-1 = 0.032\n",
      "Epoch 79  •  val top-1 = 0.032\n",
      "Epoch 80  •  val top-1 = 0.032\n",
      "Epoch 81  •  val top-1 = 0.032\n",
      "Epoch 82  •  val top-1 = 0.032\n",
      "Epoch 83  •  val top-1 = 0.032\n",
      "Epoch 84  •  val top-1 = 0.032\n",
      "Epoch 85  •  val top-1 = 0.032\n",
      "Epoch 86  •  val top-1 = 0.032\n",
      "Epoch 87  •  val top-1 = 0.032\n",
      "Epoch 88  •  val top-1 = 0.032\n",
      "Epoch 89  •  val top-1 = 0.032\n",
      "Epoch 90  •  val top-1 = 0.032\n",
      "Epoch 91  •  val top-1 = 0.032\n",
      "Epoch 92  •  val top-1 = 0.032\n",
      "Epoch 93  •  val top-1 = 0.032\n",
      "Epoch 94  •  val top-1 = 0.032\n",
      "Epoch 95  •  val top-1 = 0.032\n",
      "Epoch 96  •  val top-1 = 0.032\n",
      "Epoch 97  •  val top-1 = 0.032\n",
      "Epoch 98  •  val top-1 = 0.032\n",
      "Epoch 99  •  val top-1 = 0.032\n",
      "Epoch 100  •  val top-1 = 0.032\n",
      "Epoch 101  •  val top-1 = 0.032\n",
      "Epoch 102  •  val top-1 = 0.032\n",
      "Epoch 103  •  val top-1 = 0.032\n",
      "Epoch 104  •  val top-1 = 0.032\n",
      "Epoch 105  •  val top-1 = 0.032\n",
      "Epoch 106  •  val top-1 = 0.032\n",
      "Epoch 107  •  val top-1 = 0.032\n",
      "Epoch 108  •  val top-1 = 0.032\n",
      "Epoch 109  •  val top-1 = 0.032\n",
      "Epoch 110  •  val top-1 = 0.032\n",
      "Epoch 111  •  val top-1 = 0.032\n",
      "Epoch 112  •  val top-1 = 0.032\n",
      "Epoch 113  •  val top-1 = 0.032\n",
      "Epoch 114  •  val top-1 = 0.032\n",
      "Epoch 115  •  val top-1 = 0.032\n",
      "Epoch 116  •  val top-1 = 0.032\n",
      "Epoch 117  •  val top-1 = 0.032\n",
      "Epoch 118  •  val top-1 = 0.032\n",
      "Epoch 119  •  val top-1 = 0.032\n",
      "Epoch 120  •  val top-1 = 0.032\n",
      "Epoch 121  •  val top-1 = 0.032\n",
      "Epoch 122  •  val top-1 = 0.032\n",
      "Epoch 123  •  val top-1 = 0.032\n",
      "Epoch 124  •  val top-1 = 0.032\n",
      "Epoch 125  •  val top-1 = 0.032\n",
      "Epoch 126  •  val top-1 = 0.032\n",
      "Epoch 127  •  val top-1 = 0.032\n",
      "Epoch 128  •  val top-1 = 0.032\n",
      "Epoch 129  •  val top-1 = 0.032\n",
      "Epoch 130  •  val top-1 = 0.032\n",
      "Epoch 131  •  val top-1 = 0.032\n",
      "Epoch 132  •  val top-1 = 0.032\n",
      "Epoch 133  •  val top-1 = 0.032\n",
      "Epoch 134  •  val top-1 = 0.032\n",
      "Epoch 135  •  val top-1 = 0.032\n",
      "Epoch 136  •  val top-1 = 0.032\n",
      "Epoch 137  •  val top-1 = 0.032\n",
      "Epoch 138  •  val top-1 = 0.032\n",
      "Epoch 139  •  val top-1 = 0.032\n",
      "Epoch 140  •  val top-1 = 0.032\n",
      "Epoch 141  •  val top-1 = 0.032\n",
      "Epoch 142  •  val top-1 = 0.032\n",
      "Epoch 143  •  val top-1 = 0.032\n",
      "Epoch 144  •  val top-1 = 0.032\n",
      "Epoch 145  •  val top-1 = 0.032\n",
      "Epoch 146  •  val top-1 = 0.032\n",
      "Epoch 147  •  val top-1 = 0.032\n",
      "Epoch 148  •  val top-1 = 0.032\n",
      "Epoch 149  •  val top-1 = 0.032\n",
      "Epoch 150  •  val top-1 = 0.032\n",
      "Epoch 151  •  val top-1 = 0.032\n",
      "Epoch 152  •  val top-1 = 0.032\n",
      "Epoch 153  •  val top-1 = 0.032\n",
      "Epoch 154  •  val top-1 = 0.032\n",
      "Epoch 155  •  val top-1 = 0.032\n",
      "Epoch 156  •  val top-1 = 0.032\n",
      "Epoch 157  •  val top-1 = 0.032\n",
      "Epoch 158  •  val top-1 = 0.032\n",
      "Epoch 159  •  val top-1 = 0.032\n",
      "Epoch 160  •  val top-1 = 0.032\n",
      "Epoch 161  •  val top-1 = 0.032\n",
      "Epoch 162  •  val top-1 = 0.032\n",
      "Epoch 163  •  val top-1 = 0.032\n",
      "Epoch 164  •  val top-1 = 0.032\n",
      "Epoch 165  •  val top-1 = 0.032\n",
      "Epoch 166  •  val top-1 = 0.032\n",
      "Epoch 167  •  val top-1 = 0.032\n",
      "Epoch 168  •  val top-1 = 0.032\n",
      "Epoch 169  •  val top-1 = 0.032\n",
      "Epoch 170  •  val top-1 = 0.032\n",
      "Epoch 171  •  val top-1 = 0.032\n",
      "Epoch 172  •  val top-1 = 0.032\n",
      "Epoch 173  •  val top-1 = 0.032\n",
      "Epoch 174  •  val top-1 = 0.032\n",
      "Epoch 175  •  val top-1 = 0.032\n",
      "Epoch 176  •  val top-1 = 0.032\n",
      "Epoch 177  •  val top-1 = 0.032\n",
      "Epoch 178  •  val top-1 = 0.032\n",
      "Epoch 179  •  val top-1 = 0.032\n",
      "Epoch 180  •  val top-1 = 0.032\n",
      "Epoch 181  •  val top-1 = 0.032\n",
      "Epoch 182  •  val top-1 = 0.032\n",
      "Epoch 183  •  val top-1 = 0.032\n",
      "Epoch 184  •  val top-1 = 0.032\n",
      "Epoch 185  •  val top-1 = 0.032\n",
      "Epoch 186  •  val top-1 = 0.032\n",
      "Epoch 187  •  val top-1 = 0.032\n",
      "Epoch 188  •  val top-1 = 0.032\n",
      "Epoch 189  •  val top-1 = 0.032\n",
      "Epoch 190  •  val top-1 = 0.032\n",
      "Epoch 191  •  val top-1 = 0.032\n",
      "Epoch 192  •  val top-1 = 0.032\n",
      "Epoch 193  •  val top-1 = 0.032\n",
      "Epoch 194  •  val top-1 = 0.032\n",
      "Epoch 195  •  val top-1 = 0.032\n",
      "Epoch 196  •  val top-1 = 0.032\n",
      "Epoch 197  •  val top-1 = 0.032\n",
      "Epoch 198  •  val top-1 = 0.032\n",
      "Epoch 199  •  val top-1 = 0.032\n",
      "Epoch 200  •  val top-1 = 0.032\n",
      "Epoch 201  •  val top-1 = 0.032\n",
      "Epoch 202  •  val top-1 = 0.032\n",
      "Epoch 203  •  val top-1 = 0.032\n",
      "Epoch 204  •  val top-1 = 0.032\n",
      "Epoch 205  •  val top-1 = 0.032\n",
      "Epoch 206  •  val top-1 = 0.032\n",
      "Epoch 207  •  val top-1 = 0.032\n",
      "Epoch 208  •  val top-1 = 0.032\n",
      "Epoch 209  •  val top-1 = 0.032\n",
      "Epoch 210  •  val top-1 = 0.032\n",
      "Epoch 211  •  val top-1 = 0.032\n",
      "Epoch 212  •  val top-1 = 0.032\n",
      "Epoch 213  •  val top-1 = 0.032\n",
      "Epoch 214  •  val top-1 = 0.032\n",
      "Epoch 215  •  val top-1 = 0.032\n",
      "Epoch 216  •  val top-1 = 0.032\n",
      "Epoch 217  •  val top-1 = 0.032\n",
      "Epoch 218  •  val top-1 = 0.032\n",
      "Epoch 219  •  val top-1 = 0.032\n",
      "Epoch 220  •  val top-1 = 0.032\n",
      "Epoch 221  •  val top-1 = 0.032\n",
      "Epoch 222  •  val top-1 = 0.032\n",
      "Epoch 223  •  val top-1 = 0.032\n",
      "Epoch 224  •  val top-1 = 0.032\n",
      "Epoch 225  •  val top-1 = 0.032\n",
      "Epoch 226  •  val top-1 = 0.032\n",
      "Epoch 227  •  val top-1 = 0.032\n",
      "Epoch 228  •  val top-1 = 0.032\n",
      "Epoch 229  •  val top-1 = 0.032\n",
      "Epoch 230  •  val top-1 = 0.032\n",
      "Epoch 231  •  val top-1 = 0.032\n",
      "Epoch 232  •  val top-1 = 0.032\n",
      "Epoch 233  •  val top-1 = 0.032\n",
      "Epoch 234  •  val top-1 = 0.032\n",
      "Epoch 235  •  val top-1 = 0.032\n",
      "Epoch 236  •  val top-1 = 0.032\n",
      "Epoch 237  •  val top-1 = 0.032\n",
      "Epoch 238  •  val top-1 = 0.032\n",
      "Epoch 239  •  val top-1 = 0.032\n",
      "Epoch 240  •  val top-1 = 0.032\n",
      "Epoch 241  •  val top-1 = 0.032\n",
      "Epoch 242  •  val top-1 = 0.032\n",
      "Epoch 243  •  val top-1 = 0.032\n",
      "Epoch 244  •  val top-1 = 0.032\n",
      "Epoch 245  •  val top-1 = 0.032\n",
      "Epoch 246  •  val top-1 = 0.032\n",
      "Epoch 247  •  val top-1 = 0.032\n",
      "Epoch 248  •  val top-1 = 0.032\n",
      "Epoch 249  •  val top-1 = 0.032\n",
      "Epoch 250  •  val top-1 = 0.032\n",
      "✅ Fine-tuning complete; model saved to C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\output_model_small_session_split_data\\clip_kong_finetune\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# fine_tune_clip_reward.py\n",
    "# ----------------------------------------------------------\n",
    "# Fine-tune CLIP ViT-L/14 on your labelled Kong-centering data\n",
    "# ----------------------------------------------------------\n",
    "import os, random, json, torch\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import ToPILImage\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "from transformers import TrainerCallback\n",
    "import config\n",
    "from math import ceil\n",
    "import torch.nn.functional as F\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from reward_dataset import RewardDatasetSingleFrame  # <- merges main & label CSVs\n",
    "\n",
    "# ------------------------------ Hyper-params --------------------------------\n",
    "MODEL_NAME        = \"openai/clip-vit-large-patch14\"   # \n",
    "# MODEL_NAME        = \"openai/clip-vit-base-patch32\"   # \n",
    "POS_PROMPT        = \"a red Kong dog toy centered in the frame\"\n",
    "NEG_PROMPT        = \"an empty kitchen floor with no toy\"\n",
    "\n",
    "POS_THRESH        = 0.6      # reward ≥ thresh → positive\n",
    "BATCH_SIZE        =  64\n",
    "LR                = 1e-5\n",
    "EPOCHS            =  250\n",
    "FREEZE_BACKBONES  = True    # set True if GPU RAM limited\n",
    "OUTPUT_DIR        = os.path.join(config.OUTPUT_DIR, \"clip_kong_finetune\")\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "device = torch.device(config.DEVICE)\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def clip_val_top1(trainer):\n",
    "    val_loader = trainer.get_eval_dataloader()\n",
    "    hits = total = 0\n",
    "    model = trainer.model.eval()\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**batch)\n",
    "        preds = out.logits_per_image.argmax(dim=1)\n",
    "        hits  += (preds == torch.arange(len(preds), device=preds.device)).sum().item()\n",
    "        total += len(preds)\n",
    "    return hits / total if total else 0.0\n",
    "\n",
    "class Top1Callback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref):\n",
    "        self.trainer_ref = trainer_ref          # stash a pointer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        top1 = clip_val_top1(self.trainer_ref)\n",
    "        print(f\"Epoch {int(state.epoch)}  •  val top-1 = {top1:.3f}\")\n",
    "\n",
    "class CLIPContrastiveTrainer(Trainer):\n",
    "    \"\"\"Trainer that computes the standard CLIP InfoNCE loss.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text  = outputs.logits_per_text\n",
    "\n",
    "        batch_size = logits_per_image.size(0)\n",
    "        labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "\n",
    "        loss_i = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_t = F.cross_entropy(logits_per_text,  labels)\n",
    "        loss   = (loss_i + loss_t) / 2\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --------------------------- Base labelled dataset --------------------------\n",
    "base_ds = RewardDatasetSingleFrame(\n",
    "    main_csv_path = config.CSV_PATH,\n",
    "    reward_csv_path = config.MANUAL_COLLECTED_REWARD_CSV,\n",
    "    data_dir = config.DATA_DIR,\n",
    "    image_size = config.IMAGE_SIZE,\n",
    "    transform = None                      # we want raw PIL later\n",
    ")\n",
    "\n",
    "# --------------------------- Wrapper for CLIP -------------------------------\n",
    "class CLIPFinetuneDataset(Dataset):\n",
    "    \"\"\"Turns the labelled frame → reward pairs into (PIL, text) pairs for CLIP.\"\"\"\n",
    "    def __init__(self, reward_ds, pos_thresh, pos_prompt, neg_prompt):\n",
    "        self.ds          = reward_ds\n",
    "        self.pos_thresh  = pos_thresh\n",
    "        self.pos_prompt  = pos_prompt\n",
    "        self.neg_prompt  = neg_prompt\n",
    "        # Cache image paths + reward to avoid extra openings in __getitem__\n",
    "        self.records = reward_ds.labeled_data_df.copy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.records.iloc[idx]\n",
    "        image_path = os.path.join(self.ds.data_dir, row[\"image_path\"])\n",
    "        reward_val = row[\"assigned_reward\"]\n",
    "        prompt     = self.pos_prompt if reward_val >= self.pos_thresh else self.neg_prompt\n",
    "        # PIL read here; no transforms (CLIPProcessor handles resize / norm)\n",
    "        from PIL import Image\n",
    "        pil_img = Image.open(image_path).convert(\"RGB\")\n",
    "        return {\"image\": pil_img, \"text\": prompt}\n",
    "\n",
    "clip_ds = CLIPFinetuneDataset(base_ds, POS_THRESH, POS_PROMPT, NEG_PROMPT)\n",
    "\n",
    "# --------------------------- Train / val split ------------------------------\n",
    "VAL_SPLIT = 0.1\n",
    "val_size  = int(len(clip_ds) * VAL_SPLIT)\n",
    "train_size= len(clip_ds) - val_size\n",
    "torch.manual_seed(42)\n",
    "train_ds, val_ds = random_split(clip_ds, [train_size, val_size])\n",
    "\n",
    "print(f\"train={len(train_ds)}, val={len(val_ds)}\")\n",
    "\n",
    "# --------------------------- Model & processor ------------------------------\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "model      = CLIPModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "if FREEZE_BACKBONES:\n",
    "    for n, p in model.named_parameters():\n",
    "        if not n.startswith((\"text_projection\", \"visual_projection\", \"logit_scale\")):\n",
    "            p.requires_grad = False\n",
    "    print(\"Froze ViT & text backbones; training projection layers only.\")\n",
    "\n",
    "# --------------------------- Data collator ----------------------------------\n",
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    texts  = [item[\"text\"]  for item in batch]\n",
    "    return processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# --------------------------- Training setup ---------------------------------\n",
    "steps_per_epoch = ceil(len(train_ds) / BATCH_SIZE)         # for save/eval\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir                   = OUTPUT_DIR,\n",
    "    per_device_train_batch_size  = BATCH_SIZE,\n",
    "    per_device_eval_batch_size   = BATCH_SIZE,\n",
    "    learning_rate                = LR,\n",
    "    num_train_epochs             = EPOCHS,\n",
    "    fp16                         = True,\n",
    "    logging_steps                = 50,            # still supported\n",
    "    save_steps                   = steps_per_epoch,    # checkpoint each epoch\n",
    "    eval_steps                   = steps_per_epoch,    # run eval each epoch\n",
    "    save_total_limit             = 3,             # keep last 3 ckpts\n",
    "    remove_unused_columns        = False,\n",
    ")\n",
    "\n",
    "trainer = CLIPContrastiveTrainer(        # <-- use subclass\n",
    "    model           = model,\n",
    "    args            = args,\n",
    "    train_dataset   = train_ds,\n",
    "    eval_dataset    = val_ds,\n",
    "    data_collator   = collate_fn,\n",
    ")\n",
    "\n",
    "trainer.add_callback(Top1Callback(trainer))\n",
    "# --------------------------- Train! -----------------------------------------\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"ckpt-final\"))\n",
    "print(\"✅ Fine-tuning complete; model saved to\", OUTPUT_DIR)\n",
    "\n",
    "# --------------------------- Reward helper ----------------------------------\n",
    "# After training you can create a one-liner reward function:\n",
    "#\n",
    "# ft_model = CLIPModel.from_pretrained(OUTPUT_DIR + \"/ckpt-final\").eval().to(device)\n",
    "# with torch.no_grad():\n",
    "#     pos_emb = ft_model.get_text_features(**processor(text=POS_PROMPT,\n",
    "#                                                      return_tensors=\"pt\").to(device)\n",
    "#                   ).float().norm(dim=-1)\n",
    "# def finetuned_reward(pil_img):\n",
    "#     img_emb = ft_model.get_image_features(**processor(images=pil_img,\n",
    "#                                                       return_tensors=\"pt\").to(device)\n",
    "#                   ).float().norm(dim=-1)\n",
    "#     return (img_emb @ pos_emb.T).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c88e6-2119-40cf-ac26-afca3edd00b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
