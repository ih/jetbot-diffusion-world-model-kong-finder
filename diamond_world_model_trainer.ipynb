{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c62f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0699dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# get_ipython().system('pip install wandb')\n",
    "\n",
    "# \n",
    "# \n",
    "# get_ipython().system('pip install --upgrade typing_extensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6d62c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import datetime # For epoch timing and timestamping\n",
    "from torchvision import transforms\n",
    "from collections import deque # For moving average\n",
    "from dataclasses import dataclass \n",
    "from typing import List, Optional, Dict, Any \n",
    "import random\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import copy # Make sure to import copy at the top of the file\n",
    "\n",
    "import wandb # Will be initialized in _main_training\n",
    "\n",
    "# Your project's specific imports\n",
    "import config # Your config.py\n",
    "import models # Your models.py (which should import from diamond_models.ipynb)\n",
    "\n",
    "# Import dataset from your jetbot_dataset.ipynb\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from jetbot_dataset import JetbotDataset, filter_dataset_by_action \n",
    "\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "print(\"Imports successful.\")\n",
    "\n",
    "# DEVICE will be set in _main_training or used directly from config by other functions\n",
    "# Global config constants that might be used by imported functions like train_diamond_model\n",
    "# if they don't re-fetch from config themselves (they mostly do, but being safe).\n",
    "DM_IMG_CHANNELS = getattr(config, 'DM_IMG_CHANNELS', 3)\n",
    "DM_NUM_ACTIONS = getattr(config, 'DM_NUM_ACTIONS', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f06e05",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_dataset():\n",
    "    full_dataset = JetbotDataset(\n",
    "        csv_path=config.CSV_PATH,\n",
    "        data_dir=config.DATA_DIR,\n",
    "        image_size=config.IMAGE_SIZE,\n",
    "        num_prev_frames=config.NUM_PREV_FRAMES,\n",
    "        transform=config.TRANSFORM\n",
    "    )\n",
    "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "    split_file_path = os.path.join(config.OUTPUT_DIR, getattr(config, 'SPLIT_DATASET_FILENAME', 'dataset_split.pth'))\n",
    "    if os.path.exists(split_file_path):\n",
    "        print(f\"Loading dataset split from {split_file_path}\")\n",
    "        split_data = torch.load(split_file_path)\n",
    "        train_indices, val_indices = split_data['train_indices'], split_data['val_indices']\n",
    "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    else:\n",
    "        print(\"Creating new train/val split...\")\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(total_size * 0.9)\n",
    "        val_size = total_size - train_size\n",
    "        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size]) # Using torch.random_split by default\n",
    "        torch.save({\n",
    "            'train_indices': train_dataset.indices,\n",
    "            'val_indices': val_dataset.indices,\n",
    "        }, split_file_path)\n",
    "        print(f\"Saved new dataset split to {split_file_path}\")\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee323a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization helpers defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tensor_to_pil(tensor_img):\n",
    "    tensor_img = (tensor_img.clamp(-1, 1) + 1) / 2\n",
    "    tensor_img = tensor_img.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    if tensor_img.shape[2] == 1:\n",
    "        tensor_img = tensor_img.squeeze(2)\n",
    "    # Ensure array is writeable for PIL\n",
    "    if not tensor_img.flags.writeable:\n",
    "        tensor_img = np.ascontiguousarray(tensor_img)\n",
    "    if tensor_img.dtype != np.uint8: # This check might be problematic if tensor_img is already uint8\n",
    "        pil_img_array = (tensor_img * 255).astype(np.uint8)\n",
    "    else:\n",
    "        pil_img_array = tensor_img # Already uint8\n",
    "    pil_img = PILImage.fromarray(pil_img_array)\n",
    "    return pil_img\n",
    "\n",
    "def save_visualization_samples(generated_tensor, gt_current_tensor, gt_prev_frames_sequence, epoch, save_dir, prefix=\"val_vis\"):\n",
    "    \"\"\"\n",
    "    Saves a visualization comparing a single generated image, its corresponding GT current image,\n",
    "    and the sequence of GT previous frames.\n",
    "    - generated_tensor, gt_current_tensor: [C, H, W]\n",
    "    - gt_prev_frames_sequence: [NumPrev, C, H, W]\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    generated_tensor = generated_tensor.detach().cpu()\n",
    "    gt_current_tensor = gt_current_tensor.detach().cpu()\n",
    "    gt_prev_frames_sequence = gt_prev_frames_sequence.detach().cpu()\n",
    "\n",
    "    num_prev_frames = config.NUM_PREV_FRAMES # Get from global config\n",
    "\n",
    "    num_cols = num_prev_frames + 1  # N previous frames + 1 current GT\n",
    "    # Create a 2 rows, num_cols columns subplot\n",
    "    fig, axs = plt.subplots(2, num_cols, figsize=(num_cols * 3, 6), squeeze=False) # squeeze=False ensures axs is always 2D\n",
    "\n",
    "    try:\n",
    "        # Top row: Previous GT frames and Current GT frame\n",
    "        for i in range(num_prev_frames):\n",
    "            axs[0, i].imshow(tensor_to_pil(gt_prev_frames_sequence[i]))\n",
    "            axs[0, i].set_title(f\"GT Prev {i+1}\")\n",
    "            axs[0, i].axis('off')\n",
    "            axs[1, i].axis('off') # Keep bottom row empty under previous GT frames\n",
    "\n",
    "        axs[0, num_prev_frames].imshow(tensor_to_pil(gt_current_tensor))\n",
    "        axs[0, num_prev_frames].set_title(\"GT Current\")\n",
    "        axs[0, num_prev_frames].axis('off')\n",
    "\n",
    "        # Bottom row, last column: Generated frame (aligned under Current GT)\n",
    "        axs[1, num_prev_frames].imshow(tensor_to_pil(generated_tensor))\n",
    "        axs[1, num_prev_frames].set_title(\"Generated\")\n",
    "        axs[1, num_prev_frames].axis('off')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing image for prefix {prefix}, epoch {epoch}: {e}\")\n",
    "        # Clear figure and display error text\n",
    "        for r in range(axs.shape[0]):\n",
    "            for c in range(axs.shape[1]):\n",
    "                axs[r,c].axis('off')\n",
    "        fig.clear() \n",
    "        plt.text(0.5, 0.5, \"Error displaying image\", ha=\"center\", va=\"center\", transform=fig.transFigure)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f\"{prefix}_epoch_{epoch:04d}.png\") \n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "    return save_path\n",
    "    \n",
    "def prepare_single_sample_for_sampler(sample_data, device):\n",
    "    target_img, action_single, prev_frames_flat_unbatched = sample_data # prev_frames_flat_unbatched is [NumPrev*C, H, W]\n",
    "    \n",
    "    # Add batch dimension (B=1) and move to device\n",
    "    gt_current_frame_batch = target_img.unsqueeze(0).to(device) # Shape: [1, C, H, W]\n",
    "    action_single_batch = action_single.unsqueeze(0).to(device) # Shape: [1, 1]\n",
    "    # prev_frames_flat_for_sampler_input needs to be [B, NumPrev*C, H, W] for the view later if used directly by sampler\n",
    "    # but for DIAMOND sampler, prev_obs is [B, NumPrevFrames, C, H, W]\n",
    "    \n",
    "    num_prev_frames_const = config.NUM_PREV_FRAMES\n",
    "    img_channels_const = DM_IMG_CHANNELS # Assumes DM_IMG_CHANNELS is globally available or from config\n",
    "    img_h_const = config.IMAGE_SIZE\n",
    "    img_w_const = config.IMAGE_SIZE\n",
    "\n",
    "    # Reshape prev_frames_flat_unbatched for sampler input [1, NumPrev, C, H, W]\n",
    "    prev_obs_for_sampler_input_5d = prev_frames_flat_unbatched.view(\n",
    "        num_prev_frames_const,\n",
    "        img_channels_const,\n",
    "        img_h_const,\n",
    "        img_w_const\n",
    "    ).unsqueeze(0).to(device) # Add batch dim and send to device\n",
    "\n",
    "    action_sequence_for_sampler = action_single_batch.repeat(1, config.NUM_PREV_FRAMES).long()\n",
    "    \n",
    "    # For visualization, we want the GT previous frames, unbatched and sequenced: [NumPrev, C, H, W]\n",
    "    gt_prev_frames_seq_for_vis = prev_frames_flat_unbatched.view(\n",
    "        num_prev_frames_const,\n",
    "        img_channels_const,\n",
    "        img_h_const,\n",
    "        img_w_const\n",
    "    ) # This is already on CPU if sample_data came directly from dataset before .to(device)\n",
    "      # It will be detached and moved to CPU again in save_visualization_samples\n",
    "    \n",
    "    return prev_obs_for_sampler_input_5d, action_sequence_for_sampler, gt_current_frame_batch, gt_prev_frames_seq_for_vis\n",
    "\n",
    "print(\"Visualization helpers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b097310c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation epoch functions adapted for Batch object and Denoiser.forward.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_denoiser_epoch(denoiser_model, train_dl, opt, scheduler, grad_clip_val, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, train_step_start=0):\n",
    "    denoiser_model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_dl, desc=f\"Epoch {epoch_num_for_log} [Train]\", leave=False)\n",
    "\n",
    "    num_prev_frames = config.NUM_PREV_FRAMES\n",
    "    c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
    "    accumulation_steps = config.ACCUMULATION_STEPS\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        if isinstance(batch, models.Batch):\n",
    "            current_batch_obj = batch.to(device)\n",
    "        else:\n",
    "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
    "            current_batch_size = target_img_batch.shape[0]\n",
    "            target_img_batch = target_img_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
    "\n",
    "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
    "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
    "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()\n",
    "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
    "        \n",
    "            # Corrected Batch instantiation\n",
    "            current_batch_obj = models.Batch(obs=batch_obs_tensor, act=batch_act_tensor, mask_padding=batch_mask_padding, info=[{}] * current_batch_size)\n",
    "\n",
    "        loss, logs = denoiser_model(current_batch_obj)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            if grad_clip_val > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(denoiser_model.parameters(), grad_clip_val)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item() * accumulation_steps, \"LR\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "        # Restored wandb logging\n",
    "        if batch_idx % 10 == 0:\n",
    "            train_step = train_step_start + batch_idx\n",
    "            wandb.log({\n",
    "                \"train_batch_loss\": loss.item() * accumulation_steps, # Log un-normalized loss\n",
    "                \"train_batch_denoising_loss\": logs.get(\"loss_denoising\"),\n",
    "                \"train_step\": train_step,\n",
    "            })\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl) if len(train_dl) > 0 else 0.0\n",
    "    final_step = train_step_start + len(train_dl)\n",
    "    return avg_loss, final_step\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_denoiser_epoch(denoiser_model, val_dl, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, val_step_start=0):\n",
    "    denoiser_model.eval()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(val_dl, desc=f\"Epoch {epoch_num_for_log} [Valid]\", leave=False)\n",
    "    num_prev_frames = config.NUM_PREV_FRAMES\n",
    "    c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        if isinstance(batch, models.Batch):\n",
    "            current_batch_obj = batch.to(device)\n",
    "        else:\n",
    "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
    "            current_batch_size = target_img_batch.shape[0]\n",
    "            target_img_batch = target_img_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
    "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
    "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
    "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()\n",
    "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
    "        \n",
    "            # Corrected Batch instantiation\n",
    "            current_batch_obj = models.Batch(obs=batch_obs_tensor, act=batch_act_tensor, mask_padding=batch_mask_padding, info=[{}] * current_batch_size)\n",
    "        \n",
    "        loss, logs = denoiser_model(current_batch_obj)\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Val Loss\": loss.item()})\n",
    "        \n",
    "        # Restored wandb logging\n",
    "        if batch_idx % 10 == 0:\n",
    "            val_step = val_step_start + batch_idx\n",
    "            wandb.log({\n",
    "                \"val_batch_loss\": loss.item(),\n",
    "                \"val_batch_denoising_loss\": logs.get(\"loss_denoising\"),\n",
    "                \"val_step\": val_step,\n",
    "            })\n",
    "             \n",
    "    avg_loss = total_loss / len(val_dl) if len(val_dl) > 0 else 0.0\n",
    "    final_step = val_step_start + len(val_dl)\n",
    "    return avg_loss, final_step\n",
    "\n",
    "\n",
    "print(\"Training and validation epoch functions adapted for Batch object and Denoiser.forward.\")\n",
    "\n",
    "\n",
    "def train_diamond_model(train_loader, val_loader, start_checkpoint=None, max_steps=None):\n",
    "    \"\"\"\n",
    "    Train a denoiser model with robust, step-based early stopping.\n",
    "    \"\"\"\n",
    "    device = config.DEVICE\n",
    "    num_steps = max_steps or config.NUM_TRAIN_STEPS\n",
    "\n",
    "    # --- Model and Optimizer Setup (remains the same) ---\n",
    "    inner_cfg = models.InnerModelConfig(\n",
    "        img_channels=config.DM_IMG_CHANNELS,\n",
    "        num_steps_conditioning=config.NUM_PREV_FRAMES,\n",
    "        cond_channels=config.DM_COND_CHANNELS,\n",
    "        depths=config.DM_UNET_DEPTHS,\n",
    "        channels=config.DM_UNET_CHANNELS,\n",
    "        attn_depths=config.DM_UNET_ATTN_DEPTHS,\n",
    "        num_actions=config.DM_NUM_ACTIONS,\n",
    "        is_upsampler=config.DM_IS_UPSAMPLER,\n",
    "    )\n",
    "    denoiser_cfg = models.DenoiserConfig(\n",
    "        inner_model=inner_cfg,\n",
    "        sigma_data=config.DM_SIGMA_DATA,\n",
    "        sigma_offset_noise=config.DM_SIGMA_OFFSET_NOISE,\n",
    "        noise_previous_obs=config.DM_NOISE_PREVIOUS_OBS,\n",
    "        upsampling_factor=config.DM_UPSAMPLING_FACTOR,\n",
    "    )\n",
    "    denoiser = models.Denoiser(cfg=denoiser_cfg).to(device)\n",
    "    sigma_cfg = models.SigmaDistributionConfig(\n",
    "        loc=config.DM_SIGMA_P_MEAN,\n",
    "        scale=config.DM_SIGMA_P_STD,\n",
    "        sigma_min=config.DM_SIGMA_MIN_TRAIN,\n",
    "        sigma_max=config.DM_SIGMA_MAX_TRAIN,\n",
    "    )\n",
    "    denoiser.setup_training(sigma_cfg)\n",
    "\n",
    "    start_step_offset = -1\n",
    "    if start_checkpoint and os.path.exists(start_checkpoint):\n",
    "        state = torch.load(start_checkpoint, map_location=device)\n",
    "        if 'model_state_dict' in state:\n",
    "            denoiser.load_state_dict(state['model_state_dict'])\n",
    "        start_step_offset = state.get('step', -1)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        denoiser.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=config.LEARNING_RATE_WEIGHT_DECAY,\n",
    "        eps=config.LEARNING_RATE_EPS,\n",
    "    )\n",
    "\n",
    "    def lr_lambda(step: int):\n",
    "        warmup = config.LEARNING_RATE_WARMUP_STEPS\n",
    "        return float(step) / float(max(1, warmup)) if step < warmup else 1.0\n",
    "    scheduler = LambdaLR(opt, lr_lambda)\n",
    "    \n",
    "    # --- Robust Early Stopping & Checkpointing Setup ---\n",
    "    best_val_loss = float('inf')\n",
    "    steps_since_last_improvement = 0\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    validate_every = getattr(config, 'VALIDATE_EVERY', 50)\n",
    "    patience_steps = getattr(config, 'EARLY_STOP_PATIENCE_STEPS', 150)\n",
    "    \n",
    "    # Divergence Guard Setup\n",
    "    divergence_patience = getattr(config, 'TRAIN_DIVERGE_PATIENCE_CHECKS', 3)\n",
    "    divergence_threshold = getattr(config, 'TRAIN_DIVERGE_THRESHOLD', 0.05)\n",
    "    last_train_loss = float('inf')\n",
    "    divergence_counter = 0\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    val_step_count = 0\n",
    "    train_iter = iter(train_loader)\n",
    "    pbar = tqdm(range(num_steps), desc=\"Incremental Training Steps\")\n",
    "\n",
    "    # Sampler for visualization (similar to _main_training)\n",
    "    sampler_cfg_vis = models.DiffusionSamplerConfig(\n",
    "        num_steps_denoising=config.SAMPLER_NUM_STEPS,\n",
    "        sigma_min=config.SAMPLER_SIGMA_MIN,\n",
    "        sigma_max=config.SAMPLER_SIGMA_MAX,\n",
    "        rho=config.SAMPLER_RHO,\n",
    "        order=config.SAMPLER_ORDER,\n",
    "        s_churn=config.SAMPLER_S_CHURN,\n",
    "        s_tmin=config.SAMPLER_S_TMIN,\n",
    "        s_tmax=config.SAMPLER_S_TMAX,\n",
    "        s_noise=config.SAMPLER_S_NOISE\n",
    "    )\n",
    "    diffusion_sampler_vis = models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg_vis)\n",
    "\n",
    "    # Prepare filtered validation subsets for visualization (similar to _main_training)\n",
    "    val_stopped_subset_inc = []\n",
    "    val_moving_subset_inc = []\n",
    "    if hasattr(val_loader, 'dataset') and len(val_loader.dataset) > 0:\n",
    "        val_dataset_for_filter = val_loader.dataset\n",
    "        val_stopped_subset_inc = filter_dataset_by_action(val_dataset_for_filter, target_actions=0.0)\n",
    "        moving_action_val_vis_inc = getattr(config, 'MOVING_ACTION_VALUE_FOR_VIS', 0.13)\n",
    "        val_moving_subset_inc = filter_dataset_by_action(val_dataset_for_filter, target_actions=moving_action_val_vis_inc)\n",
    "    \n",
    "    for step in pbar:\n",
    "        # --- Standard Training Step (remains the same) ---\n",
    "        # (Batch creation, forward pass, backward pass, optimizer step)\n",
    "        try:\n",
    "            batch = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            batch = next(train_iter)\n",
    "\n",
    "        # (Code to prepare batch object `current_batch_obj` remains the same)\n",
    "        if isinstance(batch, models.Batch):\n",
    "            current_batch_obj = batch.to(device)\n",
    "        else:\n",
    "            # Unpack the batch from the DataLoader\n",
    "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
    "\n",
    "            # Move tensors to the correct device\n",
    "            target_img_batch = target_img_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
    "\n",
    "            # Reconstruct the logic from train_denoiser_epoch to create the Batch object\n",
    "            current_batch_size = target_img_batch.shape[0]\n",
    "            num_prev_frames = config.NUM_PREV_FRAMES\n",
    "            c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
    "\n",
    "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
    "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
    "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()  # Ensure this matches the expected action format for the model\n",
    "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
    "\n",
    "            current_batch_obj = models.Batch(\n",
    "                obs=batch_obs_tensor,\n",
    "                act=batch_act_tensor,\n",
    "                mask_padding=batch_mask_padding,\n",
    "                info=[{}] * current_batch_size\n",
    "            )\n",
    "\n",
    "        denoiser.train()\n",
    "        loss, logs = denoiser(current_batch_obj) \n",
    "        train_loss_val = loss.item()\n",
    "        loss = loss / config.ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "            if config.GRAD_CLIP_VALUE > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(denoiser.parameters(), config.GRAD_CLIP_VALUE)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # Logging to wandb (more frequently for steps)\n",
    "        train_step_count = start_step_offset + step + 1 # Define train_step_count here\n",
    "        if wandb.run and (step + 1) % 10 == 0: # Log every 10 steps\n",
    "            wandb.log({\n",
    "                \"incremental_step_train_loss\": train_loss_val,\n",
    "                \"incremental_step_denoising_loss\": logs.get(\"loss_denoising\"),\n",
    "                \"incremental_step_learning_rate\": scheduler.get_last_lr()[0],\n",
    "                \"train_step\": train_step_count,\n",
    "            })\n",
    "        \n",
    "        # --- Validation, Early Stopping, and Divergence Check ---\n",
    "        if (step + 1) % validate_every == 0 or (step + 1) == num_steps:\n",
    "            val_step_start = val_step_count\n",
    "            current_val_loss, val_step_count = validate_denoiser_epoch(\n",
    "                denoiser, val_loader, device, step + 1, 0, 0, val_step_start=val_step_start\n",
    "            )\n",
    "\n",
    "            # Log validation loss\n",
    "            if wandb.run:\n",
    "                wandb.log({\"incremental_eval_val_loss\": current_val_loss, \"val_step\": val_step_start})\n",
    "            \n",
    "            # Image Sampling (similar to _main_training, simplified for step-based)\n",
    "            # Tied to validation frequency for now.\n",
    "            if wandb.run and hasattr(config, 'SAMPLE_EVERY') and (step + 1) % config.SAMPLE_EVERY == 0 :\n",
    "                denoiser.eval()\n",
    "                vis_wandb_log_data_inc = {}\n",
    "                fixed_sample_idx_inc = getattr(config, 'FIXED_VIS_SAMPLE_IDX', 0)\n",
    "\n",
    "                if hasattr(val_loader, 'dataset') and fixed_sample_idx_inc < len(val_loader.dataset):\n",
    "                    fixed_sample_data_inc = val_loader.dataset[fixed_sample_idx_inc]\n",
    "                    # Ensure sample_data is a tuple (img, act, prev_frames_flat)\n",
    "                    if not (isinstance(fixed_sample_data_inc, tuple) and len(fixed_sample_data_inc) == 3):\n",
    "                         # Try to get it from .dataset if val_loader.dataset is a Subset\n",
    "                        if isinstance(val_loader.dataset, torch.utils.data.Subset):\n",
    "                            original_dataset = val_loader.dataset.dataset\n",
    "                            original_idx = val_loader.dataset.indices[fixed_sample_idx_inc]\n",
    "                            fixed_sample_data_inc = original_dataset[original_idx]\n",
    "                        else:\n",
    "                            print(f\"Skipping fixed sample visualization: data format error or direct access failed.\")\n",
    "                            fixed_sample_data_inc = None \n",
    "                            \n",
    "                    if fixed_sample_data_inc:\n",
    "                        prev_obs_fixed_inc, prev_act_fixed_inc, gt_fixed_batch_inc, gt_prev_frames_fixed_seq_inc = prepare_single_sample_for_sampler(fixed_sample_data_inc, device)\n",
    "                        with torch.no_grad():\n",
    "                            generated_output_tuple_fixed_inc = diffusion_sampler_vis.sample(prev_obs=prev_obs_fixed_inc, prev_act=prev_act_fixed_inc)\n",
    "                        if generated_output_tuple_fixed_inc:\n",
    "                            generated_image_to_save_fixed_inc = generated_output_tuple_fixed_inc[0][0]\n",
    "                            gt_image_to_save_fixed_inc = gt_fixed_batch_inc[0]\n",
    "                            vis_path_fixed_inc = save_visualization_samples(\n",
    "                                generated_image_to_save_fixed_inc, gt_image_to_save_fixed_inc, gt_prev_frames_fixed_seq_inc,\n",
    "                                step + 1, config.SAMPLE_DIR, prefix=f\"inc_vis_fixed_step{step+1}\"\n",
    "                            )\n",
    "                            vis_wandb_log_data_inc[f\"incremental_samples/fixed_idx_{fixed_sample_idx_inc}\"] = wandb.Image(vis_path_fixed_inc, caption=f\"Step {step+1} Fixed Sample\")\n",
    "\n",
    "                # Simplified: Add one random sample from val_stopped_subset_inc if available\n",
    "                if len(val_stopped_subset_inc) > 0:\n",
    "                    stopped_sample_data_inc = val_stopped_subset_inc[random.randint(0, len(val_stopped_subset_inc) - 1)]\n",
    "                    prev_obs_stop, prev_act_stop, gt_batch_stop, gt_prev_seq_stop = prepare_single_sample_for_sampler(stopped_sample_data_inc, device)\n",
    "                    with torch.no_grad():\n",
    "                        gen_out_stop = diffusion_sampler_vis.sample(prev_obs=prev_obs_stop, prev_act=prev_act_stop)\n",
    "                    if gen_out_stop:\n",
    "                        vis_path_stop = save_visualization_samples(gen_out_stop[0][0], gt_batch_stop[0], gt_prev_seq_stop, step+1, config.SAMPLE_DIR, prefix=f\"inc_vis_stopped_step{step+1}\")\n",
    "                        vis_wandb_log_data_inc[\"incremental_samples/random_stopped\"] = wandb.Image(vis_path_stop, caption=f\"Step {step+1} Random Stopped\")\n",
    "\n",
    "                # Simplified: Add one random sample from val_moving_subset_inc_subset_inc if available\n",
    "                if len(val_moving_subset_inc) > 0:\n",
    "                    moving_sample_data_inc = val_moving_subset_inc[random.randint(0, len(val_moving_subset_inc) - 1)]\n",
    "                    prev_obs_move, prev_act_move, gt_batch_move, gt_prev_seq_move = prepare_single_sample_for_sampler(moving_sample_data_inc, device)\n",
    "                    with torch.no_grad():\n",
    "                        gen_out_move = diffusion_sampler_vis.sample(prev_obs=prev_obs_move, prev_act=prev_act_move)\n",
    "                    if gen_out_move:\n",
    "                        vis_path_move = save_visualization_samples(gen_out_move[0][0], gt_batch_move[0], gt_prev_seq_move, step+1, config.SAMPLE_DIR, prefix=f\"inc_vis_moving_step{step+1}\")\n",
    "                        vis_wandb_log_data_inc[\"incremental_samples/random_moving\"] = wandb.Image(vis_path_move, caption=f\"Step {step+1} Random Moving\")\n",
    "                \n",
    "                if vis_wandb_log_data_inc:\n",
    "                    vis_wandb_log_data_inc[\"train_step\"] = train_step_count # Use the same train_step_count\n",
    "                    wandb.log(vis_wandb_log_data_inc)\n",
    "                denoiser.train() # Set back to train mode\n",
    "\n",
    "            # Check for improvement\n",
    "            if current_val_loss < best_val_loss:\n",
    "                best_val_loss = current_val_loss\n",
    "                steps_since_last_improvement = 0\n",
    "                best_model_state_dict = copy.deepcopy(denoiser.state_dict())\n",
    "                pbar.set_description(f\"New best val_loss: {best_val_loss:.4f}\")\n",
    "            else:\n",
    "                steps_since_last_improvement += validate_every\n",
    "            \n",
    "            # Check for training loss divergence\n",
    "            if train_loss_val > last_train_loss * (1 + divergence_threshold):\n",
    "                divergence_counter += 1\n",
    "            else:\n",
    "                divergence_counter = 0 # Reset if loss is stable\n",
    "            last_train_loss = train_loss_val\n",
    "\n",
    "            # Check stopping conditions\n",
    "            if steps_since_last_improvement >= patience_steps:\n",
    "                print(f\"🛑 Early stopping triggered: No improvement in {patience_steps} steps.\")\n",
    "                if wandb.run: wandb.log({\"early_stop_reason\": \"patience_met\", \"early_stop_step\": step + 1})\n",
    "                break\n",
    "            \n",
    "            if divergence_counter >= divergence_patience:\n",
    "                print(f\"🛑 Early stopping triggered: Training loss diverged for {divergence_patience} checks.\")\n",
    "                if wandb.run: wandb.log({\"early_stop_reason\": \"loss_diverged\", \"early_stop_step\": step + 1})\n",
    "                break\n",
    "\n",
    "        pbar.set_postfix({\"Train Loss\": f\"{train_loss_val:.4f}\", \"Best Val\": f\"{best_val_loss:.4f}\", \"Steps w/o Improve\": f\"{steps_since_last_improvement}\"})\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # --- Restore Best Model and Save ---\n",
    "    if best_model_state_dict:\n",
    "        print(f\"✅ Restoring model to best validation loss: {best_val_loss:.4f}\")\n",
    "        denoiser.load_state_dict(best_model_state_dict)\n",
    "    \n",
    "    # Save the final, best model for promotion testing\n",
    "    final_best_path = os.path.join(config.CHECKPOINT_DIR, \"tmp_incremental_best.pth\")\n",
    "    torch.save({\"model_state_dict\": denoiser.state_dict(), 'step': step + 1, 'val_loss': best_val_loss}, final_best_path)\n",
    "\n",
    "    return final_best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e7d030",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _main_training(finetune_checkpoint: str | None = None):\n",
    "    print(\"--- Main Training Execution --- \")\n",
    "\n",
    "    print(\"--- Configuration ---\")\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Denoiser & InnerModel specific\n",
    "    DM_SIGMA_DATA = getattr(config, 'DM_SIGMA_DATA', 0.5)\n",
    "    DM_SIGMA_OFFSET_NOISE = getattr(config, 'DM_SIGMA_OFFSET_NOISE', 0.1)\n",
    "    DM_NOISE_PREVIOUS_OBS = getattr(config, 'DM_NOISE_PREVIOUS_OBS', True)\n",
    "    # DM_IMG_CHANNELS is global for prepare_single_sample_for_sampler\n",
    "    DM_NUM_STEPS_CONDITIONING = getattr(config, 'DM_NUM_STEPS_CONDITIONING', config.NUM_PREV_FRAMES)\n",
    "    DM_COND_CHANNELS = getattr(config, 'DM_COND_CHANNELS', 256)\n",
    "    DM_UNET_DEPTHS = getattr(config, 'DM_UNET_DEPTHS', [2, 2, 2, 2])\n",
    "    DM_UNET_CHANNELS = getattr(config, 'DM_UNET_CHANNELS', [128, 256, 512, 1024])\n",
    "    DM_UNET_ATTN_DEPTHS = getattr(config, 'DM_UNET_ATTN_DEPTHS', [False, False, True, True])\n",
    "    # DM_NUM_ACTIONS is global for prepare_single_sample_for_sampler\n",
    "    DM_IS_UPSAMPLER = getattr(config, 'DM_IS_UPSAMPLER', False)\n",
    "    DM_UPSAMPLING_FACTOR = getattr(config, 'DM_UPSAMPLING_FACTOR', None)\n",
    "\n",
    "    # Sampler specific (for inference/visualization)\n",
    "    SAMPLER_NUM_STEPS = getattr(config, 'SAMPLER_NUM_STEPS', 50)\n",
    "    SAMPLER_SIGMA_MIN = getattr(config, 'SAMPLER_SIGMA_MIN', 0.002)\n",
    "    SAMPLER_SIGMA_MAX = getattr(config, 'SAMPLER_SIGMA_MAX', 80.0)\n",
    "    SAMPLER_RHO = getattr(config, 'SAMPLER_RHO', 7.0)\n",
    "    SAMPLER_ORDER = getattr(config, 'SAMPLER_ORDER', 1)\n",
    "    SAMPLER_S_CHURN = getattr(config, 'SAMPLER_S_CHURN', 0.0)\n",
    "    SAMPLER_S_TMIN = getattr(config, 'SAMPLER_S_TMIN', 0.0)\n",
    "    SAMPLER_S_TMAX = getattr(config, 'SAMPLER_S_TMAX', float(\"inf\"))\n",
    "    SAMPLER_S_NOISE = getattr(config, 'SAMPLER_S_NOISE', 1.0)\n",
    "\n",
    "    # Training specific\n",
    "    BATCH_SIZE = config.BATCH_SIZE\n",
    "    LEARNING_RATE = config.LEARNING_RATE\n",
    "    NUM_EPOCHS = config.NUM_EPOCHS\n",
    "    SAVE_MODEL_EVERY = config.SAVE_MODEL_EVERY\n",
    "    SAMPLE_EVERY = config.SAMPLE_EVERY\n",
    "    PLOT_EVERY = config.PLOT_EVERY\n",
    "    GRAD_CLIP_VALUE = getattr(config, 'GRAD_CLIP_VALUE', 1.0)\n",
    "    DM_SIGMA_P_MEAN = getattr(config, 'DM_SIGMA_P_MEAN', -1.2)\n",
    "    DM_SIGMA_P_STD = getattr(config, 'DM_SIGMA_P_STD', 1.2)\n",
    "    DM_SIGMA_MIN_TRAIN = getattr(config, 'DM_SIGMA_MIN_TRAIN', 0.002)\n",
    "    DM_SIGMA_MAX_TRAIN = getattr(config, 'DM_SIGMA_MAX_TRAIN', 80.0)\n",
    "    EARLY_STOPPING_PATIENCE = getattr(config, 'EARLY_STOPPING_PATIENCE', 10)\n",
    "    EARLY_STOPPING_MIN_EPOCHS = getattr(config, 'MIN_EPOCHS', 20)\n",
    "    EARLY_STOPPING_PERCENTAGE = getattr(config, 'EARLY_STOPPING_PERCENTAGE', 0.1)\n",
    "    TRAIN_MOVING_AVG_WINDOW = getattr(config, 'TRAIN_MOVING_AVG_WINDOW', 10)\n",
    "    VAL_MOVING_AVG_WINDOW = getattr(config, 'VAL_MOVING_AVG_WINDOW', 5)\n",
    "    print(\"Configuration loaded for _main_training.\")\n",
    "\n",
    "    wandb_config = {\n",
    "        'DM_SIGMA_DATA': DM_SIGMA_DATA,\n",
    "        'DM_SIGMA_OFFSET_NOISE': DM_SIGMA_OFFSET_NOISE,\n",
    "        'DM_NOISE_PREVIOUS_OBS': DM_NOISE_PREVIOUS_OBS,\n",
    "        'DM_IMG_CHANNELS': DM_IMG_CHANNELS,\n",
    "        'DM_NUM_STEPS_CONDITIONING': DM_NUM_STEPS_CONDITIONING,\n",
    "        'DM_COND_CHANNELS': DM_COND_CHANNELS,\n",
    "        'DM_UNET_DEPTHS': DM_UNET_DEPTHS,\n",
    "        'DM_UNET_CHANNELS': DM_UNET_CHANNELS,\n",
    "        'DM_UNET_ATTN_DEPTHS': DM_UNET_ATTN_DEPTHS,\n",
    "        'DM_NUM_ACTIONS': DM_NUM_ACTIONS,\n",
    "        'DM_IS_UPSAMPLER': DM_IS_UPSAMPLER,\n",
    "        'DM_UPSAMPLING_FACTOR': DM_UPSAMPLING_FACTOR,\n",
    "        'SAMPLER_NUM_STEPS': SAMPLER_NUM_STEPS,\n",
    "        'SAMPLER_SIGMA_MIN': SAMPLER_SIGMA_MIN,\n",
    "        'SAMPLER_SIGMA_MAX': SAMPLER_SIGMA_MAX,\n",
    "        'SAMPLER_RHO': SAMPLER_RHO,\n",
    "        'SAMPLER_ORDER': SAMPLER_ORDER,\n",
    "        'SAMPLER_S_CHURN': SAMPLER_S_CHURN,\n",
    "        'SAMPLER_S_TMIN': SAMPLER_S_TMIN,\n",
    "        'SAMPLER_S_TMAX': SAMPLER_S_TMAX,\n",
    "        'SAMPLER_S_NOISE': SAMPLER_S_NOISE,\n",
    "        'BATCH_SIZE': BATCH_SIZE,\n",
    "        'LEARNING_RATE': LEARNING_RATE,\n",
    "        'NUM_EPOCHS': NUM_EPOCHS,\n",
    "        'GRAD_CLIP_VALUE': GRAD_CLIP_VALUE,\n",
    "        'DM_SIGMA_P_MEAN': DM_SIGMA_P_MEAN,\n",
    "        'DM_SIGMA_P_STD': DM_SIGMA_P_STD,\n",
    "        'DM_SIGMA_MIN_TRAIN': DM_SIGMA_MIN_TRAIN,\n",
    "        'DM_SIGMA_MAX_TRAIN': DM_SIGMA_MAX_TRAIN,\n",
    "        'EARLY_STOPPING_PATIENCE': EARLY_STOPPING_PATIENCE,\n",
    "        'EARLY_STOPPING_MIN_EPOCHS': EARLY_STOPPING_MIN_EPOCHS,\n",
    "        'EARLY_STOPPING_PERCENTAGE': EARLY_STOPPING_PERCENTAGE,\n",
    "        'TRAIN_MOVING_AVG_WINDOW': TRAIN_MOVING_AVG_WINDOW,\n",
    "        'VAL_MOVING_AVG_WINDOW': VAL_MOVING_AVG_WINDOW,\n",
    "        'IMAGE_SIZE': config.IMAGE_SIZE,\n",
    "        'NUM_PREV_FRAMES': config.NUM_PREV_FRAMES,\n",
    "        'PROJECT_NAME': getattr(config, 'PROJECT_NAME', 'jetbot-diamond-world-model'),\n",
    "        'FIXED_VIS_SAMPLE_IDX': getattr(config, 'FIXED_VIS_SAMPLE_IDX', 0),\n",
    "        'MOVING_ACTION_VALUE_FOR_VIS': getattr(config, 'MOVING_ACTION_VALUE_FOR_VIS', 0.13)\n",
    "    }\n",
    "    wandb.init(project=wandb_config['PROJECT_NAME'], config=wandb_config)\n",
    "    print(\"Wandb initialized for _main_training.\")\n",
    "\n",
    "    print(\"--- Initializing Models for _main_training ---\")\n",
    "    try:\n",
    "        inner_model_config = models.InnerModelConfig(\n",
    "            img_channels=DM_IMG_CHANNELS,\n",
    "            num_steps_conditioning=DM_NUM_STEPS_CONDITIONING,\n",
    "            cond_channels=DM_COND_CHANNELS,\n",
    "            depths=DM_UNET_DEPTHS,\n",
    "            channels=DM_UNET_CHANNELS,\n",
    "            attn_depths=DM_UNET_ATTN_DEPTHS,\n",
    "            num_actions=DM_NUM_ACTIONS,\n",
    "            is_upsampler=DM_IS_UPSAMPLER\n",
    "        )\n",
    "        # inner_model_instance = models.InnerModel(inner_model_config).to(DEVICE) # Not strictly needed if only denoiser is used\n",
    "        # print(f\"InnerModelImpl parameter count: {sum(p.numel() for p in inner_model_instance.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "        denoiser_cfg = models.DenoiserConfig(\n",
    "            inner_model=inner_model_config, \n",
    "            sigma_data=DM_SIGMA_DATA,\n",
    "            sigma_offset_noise=DM_SIGMA_OFFSET_NOISE,\n",
    "            noise_previous_obs=DM_NOISE_PREVIOUS_OBS,\n",
    "            upsampling_factor=DM_UPSAMPLING_FACTOR\n",
    "        )\n",
    "        denoiser = models.Denoiser(cfg=denoiser_cfg).to(DEVICE)\n",
    "        sigma_dist_train_cfg = models.SigmaDistributionConfig(\n",
    "            loc=DM_SIGMA_P_MEAN, scale=DM_SIGMA_P_STD,\n",
    "            sigma_min=DM_SIGMA_MIN_TRAIN, sigma_max=DM_SIGMA_MAX_TRAIN\n",
    "        )\n",
    "        denoiser.setup_training(sigma_dist_train_cfg)\n",
    "        print(f\"Denoiser model created for _main_training. Total parameter count: {sum(p.numel() for p in denoiser.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "        sampler_cfg = models.DiffusionSamplerConfig(\n",
    "            num_steps_denoising=SAMPLER_NUM_STEPS, sigma_min=SAMPLER_SIGMA_MIN,\n",
    "            sigma_max=SAMPLER_SIGMA_MAX, rho=SAMPLER_RHO, order=SAMPLER_ORDER,\n",
    "            s_churn=SAMPLER_S_CHURN, s_tmin=SAMPLER_S_TMIN,\n",
    "            s_tmax=SAMPLER_S_TMAX, s_noise=SAMPLER_S_NOISE\n",
    "        )\n",
    "        diffusion_sampler = models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg)\n",
    "        print(\"DiffusionSampler created for visualization in _main_training.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing models in _main_training: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"--- Setting up Optimizer and Scheduler for _main_training ---\")\n",
    "    lr_for_optimizer = LEARNING_RATE\n",
    "    if finetune_checkpoint:\n",
    "        print(f\"Finetuning from checkpoint: {finetune_checkpoint}\")\n",
    "        if os.path.exists(finetune_checkpoint):\n",
    "            ckpt = torch.load(finetune_checkpoint, map_location=DEVICE)\n",
    "            if \"model_state_dict\" in ckpt:\n",
    "                denoiser.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "                lr_for_optimizer = LEARNING_RATE / 5\n",
    "                print(f\"Loaded weights for fine-tuning. LR set to {lr_for_optimizer}\")\n",
    "            else:\n",
    "                print(f\"Warning: no model_state_dict in {finetune_checkpoint}. Starting fresh\")\n",
    "        else:\n",
    "            print(f\"Warning: finetune checkpoint {finetune_checkpoint} not found. Starting fresh\")\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        denoiser.parameters(), lr=lr_for_optimizer,\n",
    "        weight_decay=config.LEARNING_RATE_WEIGHT_DECAY, eps=config.LEARNING_RATE_EPS\n",
    "    )\n",
    "    print(f\"Optimizer: AdamW with LR={lr_for_optimizer}\")\n",
    "    def lr_lambda_main(current_step: int):\n",
    "        if current_step < config.LEARNING_RATE_WARMUP_STEPS:\n",
    "            return float(current_step) / float(max(1, config.LEARNING_RATE_WARMUP_STEPS))\n",
    "        return 1.0\n",
    "    lr_scheduler = LambdaLR(optimizer, lr_lambda_main)\n",
    "    print(f\"LR Scheduler: LambdaLR with {config.LEARNING_RATE_WARMUP_STEPS} warmup steps.\")\n",
    "    wandb.watch(denoiser, log=\"all\", log_freq=100)\n",
    "    print(\"Wandb watching denoiser model.\")\n",
    "\n",
    "    START_EPOCH = 0\n",
    "    BEST_VAL_LOSS_MA_FROM_CKPT = float('inf')\n",
    "    PREVIOUS_BEST_VAL_MODEL_PATH = None\n",
    "\n",
    "    load_path_config_main = None if finetune_checkpoint else config.LOAD_CHECKPOINT\n",
    "    best_val_loss_model_default_path_main = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\")\n",
    "    load_path_main = None\n",
    "    if not finetune_checkpoint:\n",
    "        load_path_main = load_path_config_main\n",
    "        if load_path_main:\n",
    "            print(f\"Attempting to load checkpoint from config.LOAD_CHECKPOINT: {load_path_main}\")\n",
    "        elif os.path.exists(best_val_loss_model_default_path_main):\n",
    "            load_path_main = best_val_loss_model_default_path_main\n",
    "            print(f\"Using existing best_val_loss model: {load_path_main}\")\n",
    "\n",
    "        if load_path_main and os.path.exists(load_path_main):\n",
    "            print(f\"Loading checkpoint for _main_training from: {load_path_main}\")\n",
    "            try:\n",
    "                checkpoint = torch.load(load_path_main, map_location=DEVICE)\n",
    "                denoiser.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "                START_EPOCH = checkpoint.get(\"epoch\", 0) + 1\n",
    "                BEST_VAL_LOSS_MA_FROM_CKPT = checkpoint.get(\"best_val_loss_ma\", float(\"inf\"))\n",
    "                if load_path_main.endswith(\"denoiser_model_best_val_loss.pth\"):\n",
    "                    PREVIOUS_BEST_VAL_MODEL_PATH = load_path_main\n",
    "                print(f\"Resuming _main_training from epoch {START_EPOCH}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint in _main_training: {e}. Starting fresh.\")\n",
    "                START_EPOCH = 0\n",
    "        else:\n",
    "            print(\"No checkpoint found or specified for _main_training. Starting fresh.\")\n",
    "    train_dataset, val_dataset = split_dataset()\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=False, drop_last=False)\n",
    "    print(f\"Training dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    val_stopped_subset, val_moving_subset = [], []\n",
    "    if len(val_dataset) > 0:\n",
    "        print(\"Preparing filtered validation subsets for visualization...\")\n",
    "        val_stopped_subset = filter_dataset_by_action(val_dataset, target_actions=0.0)\n",
    "        moving_action_val = wandb_config['MOVING_ACTION_VALUE_FOR_VIS']\n",
    "        val_moving_subset = filter_dataset_by_action(val_dataset, target_actions=moving_action_val)\n",
    "        print(f\"Found {len(val_stopped_subset)} stopped and {len(val_moving_subset)} moving samples.\")\n",
    "    else:\n",
    "        from torch.utils.data import Subset # Ensure Subset is available if val_dataset is empty\n",
    "        val_stopped_subset = Subset(val_dataset, [])\n",
    "        val_moving_subset = Subset(val_dataset, [])\n",
    "    \n",
    "    print(\"--- Starting Training Process in _main_training ---\")\n",
    "    overall_training_start_time = time.time()\n",
    "    all_train_losses_for_plot, all_val_losses_for_plot = [], []\n",
    "    train_loss_moving_avg_q = deque(maxlen=TRAIN_MOVING_AVG_WINDOW)\n",
    "    val_loss_moving_avg_q = deque(maxlen=VAL_MOVING_AVG_WINDOW)\n",
    "    best_val_loss_ma = BEST_VAL_LOSS_MA_FROM_CKPT\n",
    "    epochs_without_improvement_val = 0\n",
    "    previous_best_val_model_path = PREVIOUS_BEST_VAL_MODEL_PATH\n",
    "    final_epoch_completed = START_EPOCH - 1\n",
    "    num_train_batches = len(train_dataloader)\n",
    "    num_val_batches = len(val_dataloader)\n",
    "    \n",
    "    train_step_count = 0\n",
    "    val_step_count = 0\n",
    "    for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_num_for_log = epoch + 1\n",
    "        avg_train_loss, train_step_count = train_denoiser_epoch(\n",
    "            denoiser_model=denoiser, train_dl=train_dataloader, opt=optimizer,\n",
    "            scheduler=lr_scheduler, grad_clip_val=GRAD_CLIP_VALUE, device=DEVICE,\n",
    "            epoch_num_for_log=current_epoch_num_for_log,\n",
    "            num_train_batches_total=num_train_batches, num_val_batches_total=num_val_batches,\n",
    "            train_step_start=train_step_count\n",
    "        )\n",
    "        \n",
    "        all_train_losses_for_plot.append(avg_train_loss)\n",
    "        train_loss_moving_avg_q.append(avg_train_loss)\n",
    "        current_train_moving_avg = sum(train_loss_moving_avg_q) / len(train_loss_moving_avg_q) if train_loss_moving_avg_q else float('inf')\n",
    "    \n",
    "        avg_val_loss, val_step_count = validate_denoiser_epoch(\n",
    "            denoiser_model=denoiser, \n",
    "            val_dl=val_dataloader, \n",
    "            device=DEVICE, \n",
    "            epoch_num_for_log=current_epoch_num_for_log,\n",
    "            num_train_batches_total=num_train_batches, \n",
    "            num_val_batches_total=num_val_batches,\n",
    "            val_step_start=val_step_count      \n",
    "        )\n",
    "        all_val_losses_for_plot.append(avg_val_loss)\n",
    "        val_loss_moving_avg_q.append(avg_val_loss) \n",
    "        current_val_moving_avg = sum(val_loss_moving_avg_q) / len(val_loss_moving_avg_q) if val_loss_moving_avg_q else float('inf')\n",
    "    \n",
    "        epoch_duration_seconds = time.time() - epoch_start_time\n",
    "        epoch_duration_formatted = str(datetime.timedelta(seconds=epoch_duration_seconds))\n",
    "    \n",
    "        print(f\"Epoch {current_epoch_num_for_log}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f} (MA: {current_train_moving_avg:.4f}), Val Loss: {avg_val_loss:.4f} (MA: {current_val_moving_avg:.4f}), Duration: {epoch_duration_formatted}\")\n",
    "    \n",
    "        ### WANDB: Log epoch-level metrics ###\n",
    "        \n",
    "        wandb_log_data = {\n",
    "            \"epoch\": current_epoch_num_for_log,\n",
    "            \"avg_train_loss\": avg_train_loss,\n",
    "            \"train_loss_ma\": current_train_moving_avg,\n",
    "            \"avg_val_loss\": avg_val_loss,\n",
    "            \"val_loss_ma\": current_val_moving_avg,\n",
    "            \"best_val_loss_ma_so_far\": best_val_loss_ma, # Log best val loss MA so far\n",
    "            \"epoch_duration_sec\": epoch_duration_seconds,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        \n",
    "        if lr_scheduler: lr_scheduler.step(avg_val_loss if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) else None)\n",
    "    \n",
    "        # Save model based on Validation Loss MA\n",
    "        if current_val_moving_avg < best_val_loss_ma:\n",
    "            improvement_val_over_absolute_best = (best_val_loss_ma - current_val_moving_avg) / abs(best_val_loss_ma + 1e-9) * 100\n",
    "            print(f\"  Val Loss MA improved to {current_val_moving_avg:.6f} from {best_val_loss_ma:.6f} ({improvement_val_over_absolute_best:.2f}% improvement).\")\n",
    "            best_val_loss_ma = current_val_moving_avg\n",
    "            new_best_val_model_path = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': denoiser.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'best_val_loss_ma': best_val_loss_ma\n",
    "            }, new_best_val_model_path)\n",
    "            print(f\"  Saved new best model (val loss MA) at epoch {current_epoch_num_for_log}\")\n",
    "            if previous_best_val_model_path and previous_best_val_model_path != new_best_val_model_path and os.path.exists(previous_best_val_model_path):\n",
    "                try:\n",
    "                    os.remove(previous_best_val_model_path)\n",
    "                    print(f\"  Deleted previous best val model: {previous_best_val_model_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"  Warning: Could not delete previous best val model '{previous_best_val_model_path}': {e}\")\n",
    "            previous_best_val_model_path = new_best_val_model_path\n",
    "    \n",
    "        should_stop_early = False\n",
    "        # Early stopping based on validation loss moving average\n",
    "        if current_epoch_num_for_log > EARLY_STOPPING_MIN_EPOCHS:\n",
    "            if current_val_moving_avg < best_val_loss_ma:\n",
    "                epochs_without_improvement_val = 0\n",
    "            else:\n",
    "                epochs_without_improvement_val += 1\n",
    "                print(f\"  No improvement in val loss MA for {epochs_without_improvement_val} epoch(s). Best MA: {best_val_loss_ma:.6f}, Current MA: {current_val_moving_avg:.6f}\")\n",
    "                if epochs_without_improvement_val >= EARLY_STOPPING_PATIENCE:\n",
    "                    should_stop_early = True\n",
    "                    print(\"Early stopping triggered due to validation loss stagnation.\")\n",
    "        if (current_epoch_num_for_log % SAVE_MODEL_EVERY == 0) or (epoch == NUM_EPOCHS - 1):\n",
    "            is_best_this_epoch = current_val_moving_avg == best_val_loss_ma\n",
    "            # Avoid saving regular checkpoint if it's also the best_val_loss epoch to prevent duplicate saves\n",
    "            if not (is_best_this_epoch and os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\") == previous_best_val_model_path):\n",
    "                 torch.save({\n",
    "                    'epoch': epoch, 'model_state_dict': denoiser.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(), 'loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'best_val_loss_ma': best_val_loss_ma\n",
    "                }, os.path.join(config.CHECKPOINT_DIR, f\"denoiser_model_epoch_{current_epoch_num_for_log:04d}.pth\"))\n",
    "                 print(f\"Saved model checkpoint at epoch {current_epoch_num_for_log}\")\n",
    "        \n",
    "        final_epoch_completed = epoch # Update last completed epoch here\n",
    "        if should_stop_early: break\n",
    "    \n",
    "        if (current_epoch_num_for_log % SAMPLE_EVERY == 0) or (epoch == NUM_EPOCHS - 1) or should_stop_early:\n",
    "            print(f\"Epoch {current_epoch_num_for_log}: Generating multiple visualization samples...\")\n",
    "            denoiser.eval()\n",
    "            vis_wandb_log_data = {} # Accumulate images here for a single wandb.log call\n",
    "    \n",
    "            # --- 1. Fixed Sample ---\n",
    "            fixed_sample_idx = wandb_config.get('FIXED_VIS_SAMPLE_IDX', 0)\n",
    "            if fixed_sample_idx < len(val_dataset):\n",
    "                print(f\"  Generating fixed sample (index {fixed_sample_idx} from val_dataset)...\")\n",
    "                fixed_sample_data = val_dataset[fixed_sample_idx]\n",
    "                prev_obs_fixed, prev_act_fixed, gt_fixed_batch, gt_prev_frames_fixed_seq = prepare_single_sample_for_sampler(fixed_sample_data, DEVICE) # gt_fixed_batch is [1,C,H,W]\n",
    "                with torch.no_grad():\n",
    "                    generated_output_tuple_fixed = diffusion_sampler.sample(prev_obs=prev_obs_fixed, prev_act=prev_act_fixed)\n",
    "                \n",
    "                if generated_output_tuple_fixed:\n",
    "                    generated_image_batch_fixed = generated_output_tuple_fixed[0] # This is [1, C, H, W]\n",
    "                    if generated_image_batch_fixed.ndim == 4 and generated_image_batch_fixed.shape[0] == 1:\n",
    "                        generated_image_to_save_fixed = generated_image_batch_fixed[0] # Extract single image: [C, H, W]\n",
    "                    else:\n",
    "                        generated_image_to_save_fixed = generated_image_batch_fixed # Fallback, though should be 4D\n",
    "        \n",
    "                    gt_image_to_save_fixed = gt_fixed_batch[0] # Extract single GT image: [C, H, W]\n",
    "        \n",
    "                    vis_path_fixed = save_visualization_samples(\n",
    "                        generated_image_to_save_fixed, # Should be [C,H,W]\n",
    "                        gt_image_to_save_fixed,        # Should be [C,H,W]\n",
    "                        gt_prev_frames_fixed_seq,\n",
    "                        current_epoch_num_for_log,\n",
    "                        config.SAMPLE_DIR,\n",
    "                        prefix=f\"val_vis_fixed_idx{fixed_sample_idx}\"\n",
    "                    )\n",
    "                    if vis_path_fixed and wandb.run:\n",
    "                        vis_wandb_log_data[f\"validation_samples/fixed_idx_{fixed_sample_idx}\"] = wandb.Image(vis_path_fixed, caption=f\"Epoch {current_epoch_num_for_log} Fixed Sample (Val Idx {fixed_sample_idx})\")\n",
    "                else:\n",
    "                    print(\"  Warning: Sampler did not return output for fixed sample.\")\n",
    "            else:\n",
    "                print(f\"  Warning: FIXED_SAMPLE_IDX {fixed_sample_idx} is out of bounds for val_dataset (size {len(val_dataset)}). Skipping fixed sample.\")\n",
    "        \n",
    "            # --- 2. Random Stopped Sample (Action 0.0) ---\n",
    "            if len(val_stopped_subset) > 0:\n",
    "                print(\"  Generating random stopped sample...\")\n",
    "                random_stopped_idx_in_subset = random.randint(0, len(val_stopped_subset) - 1)\n",
    "                stopped_sample_data = val_stopped_subset[random_stopped_idx_in_subset]\n",
    "                prev_obs_stopped, prev_act_stopped, gt_stopped_batch, gt_prev_frames_stopped_seq = prepare_single_sample_for_sampler(stopped_sample_data, DEVICE) # gt_stopped_batch is [1,C,H,W]\n",
    "                with torch.no_grad():\n",
    "                    generated_output_tuple_stopped = diffusion_sampler.sample(prev_obs=prev_obs_stopped, prev_act=prev_act_stopped)\n",
    "                \n",
    "                if generated_output_tuple_stopped:\n",
    "                    generated_image_batch_stopped = generated_output_tuple_stopped[0] # This is [1, C, H, W]\n",
    "                    if generated_image_batch_stopped.ndim == 4 and generated_image_batch_stopped.shape[0] == 1:\n",
    "                        generated_image_to_save_stopped = generated_image_batch_stopped[0] # Extract single image: [C, H, W]\n",
    "                    else:\n",
    "                        generated_image_to_save_stopped = generated_image_batch_stopped\n",
    "        \n",
    "                    gt_image_to_save_stopped = gt_stopped_batch[0] # Extract single GT image: [C, H, W]\n",
    "        \n",
    "                    vis_path_stopped = save_visualization_samples(\n",
    "                        generated_image_to_save_stopped, # Should be [C,H,W]\n",
    "                        gt_image_to_save_stopped,        # Should be [C,H,W]\n",
    "                        gt_prev_frames_stopped_seq,\n",
    "                        current_epoch_num_for_log,\n",
    "                        config.SAMPLE_DIR,\n",
    "                        prefix=\"val_vis_stopped_random\"\n",
    "                    )\n",
    "                    if vis_path_stopped and wandb.run:\n",
    "                        vis_wandb_log_data[\"validation_samples/random_stopped\"] = wandb.Image(vis_path_stopped, caption=f\"Epoch {current_epoch_num_for_log} Random Stopped Sample\")\n",
    "                else:\n",
    "                    print(\"  Warning: Sampler did not return output for stopped sample.\")\n",
    "            else:\n",
    "                print(\"  Warning: No stopped (action 0.0) samples found in validation set. Skipping random stopped sample.\")\n",
    "        \n",
    "            # --- 3. Random Moving Sample ---\n",
    "            moving_action_val_vis = wandb_config.get('MOVING_ACTION_VALUE_FOR_VIS', 0.1)\n",
    "            if len(val_moving_subset) > 0:\n",
    "                print(f\"  Generating random moving sample (action {moving_action_val_vis})...\")\n",
    "                random_moving_idx_in_subset = random.randint(0, len(val_moving_subset) - 1)\n",
    "                moving_sample_data = val_moving_subset[random_moving_idx_in_subset]\n",
    "                prev_obs_moving, prev_act_moving, gt_moving_batch, gt_prev_frames_moving_seq = prepare_single_sample_for_sampler(moving_sample_data, DEVICE) # gt_moving_batch is [1,C,H,W]\n",
    "                with torch.no_grad():\n",
    "                    generated_output_tuple_moving = diffusion_sampler.sample(prev_obs=prev_obs_moving, prev_act=prev_act_moving)\n",
    "        \n",
    "                if generated_output_tuple_moving:\n",
    "                    generated_image_batch_moving = generated_output_tuple_moving[0] # This is [1, C, H, W]\n",
    "                    if generated_image_batch_moving.ndim == 4 and generated_image_batch_moving.shape[0] == 1:\n",
    "                        generated_image_to_save_moving = generated_image_batch_moving[0] # Extract single image: [C, H, W]\n",
    "                    else:\n",
    "                        generated_image_to_save_moving = generated_image_batch_moving\n",
    "        \n",
    "                    gt_image_to_save_moving = gt_moving_batch[0] # Extract single GT image: [C, H, W]\n",
    "        \n",
    "                    vis_path_moving = save_visualization_samples(\n",
    "                        generated_image_to_save_moving, # Should be [C,H,W]\n",
    "                        gt_image_to_save_moving,        # Should be [C,H,W]\n",
    "                        gt_prev_frames_moving_seq,\n",
    "                        current_epoch_num_for_log,\n",
    "                        config.SAMPLE_DIR,\n",
    "                        prefix=f\"val_vis_moving_act{str(moving_action_val_vis).replace('.', 'p')}_random\"\n",
    "                    )\n",
    "                    if vis_path_moving and wandb.run:\n",
    "                        vis_wandb_log_data[f\"validation_samples/random_moving_act{str(moving_action_val_vis).replace('.', 'p')}\"] = wandb.Image(vis_path_moving, caption=f\"Epoch {current_epoch_num_for_log} Random Moving Sample (Action {moving_action_val_vis})\")\n",
    "                else:\n",
    "                    print(\"  Warning: Sampler did not return output for moving sample.\")\n",
    "            else:\n",
    "                print(f\"  Warning: No moving (action {moving_action_val_vis}) samples found in validation set. Skipping random moving sample.\")\n",
    "            \n",
    "            denoiser.train() # Set model back to training mode\n",
    "            # Log all accumulated data for this epoch (losses + images)\n",
    "            if wandb.run:\n",
    "                wandb.log({**wandb_log_data, **vis_wandb_log_data})\n",
    "        elif wandb.run: # If not sampling, still log epoch metrics\n",
    "             wandb.log(wandb_log_data)\n",
    "    \n",
    "    \n",
    "        if (current_epoch_num_for_log % PLOT_EVERY == 0) or (epoch == NUM_EPOCHS - 1) or should_stop_early :\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(all_train_losses_for_plot, label=\"Avg Train Loss\")\n",
    "            plt.plot(all_val_losses_for_plot, label=\"Avg Validation Loss\")\n",
    "            if len(all_train_losses_for_plot) >= TRAIN_MOVING_AVG_WINDOW:\n",
    "                train_ma_plot = [sum(all_train_losses_for_plot[i-TRAIN_MOVING_AVG_WINDOW+1:i+1])/TRAIN_MOVING_AVG_WINDOW for i in range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot))]\n",
    "                plt.plot(range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot)), train_ma_plot, label=f'Train Loss MA ({TRAIN_MOVING_AVG_WINDOW} epochs)', linestyle=':')\n",
    "            if len(all_val_losses_for_plot) >= VAL_MOVING_AVG_WINDOW:\n",
    "                val_ma_plot = [sum(all_val_losses_for_plot[i-VAL_MOVING_AVG_WINDOW+1:i+1])/VAL_MOVING_AVG_WINDOW for i in range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot))]\n",
    "                plt.plot(range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot)), val_ma_plot, label=f'Val Loss MA ({VAL_MOVING_AVG_WINDOW} epochs)', linestyle='--')\n",
    "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Progress (Epoch {current_epoch_num_for_log})\")\n",
    "            plt.legend(); plt.grid(True)\n",
    "            plt.savefig(os.path.join(config.PLOT_DIR, f\"loss_plot_epoch_{current_epoch_num_for_log:04d}.png\"))\n",
    "            ### WANDB: Log epoch loss plot ###\n",
    "            wandb.log({\"epoch_loss_plot\": wandb.Image(plt, caption=f\"Loss Plot Epoch {current_epoch_num_for_log}\")})\n",
    "            plt.close()\n",
    "            print(f\"Saved loss plot up to epoch {current_epoch_num_for_log}\")\n",
    "    \n",
    "    overall_training_end_time = time.time()\n",
    "    total_training_duration_seconds = overall_training_end_time - overall_training_start_time\n",
    "    total_training_duration_formatted = str(datetime.timedelta(seconds=total_training_duration_seconds))\n",
    "    \n",
    "    # final_epoch_completed is the last epoch index that ran (0-indexed)\n",
    "    print(f\"--- Training Complete (Stopped after epoch {final_epoch_completed + 1}) ---\") \n",
    "    print(f\"Total training duration: {total_training_duration_formatted}\") \n",
    "    \n",
    "    # Final Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_train_losses_for_plot, label=\"Avg Train Loss\")\n",
    "    plt.plot(all_val_losses_for_plot, label=\"Avg Validation Loss\")\n",
    "    if len(all_train_losses_for_plot) >= TRAIN_MOVING_AVG_WINDOW:\n",
    "        train_ma_plot = [sum(all_train_losses_for_plot[i-TRAIN_MOVING_AVG_WINDOW+1:i+1])/TRAIN_MOVING_AVG_WINDOW for i in range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot))]\n",
    "        plt.plot(range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot)), train_ma_plot, label=f'Train Loss MA ({TRAIN_MOVING_AVG_WINDOW} epochs)', linestyle=':')\n",
    "    if len(all_val_losses_for_plot) >= VAL_MOVING_AVG_WINDOW:\n",
    "        val_ma_plot = [sum(all_val_losses_for_plot[i-VAL_MOVING_AVG_WINDOW+1:i+1])/VAL_MOVING_AVG_WINDOW for i in range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot))]\n",
    "        plt.plot(range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot)), val_ma_plot, label=f'Val Loss MA ({VAL_MOVING_AVG_WINDOW} epochs)', linestyle='--')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Denoiser Final Training & Validation Loss (Up to Epoch {final_epoch_completed + 1})\")\n",
    "    plt.legend(); plt.grid(True)\n",
    "    final_loss_plot_path = os.path.join(config.PLOT_DIR, \"denoiser_final_loss_plot.png\")\n",
    "    plt.savefig(final_loss_plot_path)\n",
    "    # plt.show() # Usually not needed in script, but can be uncommented for interactive\n",
    "    print(f\"Final loss plot saved to {final_loss_plot_path}\")\n",
    "    \n",
    "    ### WANDB: Log final loss plot and finish run ###\n",
    "    wandb.log({\"final_loss_plot\": wandb.Image(final_loss_plot_path, caption=f\"Final Loss Plot up to Epoch {final_epoch_completed + 1}\")})\n",
    "    wandb.finish()\n",
    "    print(\"Wandb run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae2abe1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Main Training Execution --- \n",
      "--- Configuration ---\n",
      "Using device: cuda\n",
      "Configuration loaded for _main_training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: irvin-hwang (irvin-hwang-simulacra-systems) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\jetbot-diffusion-world-model-kong-finder\\wandb\\run-20250708_144706-f17ki3ke</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model/runs/f17ki3ke' target=\"_blank\">olive-voice-35</a></strong> to <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model/runs/f17ki3ke' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model/runs/f17ki3ke</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb initialized for _main_training.\n",
      "--- Initializing Models for _main_training ---\n",
      "Denoiser model created for _main_training. Total parameter count: 330,464,131\n",
      "DiffusionSampler created for visualization in _main_training.\n",
      "--- Setting up Optimizer and Scheduler for _main_training ---\n",
      "Finetuning from checkpoint: C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\output_model_5hz_DIAMOND_laundry_incremental\\checkpoints\\denoiser_model_best_val_loss.pth\n",
      "Loaded weights for fine-tuning. LR set to 2e-05\n",
      "Optimizer: AdamW with LR=2e-05\n",
      "LR Scheduler: LambdaLR with 100 warmup steps.\n",
      "Wandb watching denoiser model.\n",
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n",
      "Full dataset size: 22801\n",
      "Loading dataset split from C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\output_model_4hz_DIAMOND_laundry_finetune\\dataset_split.pth\n",
      "Training dataset size: 20520, Validation dataset size: 2281\n",
      "Preparing filtered validation subsets for visualization...\n",
      "Filtering dataset with 2281 samples for actions: [0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef41f95589ff48beb7e3d3cd56a3142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering Dataset:   0%|          | 0/2281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered down to 1118 samples.\n",
      "Filtering dataset with 2281 samples for actions: [0.13]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d797f21fb745fe8ca363bc988ff45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering Dataset:   0%|          | 0/2281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered down to 1163 samples.\n",
      "Found 1118 stopped and 1163 moving samples.\n",
      "--- Starting Training Process in _main_training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f651bb2fef42db9d995f9a8cec9b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/10260 [00:14<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43m_main_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProjects\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mjetbot-diffusion-world-model-kong-finder-aux\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43moutput_model_5hz_DIAMOND_laundry_incremental\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdenoiser_model_best_val_loss.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 232\u001b[0m, in \u001b[0;36m_main_training\u001b[1;34m(finetune_checkpoint)\u001b[0m\n\u001b[0;32m    230\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    231\u001b[0m current_epoch_num_for_log \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 232\u001b[0m avg_train_loss, train_step_count \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_denoiser_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenoiser_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGRAD_CLIP_VALUE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_num_for_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_epoch_num_for_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_batches_total\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_val_batches_total\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_val_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_step_count\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m all_train_losses_for_plot\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n\u001b[0;32m    241\u001b[0m train_loss_moving_avg_q\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m, in \u001b[0;36mtrain_denoiser_epoch\u001b[1;34m(denoiser_model, train_dl, opt, scheduler, grad_clip_val, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, train_step_start)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m         train_step \u001b[38;5;241m=\u001b[39m train_step_start \u001b[38;5;241m+\u001b[39m batch_idx\n\u001b[1;32m---> 47\u001b[0m         \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Log un-normalized loss\u001b[39;49;00m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_denoising_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_denoising\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dl) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_dl) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     54\u001b[0m final_step \u001b[38;5;241m=\u001b[39m train_step_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dl)\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:406\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:503\u001b[0m, in \u001b[0;36m_noop_if_forked_with_no_service.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m init_pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_using_service \u001b[38;5;129;01mor\u001b[39;00m init_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid():\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` ignored (called from pid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetpid()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `init` called from pid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_pid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m See: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_registry\u001b[38;5;241m.\u001b[39murl(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiprocess\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m )\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# This attribute may not exist because it is not included in the run's\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# pickled state.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:464\u001b[0m, in \u001b[0;36m_raise_if_finished.<locals>.wrapper_fn\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Run, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 464\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m     )\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(message)\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:451\u001b[0m, in \u001b[0;36m_attach.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    449\u001b[0m         _is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2071\u001b[0m, in \u001b[0;36mRun.log\u001b[1;34m(self, data, step, commit, sync)\u001b[0m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_shared \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2065\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[0;32m   2066\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn shared mode, the use of `wandb.log` with the step argument is not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2067\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be ignored. Please refer to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_registry\u001b[38;5;241m.\u001b[39murl(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefine-metric\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2068\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon how to customize your x-axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2069\u001b[0m         repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2070\u001b[0m     )\n\u001b[1;32m-> 2071\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1783\u001b[0m, in \u001b[0;36mRun._log\u001b[1;34m(self, data, step, commit)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1783\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetpid() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attached:\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:406\u001b[0m, in \u001b[0;36m_log_to_run.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_id\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging\u001b[38;5;241m.\u001b[39mlog_to_run(run_id):\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1610\u001b[0m, in \u001b[0;36mRun._partial_history_callback\u001b[1;34m(self, data, step, commit)\u001b[0m\n\u001b[0;32m   1607\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialize_custom_charts(data)\n\u001b[0;32m   1609\u001b[0m not_using_tensorboard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(wandb\u001b[38;5;241m.\u001b[39mpatched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1610\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py:691\u001b[0m, in \u001b[0;36mInterfaceBase.publish_partial_history\u001b[1;34m(self, run, data, user_step, step, flush, publish_step)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flush \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     partial_history\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mflush \u001b[38;5;241m=\u001b[39m flush\n\u001b[1;32m--> 691\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_partial_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:48\u001b[0m, in \u001b[0;36mInterfaceShared._publish_partial_history\u001b[1;34m(self, partial_history)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_partial_history\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m, partial_history: pb\u001b[38;5;241m.\u001b[39mPartialHistoryRequest\n\u001b[0;32m     46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(partial_history\u001b[38;5;241m=\u001b[39mpartial_history)\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[1;34m(self, record, local)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[0;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x0000024DF70620D0>> (for post_run_cell), with arguments args (<ExecutionResult object at 24df575ce10, execution_count=7 error_before_exec=None error_in_exec=[WinError 10054] An existing connection was forcibly closed by the remote host info=<ExecutionInfo object at 24dc77f3110, raw_cell=\"\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    _main_training(\"C..\" store_history=True silent=False shell_futures=True cell_id=dae2abe1> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:614\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py:778\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    777\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[1;32m--> 778\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[1;34m(self, resume)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[1;34m(self, record, local)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[0;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main_training()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
