{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4c62f1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0699dbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# \n",
        "# \n",
        "# get_ipython().system('pip install wandb')\n",
        "\n",
        "# \n",
        "# \n",
        "# get_ipython().system('pip install --upgrade typing_extensions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6d62c9",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import datetime # For epoch timing and timestamping\n",
        "from torchvision import transforms\n",
        "from collections import deque # For moving average\n",
        "from dataclasses import dataclass \n",
        "from typing import List, Optional, Dict, Any \n",
        "import random\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import copy # Make sure to import copy at the top of the file\n",
        "\n",
        "import wandb # Will be initialized in _main_training\n",
        "\n",
        "# Your project's specific imports\n",
        "import config # Your config.py\n",
        "import models # Your models.py (which should import from diamond_models.ipynb)\n",
        "\n",
        "# Import dataset from your jetbot_dataset.ipynb\n",
        "from importnb import Notebook\n",
        "with Notebook():\n",
        "    from jetbot_dataset import JetbotDataset, filter_dataset_by_action \n",
        "\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "print(\"Imports successful.\")\n",
        "\n",
        "# DEVICE will be set in _main_training or used directly from config by other functions\n",
        "# Global config constants that might be used by imported functions like train_diamond_model\n",
        "# if they don't re-fetch from config themselves (they mostly do, but being safe).\n",
        "DM_IMG_CHANNELS = getattr(config, 'DM_IMG_CHANNELS', 3)\n",
        "DM_NUM_ACTIONS = getattr(config, 'DM_NUM_ACTIONS', 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f06e05",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def split_dataset():\n",
        "    full_dataset = JetbotDataset(\n",
        "        csv_path=config.CSV_PATH,\n",
        "        data_dir=config.DATA_DIR,\n",
        "        image_size=config.IMAGE_SIZE,\n",
        "        num_prev_frames=config.NUM_PREV_FRAMES,\n",
        "        transform=config.TRANSFORM\n",
        "    )\n",
        "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
        "    split_file_path = os.path.join(config.OUTPUT_DIR, getattr(config, 'SPLIT_DATASET_FILENAME', 'dataset_split.pth'))\n",
        "    if os.path.exists(split_file_path):\n",
        "        print(f\"Loading dataset split from {split_file_path}\")\n",
        "        split_data = torch.load(split_file_path)\n",
        "        train_indices, val_indices = split_data['train_indices'], split_data['val_indices']\n",
        "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "    else:\n",
        "        print(\"Creating new train/val split...\")\n",
        "        total_size = len(full_dataset)\n",
        "        train_size = int(total_size * 0.9)\n",
        "        val_size = total_size - train_size\n",
        "        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size]) # Using torch.random_split by default\n",
        "        torch.save({\n",
        "            'train_indices': train_dataset.indices,\n",
        "            'val_indices': val_dataset.indices,\n",
        "        }, split_file_path)\n",
        "        print(f\"Saved new dataset split to {split_file_path}\")\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee323a9",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tensor_to_pil(tensor_img):\n",
        "    tensor_img = (tensor_img.clamp(-1, 1) + 1) / 2\n",
        "    tensor_img = tensor_img.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    if tensor_img.shape[2] == 1:\n",
        "        tensor_img = tensor_img.squeeze(2)\n",
        "    # Ensure array is writeable for PIL\n",
        "    if not tensor_img.flags.writeable:\n",
        "        tensor_img = np.ascontiguousarray(tensor_img)\n",
        "    if tensor_img.dtype != np.uint8: # This check might be problematic if tensor_img is already uint8\n",
        "        pil_img_array = (tensor_img * 255).astype(np.uint8)\n",
        "    else:\n",
        "        pil_img_array = tensor_img # Already uint8\n",
        "    pil_img = PILImage.fromarray(pil_img_array)\n",
        "    return pil_img\n",
        "\n",
        "def save_visualization_samples(generated_tensor, gt_current_tensor, gt_prev_frames_sequence, epoch, save_dir, prefix=\"val_vis\"):\n",
        "    \"\"\"\n",
        "    Saves a visualization comparing a single generated image, its corresponding GT current image,\n",
        "    and the sequence of GT previous frames.\n",
        "    - generated_tensor, gt_current_tensor: [C, H, W]\n",
        "    - gt_prev_frames_sequence: [NumPrev, C, H, W]\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    generated_tensor = generated_tensor.detach().cpu()\n",
        "    gt_current_tensor = gt_current_tensor.detach().cpu()\n",
        "    gt_prev_frames_sequence = gt_prev_frames_sequence.detach().cpu()\n",
        "\n",
        "    num_prev_frames = config.NUM_PREV_FRAMES # Get from global config\n",
        "\n",
        "    num_cols = num_prev_frames + 1  # N previous frames + 1 current GT\n",
        "    # Create a 2 rows, num_cols columns subplot\n",
        "    fig, axs = plt.subplots(2, num_cols, figsize=(num_cols * 3, 6), squeeze=False) # squeeze=False ensures axs is always 2D\n",
        "\n",
        "    try:\n",
        "        # Top row: Previous GT frames and Current GT frame\n",
        "        for i in range(num_prev_frames):\n",
        "            axs[0, i].imshow(tensor_to_pil(gt_prev_frames_sequence[i]))\n",
        "            axs[0, i].set_title(f\"GT Prev {i+1}\")\n",
        "            axs[0, i].axis('off')\n",
        "            axs[1, i].axis('off') # Keep bottom row empty under previous GT frames\n",
        "\n",
        "        axs[0, num_prev_frames].imshow(tensor_to_pil(gt_current_tensor))\n",
        "        axs[0, num_prev_frames].set_title(\"GT Current\")\n",
        "        axs[0, num_prev_frames].axis('off')\n",
        "\n",
        "        # Bottom row, last column: Generated frame (aligned under Current GT)\n",
        "        axs[1, num_prev_frames].imshow(tensor_to_pil(generated_tensor))\n",
        "        axs[1, num_prev_frames].set_title(\"Generated\")\n",
        "        axs[1, num_prev_frames].axis('off')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error visualizing image for prefix {prefix}, epoch {epoch}: {e}\")\n",
        "        # Clear figure and display error text\n",
        "        for r in range(axs.shape[0]):\n",
        "            for c in range(axs.shape[1]):\n",
        "                axs[r,c].axis('off')\n",
        "        fig.clear() \n",
        "        plt.text(0.5, 0.5, \"Error displaying image\", ha=\"center\", va=\"center\", transform=fig.transFigure)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(save_dir, f\"{prefix}_epoch_{epoch:04d}.png\") \n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "    return save_path\n",
        "    \n",
        "def prepare_single_sample_for_sampler(sample_data, device):\n",
        "    target_img, action_single, prev_frames_flat_unbatched = sample_data # prev_frames_flat_unbatched is [NumPrev*C, H, W]\n",
        "    \n",
        "    # Add batch dimension (B=1) and move to device\n",
        "    gt_current_frame_batch = target_img.unsqueeze(0).to(device) # Shape: [1, C, H, W]\n",
        "    action_single_batch = action_single.unsqueeze(0).to(device) # Shape: [1, 1]\n",
        "    # prev_frames_flat_for_sampler_input needs to be [B, NumPrev*C, H, W] for the view later if used directly by sampler\n",
        "    # but for DIAMOND sampler, prev_obs is [B, NumPrevFrames, C, H, W]\n",
        "    \n",
        "    num_prev_frames_const = config.NUM_PREV_FRAMES\n",
        "    img_channels_const = DM_IMG_CHANNELS # Assumes DM_IMG_CHANNELS is globally available or from config\n",
        "    img_h_const = config.IMAGE_SIZE\n",
        "    img_w_const = config.IMAGE_SIZE\n",
        "\n",
        "    # Reshape prev_frames_flat_unbatched for sampler input [1, NumPrev, C, H, W]\n",
        "    prev_obs_for_sampler_input_5d = prev_frames_flat_unbatched.view(\n",
        "        num_prev_frames_const,\n",
        "        img_channels_const,\n",
        "        img_h_const,\n",
        "        img_w_const\n",
        "    ).unsqueeze(0).to(device) # Add batch dim and send to device\n",
        "\n",
        "    action_sequence_for_sampler = action_single_batch.repeat(1, config.NUM_PREV_FRAMES).long()\n",
        "    \n",
        "    # For visualization, we want the GT previous frames, unbatched and sequenced: [NumPrev, C, H, W]\n",
        "    gt_prev_frames_seq_for_vis = prev_frames_flat_unbatched.view(\n",
        "        num_prev_frames_const,\n",
        "        img_channels_const,\n",
        "        img_h_const,\n",
        "        img_w_const\n",
        "    ) # This is already on CPU if sample_data came directly from dataset before .to(device)\n",
        "      # It will be detached and moved to CPU again in save_visualization_samples\n",
        "    \n",
        "    return prev_obs_for_sampler_input_5d, action_sequence_for_sampler, gt_current_frame_batch, gt_prev_frames_seq_for_vis\n",
        "\n",
        "print(\"Visualization helpers defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import heapq, json, os\n",
        "import torch.nn.functional as F\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "WORST_K = 50\n",
        "\n",
        "def _to_batch(batch, device=config.DEVICE):\n",
        "    if isinstance(batch, models.Batch):\n",
        "        return batch.to(device)\n",
        "    target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
        "    b = target_img_batch.shape[0]\n",
        "    target_img_batch = target_img_batch.to(device)\n",
        "    action_batch = action_batch.to(device)\n",
        "    prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
        "    num_prev_frames = config.NUM_PREV_FRAMES\n",
        "    c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
        "    prev_seq = prev_frames_flat_batch.view(b, num_prev_frames, c, h, w)\n",
        "    obs = torch.cat((prev_seq, target_img_batch.unsqueeze(1)), dim=1)\n",
        "    act = action_batch.repeat(1, num_prev_frames).long()\n",
        "    mask = torch.ones(b, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
        "    return models.Batch(obs=obs, act=act, mask_padding=mask, info=[{}]*b)\n",
        "\n",
        "def mse_ssim(pred, tgt):\n",
        "    mse = F.mse_loss(pred, tgt, reduction='none').mean((1,2,3))\n",
        "    ssim_vals = torch.tensor([ssim(p.permute(1,2,0).cpu().numpy(),\n",
        "                               t.permute(1,2,0).cpu().numpy(),\n",
        "                               data_range=1.0, channel_axis=-1)\n",
        "                          for p, t in zip(pred, tgt)], device=pred.device)\n",
        "    return mse, ssim_vals\n",
        "\n",
        "def make_debug_panel(pred, tgt, prev_seq):\n",
        "    diff = (pred - tgt).abs()\n",
        "    diff = diff / diff.max().clamp(min=1e-4)\n",
        "    panel = torch.cat([*prev_seq, tgt, pred, diff], dim=0)\n",
        "    return vutils.make_grid(panel, nrow=len(prev_seq)+3, normalize=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_denoiser_epoch_v2(denoiser_model, val_dl, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, val_step_start=0):\n",
        "    denoiser_model.eval()\n",
        "    worst_mse, worst_ssim = [], []\n",
        "    all_mse, all_ssim = [], []\n",
        "    move_mse, move_ssim, stop_mse, stop_ssim = [], [], [], []\n",
        "    global_idx = 0\n",
        "    progress_bar = tqdm(val_dl, desc=f'Epoch {epoch_num_for_log} [Valid]', leave=False)\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        current_batch = _to_batch(batch, device)\n",
        "        pred, _ = denoiser_model.sample(current_batch)\n",
        "        tgt = current_batch.obs[:,-1]\n",
        "        mse, ssim_vals = mse_ssim(pred, tgt)\n",
        "        all_mse.append(mse); all_ssim.append(ssim_vals)\n",
        "        move_mask = current_batch.act[:,0] == 1\n",
        "        stop_mask = ~move_mask\n",
        "        if move_mask.any():\n",
        "            move_mse.append(mse[move_mask]); move_ssim.append(ssim_vals[move_mask])\n",
        "        if stop_mask.any():\n",
        "            stop_mse.append(mse[stop_mask]); stop_ssim.append(ssim_vals[stop_mask])\n",
        "        for i, (m, s) in enumerate(zip(mse, ssim_vals)):\n",
        "            item = (m.item(), s.item(), global_idx+i,\n",
        "                    pred[i].cpu(), tgt[i].cpu(),\n",
        "                    current_batch.obs[i,:-1].cpu(), current_batch.act[i,0].item())\n",
        "            if len(worst_mse) < WORST_K: heapq.heappush(worst_mse, item)\n",
        "            elif m > worst_mse[0][0]: heapq.heapreplace(worst_mse, item)\n",
        "            if len(worst_ssim) < WORST_K: heapq.heappush(worst_ssim, item)\n",
        "            elif s < worst_ssim[0][1]: heapq.heapreplace(worst_ssim, item)\n",
        "        global_idx += mse.numel()\n",
        "        if batch_idx % 10 == 0:\n",
        "            val_step = val_step_start + batch_idx\n",
        "            wandb.log({'val_batch_mse': mse.mean().item(), 'val_batch_ssim': ssim_vals.mean().item(), 'val_step': val_step})\n",
        "    mse_vec = torch.cat(all_mse); ssim_vec = torch.cat(all_ssim)\n",
        "    mse_move = torch.cat(move_mse).mean().item() if move_mse else 0\n",
        "    ssim_move = torch.cat(move_ssim).mean().item() if move_ssim else 0\n",
        "    mse_stop = torch.cat(stop_mse).mean().item() if stop_mse else 0\n",
        "    ssim_stop = torch.cat(stop_ssim).mean().item() if stop_ssim else 0\n",
        "    wandb.log({\n",
        "        'val/mse_mean': mse_vec.mean().item(),\n",
        "        'val/ssim_mean': ssim_vec.mean().item(),\n",
        "        'val/mse_move': mse_move,\n",
        "        'val/ssim_move': ssim_move,\n",
        "        'val/mse_stop': mse_stop,\n",
        "        'val/ssim_stop': ssim_stop\n",
        "    })\n",
        "    for rank, (m, s, ds_idx, pred_img, tgt_img, prev_seq, act) in enumerate(sorted(worst_mse, key=lambda x: -x[0])):\n",
        "        grid = make_debug_panel(pred_img, tgt_img, prev_seq)\n",
        "        wandb.log({f'worst_mse/pred_vs_gt_{rank}': wandb.Image(grid, caption=f'idx={ds_idx}, act={act}, mse={m:.4f}, ssim={s:.3f}')}, commit=False)\n",
        "    for rank, (m, s, ds_idx, pred_img, tgt_img, prev_seq, act) in enumerate(sorted(worst_ssim, key=lambda x: x[1])):\n",
        "        grid = make_debug_panel(pred_img, tgt_img, prev_seq)\n",
        "        wandb.log({f'worst_ssim/pred_vs_gt_{rank}': wandb.Image(grid, caption=f'idx={ds_idx}, act={act}, mse={m:.4f}, ssim={s:.3f}')}, commit=False)\n",
        "    bad_idx_path = os.path.join(config.OUTPUT_DIR, f'bad_samples_epoch{epoch_num_for_log}.json')\n",
        "    json.dump([int(t[2]) for t in worst_mse], open(bad_idx_path, 'w'))\n",
        "    final_step = val_step_start + len(val_dl)\n",
        "    return mse_vec.mean().item(), final_step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b097310c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_denoiser_epoch(denoiser_model, train_dl, opt, scheduler, grad_clip_val, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, train_step_start=0):\n",
        "    denoiser_model.train()\n",
        "    total_loss = 0.0\n",
        "    progress_bar = tqdm(train_dl, desc=f\"Epoch {epoch_num_for_log} [Train]\", leave=False)\n",
        "\n",
        "    num_prev_frames = config.NUM_PREV_FRAMES\n",
        "    c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
        "    accumulation_steps = config.ACCUMULATION_STEPS\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        if isinstance(batch, models.Batch):\n",
        "            current_batch_obj = batch.to(device)\n",
        "        else:\n",
        "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
        "            current_batch_size = target_img_batch.shape[0]\n",
        "            target_img_batch = target_img_batch.to(device)\n",
        "            action_batch = action_batch.to(device)\n",
        "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
        "\n",
        "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
        "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
        "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()\n",
        "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
        "        \n",
        "            # Corrected Batch instantiation\n",
        "            current_batch_obj = models.Batch(obs=batch_obs_tensor, act=batch_act_tensor, mask_padding=batch_mask_padding, info=[{}] * current_batch_size)\n",
        "\n",
        "        loss, logs = denoiser_model(current_batch_obj)\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            if grad_clip_val > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(denoiser_model.parameters(), grad_clip_val)\n",
        "            opt.step()\n",
        "            scheduler.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item() * accumulation_steps, \"LR\": scheduler.get_last_lr()[0]})\n",
        "\n",
        "        # Restored wandb logging\n",
        "        if batch_idx % 10 == 0:\n",
        "            train_step = train_step_start + batch_idx\n",
        "            wandb.log({\n",
        "                \"train_batch_loss\": loss.item() * accumulation_steps, # Log un-normalized loss\n",
        "                \"train_batch_denoising_loss\": logs.get(\"loss_denoising\"),\n",
        "                \"train_step\": train_step,\n",
        "            })\n",
        "\n",
        "    avg_loss = total_loss / len(train_dl) if len(train_dl) > 0 else 0.0\n",
        "    final_step = train_step_start + len(train_dl)\n",
        "    return avg_loss, final_step\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_denoiser_epoch_old(denoiser_model, val_dl, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, val_step_start=0):\n",
        "    denoiser_model.eval()\n",
        "    total_loss = 0.0\n",
        "    progress_bar = tqdm(val_dl, desc=f\"Epoch {epoch_num_for_log} [Valid]\", leave=False)\n",
        "    num_prev_frames = config.NUM_PREV_FRAMES\n",
        "    c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        if isinstance(batch, models.Batch):\n",
        "            current_batch_obj = batch.to(device)\n",
        "        else:\n",
        "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
        "            current_batch_size = target_img_batch.shape[0]\n",
        "            target_img_batch = target_img_batch.to(device)\n",
        "            action_batch = action_batch.to(device)\n",
        "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
        "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
        "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
        "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()\n",
        "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
        "        \n",
        "            # Corrected Batch instantiation\n",
        "            current_batch_obj = models.Batch(obs=batch_obs_tensor, act=batch_act_tensor, mask_padding=batch_mask_padding, info=[{}] * current_batch_size)\n",
        "        \n",
        "        loss, logs = denoiser_model(current_batch_obj)\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"Val Loss\": loss.item()})\n",
        "        \n",
        "        # Restored wandb logging\n",
        "        if batch_idx % 10 == 0:\n",
        "            val_step = val_step_start + batch_idx\n",
        "            wandb.log({\n",
        "                \"val_batch_loss\": loss.item(),\n",
        "                \"val_batch_denoising_loss\": logs.get(\"loss_denoising\"),\n",
        "                \"val_step\": val_step,\n",
        "            })\n",
        "             \n",
        "    avg_loss = total_loss / len(val_dl) if len(val_dl) > 0 else 0.0\n",
        "    final_step = val_step_start + len(val_dl)\n",
        "    return avg_loss, final_step\n",
        "\n",
        "\n",
        "print(\"Training and validation epoch functions adapted for Batch object and Denoiser.forward.\")\n",
        "\n",
        "\n",
        "def train_diamond_model(train_loader, val_loader, start_checkpoint=None, max_steps=None):\n",
        "    \"\"\"\n",
        "    Train a denoiser model with robust, step-based early stopping.\n",
        "    \"\"\"\n",
        "    device = config.DEVICE\n",
        "    num_steps = max_steps or config.NUM_TRAIN_STEPS\n",
        "\n",
        "    # --- Model and Optimizer Setup (remains the same) ---\n",
        "    inner_cfg = models.InnerModelConfig(\n",
        "        img_channels=config.DM_IMG_CHANNELS,\n",
        "        num_steps_conditioning=config.NUM_PREV_FRAMES,\n",
        "        cond_channels=config.DM_COND_CHANNELS,\n",
        "        depths=config.DM_UNET_DEPTHS,\n",
        "        channels=config.DM_UNET_CHANNELS,\n",
        "        attn_depths=config.DM_UNET_ATTN_DEPTHS,\n",
        "        num_actions=config.DM_NUM_ACTIONS,\n",
        "        is_upsampler=config.DM_IS_UPSAMPLER,\n",
        "    )\n",
        "    denoiser_cfg = models.DenoiserConfig(\n",
        "        inner_model=inner_cfg,\n",
        "        sigma_data=config.DM_SIGMA_DATA,\n",
        "        sigma_offset_noise=config.DM_SIGMA_OFFSET_NOISE,\n",
        "        noise_previous_obs=config.DM_NOISE_PREVIOUS_OBS,\n",
        "        upsampling_factor=config.DM_UPSAMPLING_FACTOR,\n",
        "    )\n",
        "    denoiser = models.Denoiser(cfg=denoiser_cfg).to(device)\n",
        "    sigma_cfg = models.SigmaDistributionConfig(\n",
        "        loc=config.DM_SIGMA_P_MEAN,\n",
        "        scale=config.DM_SIGMA_P_STD,\n",
        "        sigma_min=config.DM_SIGMA_MIN_TRAIN,\n",
        "        sigma_max=config.DM_SIGMA_MAX_TRAIN,\n",
        "    )\n",
        "    denoiser.setup_training(sigma_cfg)\n",
        "\n",
        "    start_step_offset = -1\n",
        "    if start_checkpoint and os.path.exists(start_checkpoint):\n",
        "        state = torch.load(start_checkpoint, map_location=device)\n",
        "        if 'model_state_dict' in state:\n",
        "            denoiser.load_state_dict(state['model_state_dict'])\n",
        "        start_step_offset = state.get('step', -1)\n",
        "\n",
        "    opt = torch.optim.AdamW(\n",
        "        denoiser.parameters(),\n",
        "        lr=config.LEARNING_RATE,\n",
        "        weight_decay=config.LEARNING_RATE_WEIGHT_DECAY,\n",
        "        eps=config.LEARNING_RATE_EPS,\n",
        "    )\n",
        "\n",
        "    def lr_lambda(step: int):\n",
        "        warmup = config.LEARNING_RATE_WARMUP_STEPS\n",
        "        return float(step) / float(max(1, warmup)) if step < warmup else 1.0\n",
        "    scheduler = LambdaLR(opt, lr_lambda)\n",
        "    \n",
        "    # --- Robust Early Stopping & Checkpointing Setup ---\n",
        "    best_val_loss = float('inf')\n",
        "    steps_since_last_improvement = 0\n",
        "    best_model_state_dict = None\n",
        "\n",
        "    validate_every = getattr(config, 'VALIDATE_EVERY', 50)\n",
        "    patience_steps = getattr(config, 'EARLY_STOP_PATIENCE_STEPS', 150)\n",
        "    \n",
        "    # Divergence Guard Setup\n",
        "    divergence_patience = getattr(config, 'TRAIN_DIVERGE_PATIENCE_CHECKS', 3)\n",
        "    divergence_threshold = getattr(config, 'TRAIN_DIVERGE_THRESHOLD', 0.05)\n",
        "    last_train_loss = float('inf')\n",
        "    divergence_counter = 0\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    val_step_count = 0\n",
        "    train_iter = iter(train_loader)\n",
        "    pbar = tqdm(range(num_steps), desc=\"Incremental Training Steps\")\n",
        "\n",
        "    # Sampler for visualization (similar to _main_training)\n",
        "    sampler_cfg_vis = models.DiffusionSamplerConfig(\n",
        "        num_steps_denoising=config.SAMPLER_NUM_STEPS,\n",
        "        sigma_min=config.SAMPLER_SIGMA_MIN,\n",
        "        sigma_max=config.SAMPLER_SIGMA_MAX,\n",
        "        rho=config.SAMPLER_RHO,\n",
        "        order=config.SAMPLER_ORDER,\n",
        "        s_churn=config.SAMPLER_S_CHURN,\n",
        "        s_tmin=config.SAMPLER_S_TMIN,\n",
        "        s_tmax=config.SAMPLER_S_TMAX,\n",
        "        s_noise=config.SAMPLER_S_NOISE\n",
        "    )\n",
        "    diffusion_sampler_vis = models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg_vis)\n",
        "\n",
        "    # Prepare filtered validation subsets for visualization (similar to _main_training)\n",
        "    val_stopped_subset_inc = []\n",
        "    val_moving_subset_inc = []\n",
        "    if hasattr(val_loader, 'dataset') and len(val_loader.dataset) > 0:\n",
        "        val_dataset_for_filter = val_loader.dataset\n",
        "        val_stopped_subset_inc = filter_dataset_by_action(val_dataset_for_filter, target_actions=0.0)\n",
        "        moving_action_val_vis_inc = getattr(config, 'MOVING_ACTION_VALUE_FOR_VIS', 0.13)\n",
        "        val_moving_subset_inc = filter_dataset_by_action(val_dataset_for_filter, target_actions=moving_action_val_vis_inc)\n",
        "    \n",
        "    for step in pbar:\n",
        "        # --- Standard Training Step (remains the same) ---\n",
        "        # (Batch creation, forward pass, backward pass, optimizer step)\n",
        "        try:\n",
        "            batch = next(train_iter)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(train_loader)\n",
        "            batch = next(train_iter)\n",
        "\n",
        "        # (Code to prepare batch object `current_batch_obj` remains the same)\n",
        "        if isinstance(batch, models.Batch):\n",
        "            current_batch_obj = batch.to(device)\n",
        "        else:\n",
        "            # Unpack the batch from the DataLoader\n",
        "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
        "\n",
        "            # Move tensors to the correct device\n",
        "            target_img_batch = target_img_batch.to(device)\n",
        "            action_batch = action_batch.to(device)\n",
        "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
        "\n",
        "            # Reconstruct the logic from train_denoiser_epoch to create the Batch object\n",
        "            current_batch_size = target_img_batch.shape[0]\n",
        "            num_prev_frames = config.NUM_PREV_FRAMES\n",
        "            c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
        "\n",
        "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
        "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
        "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()  # Ensure this matches the expected action format for the model\n",
        "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
        "\n",
        "            current_batch_obj = models.Batch(\n",
        "                obs=batch_obs_tensor,\n",
        "                act=batch_act_tensor,\n",
        "                mask_padding=batch_mask_padding,\n",
        "                info=[{}] * current_batch_size\n",
        "            )\n",
        "\n",
        "        denoiser.train()\n",
        "        loss, logs = denoiser(current_batch_obj) \n",
        "        train_loss_val = loss.item()\n",
        "        loss = loss / config.ACCUMULATION_STEPS\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % config.ACCUMULATION_STEPS == 0:\n",
        "            if config.GRAD_CLIP_VALUE > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(denoiser.parameters(), config.GRAD_CLIP_VALUE)\n",
        "            opt.step()\n",
        "            scheduler.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        # Logging to wandb (more frequently for steps)\n",
        "        train_step_count = start_step_offset + step + 1 # Define train_step_count here\n",
        "        if wandb.run and (step + 1) % 10 == 0: # Log every 10 steps\n",
        "            wandb.log({\n",
        "                \"incremental_step_train_loss\": train_loss_val,\n",
        "                \"incremental_step_denoising_loss\": logs.get(\"loss_denoising\"),\n",
        "                \"incremental_step_learning_rate\": scheduler.get_last_lr()[0],\n",
        "                \"train_step\": train_step_count,\n",
        "            })\n",
        "        \n",
        "        # --- Validation, Early Stopping, and Divergence Check ---\n",
        "        if (step + 1) % validate_every == 0 or (step + 1) == num_steps:\n",
        "            val_step_start = val_step_count\n",
        "            current_val_loss, val_step_count = validate_denoiser_epoch_v2(\n",
        "                denoiser, val_loader, device, step + 1, 0, 0, val_step_start=val_step_start\n",
        "            )\n",
        "\n",
        "            # Log validation loss\n",
        "            if wandb.run:\n",
        "                wandb.log({\"incremental_eval_val_loss\": current_val_loss, \"val_step\": val_step_start})\n",
        "            \n",
        "            # Image Sampling (similar to _main_training, simplified for step-based)\n",
        "            # Tied to validation frequency for now.\n",
        "            if wandb.run and hasattr(config, 'SAMPLE_EVERY') and (step + 1) % config.SAMPLE_EVERY == 0 :\n",
        "                denoiser.eval()\n",
        "                vis_wandb_log_data_inc = {}\n",
        "                fixed_sample_idx_inc = getattr(config, 'FIXED_VIS_SAMPLE_IDX', 0)\n",
        "\n",
        "                if hasattr(val_loader, 'dataset') and fixed_sample_idx_inc < len(val_loader.dataset):\n",
        "                    fixed_sample_data_inc = val_loader.dataset[fixed_sample_idx_inc]\n",
        "                    # Ensure sample_data is a tuple (img, act, prev_frames_flat)\n",
        "                    if not (isinstance(fixed_sample_data_inc, tuple) and len(fixed_sample_data_inc) == 3):\n",
        "                         # Try to get it from .dataset if val_loader.dataset is a Subset\n",
        "                        if isinstance(val_loader.dataset, torch.utils.data.Subset):\n",
        "                            original_dataset = val_loader.dataset.dataset\n",
        "                            original_idx = val_loader.dataset.indices[fixed_sample_idx_inc]\n",
        "                            fixed_sample_data_inc = original_dataset[original_idx]\n",
        "                        else:\n",
        "                            print(f\"Skipping fixed sample visualization: data format error or direct access failed.\")\n",
        "                            fixed_sample_data_inc = None \n",
        "                            \n",
        "                    if fixed_sample_data_inc:\n",
        "                        prev_obs_fixed_inc, prev_act_fixed_inc, gt_fixed_batch_inc, gt_prev_frames_fixed_seq_inc = prepare_single_sample_for_sampler(fixed_sample_data_inc, device)\n",
        "                        with torch.no_grad():\n",
        "                            generated_output_tuple_fixed_inc = diffusion_sampler_vis.sample(prev_obs=prev_obs_fixed_inc, prev_act=prev_act_fixed_inc)\n",
        "                        if generated_output_tuple_fixed_inc:\n",
        "                            generated_image_to_save_fixed_inc = generated_output_tuple_fixed_inc[0][0]\n",
        "                            gt_image_to_save_fixed_inc = gt_fixed_batch_inc[0]\n",
        "                            vis_path_fixed_inc = save_visualization_samples(\n",
        "                                generated_image_to_save_fixed_inc, gt_image_to_save_fixed_inc, gt_prev_frames_fixed_seq_inc,\n",
        "                                step + 1, config.SAMPLE_DIR, prefix=f\"inc_vis_fixed_step{step+1}\"\n",
        "                            )\n",
        "                            vis_wandb_log_data_inc[f\"incremental_samples/fixed_idx_{fixed_sample_idx_inc}\"] = wandb.Image(vis_path_fixed_inc, caption=f\"Step {step+1} Fixed Sample\")\n",
        "\n",
        "                # Simplified: Add one random sample from val_stopped_subset_inc if available\n",
        "                if len(val_stopped_subset_inc) > 0:\n",
        "                    stopped_sample_data_inc = val_stopped_subset_inc[random.randint(0, len(val_stopped_subset_inc) - 1)]\n",
        "                    prev_obs_stop, prev_act_stop, gt_batch_stop, gt_prev_seq_stop = prepare_single_sample_for_sampler(stopped_sample_data_inc, device)\n",
        "                    with torch.no_grad():\n",
        "                        gen_out_stop = diffusion_sampler_vis.sample(prev_obs=prev_obs_stop, prev_act=prev_act_stop)\n",
        "                    if gen_out_stop:\n",
        "                        vis_path_stop = save_visualization_samples(gen_out_stop[0][0], gt_batch_stop[0], gt_prev_seq_stop, step+1, config.SAMPLE_DIR, prefix=f\"inc_vis_stopped_step{step+1}\")\n",
        "                        vis_wandb_log_data_inc[\"incremental_samples/random_stopped\"] = wandb.Image(vis_path_stop, caption=f\"Step {step+1} Random Stopped\")\n",
        "\n",
        "                # Simplified: Add one random sample from val_moving_subset_inc_subset_inc if available\n",
        "                if len(val_moving_subset_inc) > 0:\n",
        "                    moving_sample_data_inc = val_moving_subset_inc[random.randint(0, len(val_moving_subset_inc) - 1)]\n",
        "                    prev_obs_move, prev_act_move, gt_batch_move, gt_prev_seq_move = prepare_single_sample_for_sampler(moving_sample_data_inc, device)\n",
        "                    with torch.no_grad():\n",
        "                        gen_out_move = diffusion_sampler_vis.sample(prev_obs=prev_obs_move, prev_act=prev_act_move)\n",
        "                    if gen_out_move:\n",
        "                        vis_path_move = save_visualization_samples(gen_out_move[0][0], gt_batch_move[0], gt_prev_seq_move, step+1, config.SAMPLE_DIR, prefix=f\"inc_vis_moving_step{step+1}\")\n",
        "                        vis_wandb_log_data_inc[\"incremental_samples/random_moving\"] = wandb.Image(vis_path_move, caption=f\"Step {step+1} Random Moving\")\n",
        "                \n",
        "                if vis_wandb_log_data_inc:\n",
        "                    vis_wandb_log_data_inc[\"train_step\"] = train_step_count # Use the same train_step_count\n",
        "                    wandb.log(vis_wandb_log_data_inc)\n",
        "                denoiser.train() # Set back to train mode\n",
        "\n",
        "            # Check for improvement\n",
        "            if current_val_loss < best_val_loss:\n",
        "                best_val_loss = current_val_loss\n",
        "                steps_since_last_improvement = 0\n",
        "                best_model_state_dict = copy.deepcopy(denoiser.state_dict())\n",
        "                pbar.set_description(f\"New best val_loss: {best_val_loss:.4f}\")\n",
        "            else:\n",
        "                steps_since_last_improvement += validate_every\n",
        "            \n",
        "            # Check for training loss divergence\n",
        "            if train_loss_val > last_train_loss * (1 + divergence_threshold):\n",
        "                divergence_counter += 1\n",
        "            else:\n",
        "                divergence_counter = 0 # Reset if loss is stable\n",
        "            last_train_loss = train_loss_val\n",
        "\n",
        "            # Check stopping conditions\n",
        "            if steps_since_last_improvement >= patience_steps:\n",
        "                print(f\"🛑 Early stopping triggered: No improvement in {patience_steps} steps.\")\n",
        "                if wandb.run: wandb.log({\"early_stop_reason\": \"patience_met\", \"early_stop_step\": step + 1})\n",
        "                break\n",
        "            \n",
        "            if divergence_counter >= divergence_patience:\n",
        "                print(f\"🛑 Early stopping triggered: Training loss diverged for {divergence_patience} checks.\")\n",
        "                if wandb.run: wandb.log({\"early_stop_reason\": \"loss_diverged\", \"early_stop_step\": step + 1})\n",
        "                break\n",
        "\n",
        "        pbar.set_postfix({\"Train Loss\": f\"{train_loss_val:.4f}\", \"Best Val\": f\"{best_val_loss:.4f}\", \"Steps w/o Improve\": f\"{steps_since_last_improvement}\"})\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    # --- Restore Best Model and Save ---\n",
        "    if best_model_state_dict:\n",
        "        print(f\"✅ Restoring model to best validation loss: {best_val_loss:.4f}\")\n",
        "        denoiser.load_state_dict(best_model_state_dict)\n",
        "    \n",
        "    # Save the final, best model for promotion testing\n",
        "    final_best_path = os.path.join(config.CHECKPOINT_DIR, \"tmp_incremental_best.pth\")\n",
        "    torch.save({\"model_state_dict\": denoiser.state_dict(), 'step': step + 1, 'val_loss': best_val_loss}, final_best_path)\n",
        "\n",
        "    return final_best_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e7d030",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _main_training():\n",
        "    print(\"--- Main Training Execution --- \")\n",
        "\n",
        "    print(\"--- Configuration ---\")\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # Denoiser & InnerModel specific\n",
        "    DM_SIGMA_DATA = getattr(config, 'DM_SIGMA_DATA', 0.5)\n",
        "    DM_SIGMA_OFFSET_NOISE = getattr(config, 'DM_SIGMA_OFFSET_NOISE', 0.1)\n",
        "    DM_NOISE_PREVIOUS_OBS = getattr(config, 'DM_NOISE_PREVIOUS_OBS', True)\n",
        "    # DM_IMG_CHANNELS is global for prepare_single_sample_for_sampler\n",
        "    DM_NUM_STEPS_CONDITIONING = getattr(config, 'DM_NUM_STEPS_CONDITIONING', config.NUM_PREV_FRAMES)\n",
        "    DM_COND_CHANNELS = getattr(config, 'DM_COND_CHANNELS', 256)\n",
        "    DM_UNET_DEPTHS = getattr(config, 'DM_UNET_DEPTHS', [2, 2, 2, 2])\n",
        "    DM_UNET_CHANNELS = getattr(config, 'DM_UNET_CHANNELS', [128, 256, 512, 1024])\n",
        "    DM_UNET_ATTN_DEPTHS = getattr(config, 'DM_UNET_ATTN_DEPTHS', [False, False, True, True])\n",
        "    # DM_NUM_ACTIONS is global for prepare_single_sample_for_sampler\n",
        "    DM_IS_UPSAMPLER = getattr(config, 'DM_IS_UPSAMPLER', False)\n",
        "    DM_UPSAMPLING_FACTOR = getattr(config, 'DM_UPSAMPLING_FACTOR', None)\n",
        "\n",
        "    # Sampler specific (for inference/visualization)\n",
        "    SAMPLER_NUM_STEPS = getattr(config, 'SAMPLER_NUM_STEPS', 50)\n",
        "    SAMPLER_SIGMA_MIN = getattr(config, 'SAMPLER_SIGMA_MIN', 0.002)\n",
        "    SAMPLER_SIGMA_MAX = getattr(config, 'SAMPLER_SIGMA_MAX', 80.0)\n",
        "    SAMPLER_RHO = getattr(config, 'SAMPLER_RHO', 7.0)\n",
        "    SAMPLER_ORDER = getattr(config, 'SAMPLER_ORDER', 1)\n",
        "    SAMPLER_S_CHURN = getattr(config, 'SAMPLER_S_CHURN', 0.0)\n",
        "    SAMPLER_S_TMIN = getattr(config, 'SAMPLER_S_TMIN', 0.0)\n",
        "    SAMPLER_S_TMAX = getattr(config, 'SAMPLER_S_TMAX', float(\"inf\"))\n",
        "    SAMPLER_S_NOISE = getattr(config, 'SAMPLER_S_NOISE', 1.0)\n",
        "\n",
        "    # Training specific\n",
        "    BATCH_SIZE = config.BATCH_SIZE\n",
        "    LEARNING_RATE = config.LEARNING_RATE\n",
        "    NUM_EPOCHS = config.NUM_EPOCHS\n",
        "    SAVE_MODEL_EVERY = config.SAVE_MODEL_EVERY\n",
        "    SAMPLE_EVERY = config.SAMPLE_EVERY\n",
        "    PLOT_EVERY = config.PLOT_EVERY\n",
        "    GRAD_CLIP_VALUE = getattr(config, 'GRAD_CLIP_VALUE', 1.0)\n",
        "    DM_SIGMA_P_MEAN = getattr(config, 'DM_SIGMA_P_MEAN', -1.2)\n",
        "    DM_SIGMA_P_STD = getattr(config, 'DM_SIGMA_P_STD', 1.2)\n",
        "    DM_SIGMA_MIN_TRAIN = getattr(config, 'DM_SIGMA_MIN_TRAIN', 0.002)\n",
        "    DM_SIGMA_MAX_TRAIN = getattr(config, 'DM_SIGMA_MAX_TRAIN', 80.0)\n",
        "    EARLY_STOPPING_PATIENCE = getattr(config, 'EARLY_STOPPING_PATIENCE', 10)\n",
        "    EARLY_STOPPING_MIN_EPOCHS = getattr(config, 'MIN_EPOCHS', 20)\n",
        "    EARLY_STOPPING_PERCENTAGE = getattr(config, 'EARLY_STOPPING_PERCENTAGE', 0.1)\n",
        "    TRAIN_MOVING_AVG_WINDOW = getattr(config, 'TRAIN_MOVING_AVG_WINDOW', 10)\n",
        "    VAL_MOVING_AVG_WINDOW = getattr(config, 'VAL_MOVING_AVG_WINDOW', 5)\n",
        "    print(\"Configuration loaded for _main_training.\")\n",
        "\n",
        "    wandb_config = {\n",
        "        'DM_SIGMA_DATA': DM_SIGMA_DATA,\n",
        "        'DM_SIGMA_OFFSET_NOISE': DM_SIGMA_OFFSET_NOISE,\n",
        "        'DM_NOISE_PREVIOUS_OBS': DM_NOISE_PREVIOUS_OBS,\n",
        "        'DM_IMG_CHANNELS': DM_IMG_CHANNELS,\n",
        "        'DM_NUM_STEPS_CONDITIONING': DM_NUM_STEPS_CONDITIONING,\n",
        "        'DM_COND_CHANNELS': DM_COND_CHANNELS,\n",
        "        'DM_UNET_DEPTHS': DM_UNET_DEPTHS,\n",
        "        'DM_UNET_CHANNELS': DM_UNET_CHANNELS,\n",
        "        'DM_UNET_ATTN_DEPTHS': DM_UNET_ATTN_DEPTHS,\n",
        "        'DM_NUM_ACTIONS': DM_NUM_ACTIONS,\n",
        "        'DM_IS_UPSAMPLER': DM_IS_UPSAMPLER,\n",
        "        'DM_UPSAMPLING_FACTOR': DM_UPSAMPLING_FACTOR,\n",
        "        'SAMPLER_NUM_STEPS': SAMPLER_NUM_STEPS,\n",
        "        'SAMPLER_SIGMA_MIN': SAMPLER_SIGMA_MIN,\n",
        "        'SAMPLER_SIGMA_MAX': SAMPLER_SIGMA_MAX,\n",
        "        'SAMPLER_RHO': SAMPLER_RHO,\n",
        "        'SAMPLER_ORDER': SAMPLER_ORDER,\n",
        "        'SAMPLER_S_CHURN': SAMPLER_S_CHURN,\n",
        "        'SAMPLER_S_TMIN': SAMPLER_S_TMIN,\n",
        "        'SAMPLER_S_TMAX': SAMPLER_S_TMAX,\n",
        "        'SAMPLER_S_NOISE': SAMPLER_S_NOISE,\n",
        "        'BATCH_SIZE': BATCH_SIZE,\n",
        "        'LEARNING_RATE': LEARNING_RATE,\n",
        "        'NUM_EPOCHS': NUM_EPOCHS,\n",
        "        'GRAD_CLIP_VALUE': GRAD_CLIP_VALUE,\n",
        "        'DM_SIGMA_P_MEAN': DM_SIGMA_P_MEAN,\n",
        "        'DM_SIGMA_P_STD': DM_SIGMA_P_STD,\n",
        "        'DM_SIGMA_MIN_TRAIN': DM_SIGMA_MIN_TRAIN,\n",
        "        'DM_SIGMA_MAX_TRAIN': DM_SIGMA_MAX_TRAIN,\n",
        "        'EARLY_STOPPING_PATIENCE': EARLY_STOPPING_PATIENCE,\n",
        "        'EARLY_STOPPING_MIN_EPOCHS': EARLY_STOPPING_MIN_EPOCHS,\n",
        "        'EARLY_STOPPING_PERCENTAGE': EARLY_STOPPING_PERCENTAGE,\n",
        "        'TRAIN_MOVING_AVG_WINDOW': TRAIN_MOVING_AVG_WINDOW,\n",
        "        'VAL_MOVING_AVG_WINDOW': VAL_MOVING_AVG_WINDOW,\n",
        "        'IMAGE_SIZE': config.IMAGE_SIZE,\n",
        "        'NUM_PREV_FRAMES': config.NUM_PREV_FRAMES,\n",
        "        'PROJECT_NAME': getattr(config, 'PROJECT_NAME', 'jetbot-diamond-world-model'),\n",
        "        'FIXED_VIS_SAMPLE_IDX': getattr(config, 'FIXED_VIS_SAMPLE_IDX', 0),\n",
        "        'MOVING_ACTION_VALUE_FOR_VIS': getattr(config, 'MOVING_ACTION_VALUE_FOR_VIS', 0.13)\n",
        "    }\n",
        "    wandb.init(project=wandb_config['PROJECT_NAME'], config=wandb_config)\n",
        "    print(\"Wandb initialized for _main_training.\")\n",
        "\n",
        "    print(\"--- Initializing Models for _main_training ---\")\n",
        "    try:\n",
        "        inner_model_config = models.InnerModelConfig(\n",
        "            img_channels=DM_IMG_CHANNELS,\n",
        "            num_steps_conditioning=DM_NUM_STEPS_CONDITIONING,\n",
        "            cond_channels=DM_COND_CHANNELS,\n",
        "            depths=DM_UNET_DEPTHS,\n",
        "            channels=DM_UNET_CHANNELS,\n",
        "            attn_depths=DM_UNET_ATTN_DEPTHS,\n",
        "            num_actions=DM_NUM_ACTIONS,\n",
        "            is_upsampler=DM_IS_UPSAMPLER\n",
        "        )\n",
        "        # inner_model_instance = models.InnerModel(inner_model_config).to(DEVICE) # Not strictly needed if only denoiser is used\n",
        "        # print(f\"InnerModelImpl parameter count: {sum(p.numel() for p in inner_model_instance.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "        denoiser_cfg = models.DenoiserConfig(\n",
        "            inner_model=inner_model_config, \n",
        "            sigma_data=DM_SIGMA_DATA,\n",
        "            sigma_offset_noise=DM_SIGMA_OFFSET_NOISE,\n",
        "            noise_previous_obs=DM_NOISE_PREVIOUS_OBS,\n",
        "            upsampling_factor=DM_UPSAMPLING_FACTOR\n",
        "        )\n",
        "        denoiser = models.Denoiser(cfg=denoiser_cfg).to(DEVICE)\n",
        "        sigma_dist_train_cfg = models.SigmaDistributionConfig(\n",
        "            loc=DM_SIGMA_P_MEAN, scale=DM_SIGMA_P_STD,\n",
        "            sigma_min=DM_SIGMA_MIN_TRAIN, sigma_max=DM_SIGMA_MAX_TRAIN\n",
        "        )\n",
        "        denoiser.setup_training(sigma_dist_train_cfg)\n",
        "        print(f\"Denoiser model created for _main_training. Total parameter count: {sum(p.numel() for p in denoiser.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "        sampler_cfg = models.DiffusionSamplerConfig(\n",
        "            num_steps_denoising=SAMPLER_NUM_STEPS, sigma_min=SAMPLER_SIGMA_MIN,\n",
        "            sigma_max=SAMPLER_SIGMA_MAX, rho=SAMPLER_RHO, order=SAMPLER_ORDER,\n",
        "            s_churn=SAMPLER_S_CHURN, s_tmin=SAMPLER_S_TMIN,\n",
        "            s_tmax=SAMPLER_S_TMAX, s_noise=SAMPLER_S_NOISE\n",
        "        )\n",
        "        diffusion_sampler = models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg)\n",
        "        print(\"DiffusionSampler created for visualization in _main_training.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing models in _main_training: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(\"--- Setting up Optimizer and Scheduler for _main_training ---\")\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        denoiser.parameters(), lr=LEARNING_RATE, \n",
        "        weight_decay=config.LEARNING_RATE_WEIGHT_DECAY, eps=config.LEARNING_RATE_EPS\n",
        "    )\n",
        "    print(f\"Optimizer: AdamW with LR={LEARNING_RATE}\")\n",
        "    def lr_lambda_main(current_step: int):\n",
        "        if current_step < config.LEARNING_RATE_WARMUP_STEPS:\n",
        "            return float(current_step) / float(max(1, config.LEARNING_RATE_WARMUP_STEPS))\n",
        "        return 1.0\n",
        "    lr_scheduler = LambdaLR(optimizer, lr_lambda_main)\n",
        "    print(f\"LR Scheduler: LambdaLR with {config.LEARNING_RATE_WARMUP_STEPS} warmup steps.\")\n",
        "    wandb.watch(denoiser, log=\"all\", log_freq=100)\n",
        "    print(\"Wandb watching denoiser model.\")\n",
        "\n",
        "    START_EPOCH = 0\n",
        "    BEST_TRAIN_LOSS_MA_FROM_CKPT = float('inf')\n",
        "    PREVIOUS_BEST_TRAIN_MODEL_PATH = None\n",
        "    BEST_VAL_LOSS_MA_FROM_CKPT = float('inf')\n",
        "    PREVIOUS_BEST_VAL_MODEL_PATH = None\n",
        "\n",
        "    load_path_config_main = config.LOAD_CHECKPOINT\n",
        "    best_train_loss_model_default_path_main = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_train_loss.pth\")\n",
        "    best_val_loss_model_default_path_main = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\")\n",
        "    load_path_main = load_path_config_main\n",
        "    if load_path_main:\n",
        "        print(f\"Attempting to load checkpoint from config.LOAD_CHECKPOINT: {load_path_main}\")\n",
        "    elif os.path.exists(best_val_loss_model_default_path_main):\n",
        "        load_path_main = best_val_loss_model_default_path_main\n",
        "        print(f\"Using existing best_val_loss model: {load_path_main}\")\n",
        "    elif os.path.exists(best_train_loss_model_default_path_main):\n",
        "        load_path_main = best_train_loss_model_default_path_main\n",
        "        print(f\"Using existing best_train_loss model: {load_path_main}\")\n",
        "    \n",
        "    if load_path_main and os.path.exists(load_path_main):\n",
        "        print(f\"Loading checkpoint for _main_training from: {load_path_main}\")\n",
        "        try:\n",
        "            checkpoint = torch.load(load_path_main, map_location=DEVICE)\n",
        "            denoiser.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            START_EPOCH = checkpoint.get('epoch', 0) + 1\n",
        "            BEST_TRAIN_LOSS_MA_FROM_CKPT = checkpoint.get('best_train_loss_ma', float('inf'))\n",
        "            BEST_VAL_LOSS_MA_FROM_CKPT = checkpoint.get('best_val_loss_ma', float('inf'))\n",
        "            if load_path_main.endswith(\"denoiser_model_best_train_loss.pth\"): PREVIOUS_BEST_TRAIN_MODEL_PATH = load_path_main\n",
        "            elif load_path_main.endswith(\"denoiser_model_best_val_loss.pth\"): PREVIOUS_BEST_VAL_MODEL_PATH = load_path_main\n",
        "            print(f\"Resuming _main_training from epoch {START_EPOCH}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint in _main_training: {e}. Starting fresh.\")\n",
        "            START_EPOCH = 0\n",
        "    else:\n",
        "        print(\"No checkpoint found or specified for _main_training. Starting fresh.\")\n",
        "\n",
        "    train_dataset, val_dataset = split_dataset()\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True, drop_last=False)\n",
        "    print(f\"Training dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")\n",
        "    \n",
        "    val_stopped_subset, val_moving_subset = [], []\n",
        "    if len(val_dataset) > 0:\n",
        "        print(\"Preparing filtered validation subsets for visualization...\")\n",
        "        val_stopped_subset = filter_dataset_by_action(val_dataset, target_actions=0.0)\n",
        "        moving_action_val = wandb_config['MOVING_ACTION_VALUE_FOR_VIS']\n",
        "        val_moving_subset = filter_dataset_by_action(val_dataset, target_actions=moving_action_val)\n",
        "        print(f\"Found {len(val_stopped_subset)} stopped and {len(val_moving_subset)} moving samples.\")\n",
        "    else:\n",
        "        from torch.utils.data import Subset # Ensure Subset is available if val_dataset is empty\n",
        "        val_stopped_subset = Subset(val_dataset, [])\n",
        "        val_moving_subset = Subset(val_dataset, [])\n",
        "    \n",
        "    print(\"--- Starting Training Process in _main_training ---\")\n",
        "    overall_training_start_time = time.time()\n",
        "    all_train_losses_for_plot, all_val_losses_for_plot = [], []\n",
        "    train_loss_moving_avg_q = deque(maxlen=TRAIN_MOVING_AVG_WINDOW)\n",
        "    best_train_loss_ma = BEST_TRAIN_LOSS_MA_FROM_CKPT\n",
        "    epochs_without_improvement_train = 0\n",
        "    previous_best_train_model_path = PREVIOUS_BEST_TRAIN_MODEL_PATH\n",
        "    val_loss_moving_avg_q = deque(maxlen=VAL_MOVING_AVG_WINDOW)\n",
        "    best_val_loss_ma = BEST_VAL_LOSS_MA_FROM_CKPT\n",
        "    previous_best_val_model_path = PREVIOUS_BEST_VAL_MODEL_PATH\n",
        "    final_epoch_completed = START_EPOCH - 1\n",
        "    num_train_batches = len(train_dataloader)\n",
        "    num_val_batches = len(val_dataloader)\n",
        "    \n",
        "    train_step_count = 0\n",
        "    val_step_count = 0\n",
        "    for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        current_epoch_num_for_log = epoch + 1\n",
        "        avg_train_loss, train_step_count = train_denoiser_epoch(\n",
        "            denoiser_model=denoiser, train_dl=train_dataloader, opt=optimizer,\n",
        "            scheduler=lr_scheduler, grad_clip_val=GRAD_CLIP_VALUE, device=DEVICE,\n",
        "            epoch_num_for_log=current_epoch_num_for_log,\n",
        "            num_train_batches_total=num_train_batches, num_val_batches_total=num_val_batches,\n",
        "            train_step_start=train_step_count\n",
        "        )\n",
        "        \n",
        "        all_train_losses_for_plot.append(avg_train_loss)\n",
        "        train_loss_moving_avg_q.append(avg_train_loss)\n",
        "        current_train_moving_avg = sum(train_loss_moving_avg_q) / len(train_loss_moving_avg_q) if train_loss_moving_avg_q else float('inf')\n",
        "    \n",
        "        avg_val_loss, val_step_count = validate_denoiser_epoch_v2(\n",
        "            denoiser_model=denoiser, \n",
        "            val_dl=val_dataloader, \n",
        "            device=DEVICE, \n",
        "            epoch_num_for_log=current_epoch_num_for_log,\n",
        "            num_train_batches_total=num_train_batches, \n",
        "            num_val_batches_total=num_val_batches,\n",
        "            val_step_start=val_step_count      \n",
        "        )\n",
        "        all_val_losses_for_plot.append(avg_val_loss)\n",
        "        val_loss_moving_avg_q.append(avg_val_loss) \n",
        "        current_val_moving_avg = sum(val_loss_moving_avg_q) / len(val_loss_moving_avg_q) if val_loss_moving_avg_q else float('inf')\n",
        "    \n",
        "        epoch_duration_seconds = time.time() - epoch_start_time\n",
        "        epoch_duration_formatted = str(datetime.timedelta(seconds=epoch_duration_seconds))\n",
        "    \n",
        "        print(f\"Epoch {current_epoch_num_for_log}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f} (MA: {current_train_moving_avg:.4f}), Val Loss: {avg_val_loss:.4f} (MA: {current_val_moving_avg:.4f}), Duration: {epoch_duration_formatted}\")\n",
        "    \n",
        "        ### WANDB: Log epoch-level metrics ###\n",
        "        \n",
        "        wandb_log_data = {\n",
        "            \"epoch\": current_epoch_num_for_log,\n",
        "            \"avg_train_loss\": avg_train_loss,\n",
        "            \"train_loss_ma\": current_train_moving_avg,\n",
        "            \"avg_val_loss\": avg_val_loss,\n",
        "            \"val_loss_ma\": current_val_moving_avg,\n",
        "            \"best_val_loss_ma_so_far\": best_val_loss_ma, # Log best val loss MA so far\n",
        "            \"epoch_duration_sec\": epoch_duration_seconds,\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "        }\n",
        "        \n",
        "        if lr_scheduler: lr_scheduler.step(avg_val_loss if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) else None)\n",
        "    \n",
        "        # Save model based on Validation Loss MA\n",
        "        if current_val_moving_avg < best_val_loss_ma:\n",
        "            improvement_val_over_absolute_best = (best_val_loss_ma - current_val_moving_avg) / abs(best_val_loss_ma + 1e-9) * 100\n",
        "            print(f\"  Val Loss MA improved to {current_val_moving_avg:.6f} from {best_val_loss_ma:.6f} ({improvement_val_over_absolute_best:.2f}% improvement).\")\n",
        "            best_val_loss_ma = current_val_moving_avg\n",
        "            new_best_val_model_path = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': denoiser.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "                'best_train_loss_ma': best_train_loss_ma, \n",
        "                'best_val_loss_ma': best_val_loss_ma\n",
        "            }, new_best_val_model_path)\n",
        "            print(f\"  Saved new best model (val loss MA) at epoch {current_epoch_num_for_log}\")\n",
        "            if previous_best_val_model_path and previous_best_val_model_path != new_best_val_model_path and os.path.exists(previous_best_val_model_path):\n",
        "                try:\n",
        "                    os.remove(previous_best_val_model_path)\n",
        "                    print(f\"  Deleted previous best val model: {previous_best_val_model_path}\")\n",
        "                except OSError as e:\n",
        "                    print(f\"  Warning: Could not delete previous best val model '{previous_best_val_model_path}': {e}\")\n",
        "            previous_best_val_model_path = new_best_val_model_path\n",
        "    \n",
        "        should_stop_early = False\n",
        "        # Early stopping logic (using EARLY_STOPPING_MIN_EPOCHS correctly)\n",
        "        if current_epoch_num_for_log > EARLY_STOPPING_MIN_EPOCHS: # Check after min epochs completed\n",
        "            if current_train_moving_avg < best_train_loss_ma : \n",
        "                # ... (rest of early stopping logic seems okay, ensure it uses current_epoch_num_for_log correctly)\n",
        "                improvement_over_absolute_best = (best_train_loss_ma - current_train_moving_avg) / abs(best_train_loss_ma + 1e-9) * 100\n",
        "                print(f\"  Train Loss MA improved to {current_train_moving_avg:.6f} from {best_train_loss_ma:.6f} ({improvement_over_absolute_best:.2f}% improvement).\")\n",
        "                best_train_loss_ma = current_train_moving_avg\n",
        "                epochs_without_improvement_train = 0\n",
        "                new_best_model_path = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_train_loss.pth\")\n",
        "                torch.save({\n",
        "                    'epoch': epoch, 'model_state_dict': denoiser.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(), 'loss': avg_train_loss, \n",
        "                    'val_loss': avg_val_loss, 'best_train_loss_ma': best_train_loss_ma,\n",
        "                    'best_val_loss_ma': best_val_loss_ma # Also save best_val_loss_ma when saving based on train loss\n",
        "                }, new_best_model_path)\n",
        "                print(f\"  Saved new best model (train loss MA) at epoch {current_epoch_num_for_log}\")\n",
        "                if previous_best_train_model_path and previous_best_train_model_path != new_best_model_path and os.path.exists(previous_best_train_model_path):\n",
        "                    try: os.remove(previous_best_train_model_path); print(f\"  Deleted previous best train model: {previous_best_train_model_path}\")\n",
        "                    except OSError as e: print(f\"  Warning: Could not delete previous best train model '{previous_best_train_model_path}': {e}\")\n",
        "                previous_best_train_model_path = new_best_model_path\n",
        "            else: \n",
        "                epochs_without_improvement_train += 1\n",
        "                print(f\"  No improvement in train loss MA for {epochs_without_improvement_train} epoch(s). Best MA: {best_train_loss_ma:.6f}, Current MA: {current_train_moving_avg:.6f}\")\n",
        "                if epochs_without_improvement_train >= EARLY_STOPPING_PATIENCE:\n",
        "                    # ... (percentage improvement check)\n",
        "                    idx_before_streak_started = len(all_train_losses_for_plot) - epochs_without_improvement_train -1 # Index of the epoch before non-improvement streak\n",
        "                    # Ensure indices are valid\n",
        "                    if idx_before_streak_started >= 0:\n",
        "                        # Calculate MA from historical_losses_for_ma of length TRAIN_MOVING_AVG_WINDOW ending at idx_before_streak_started\n",
        "                        historical_window_start = max(0, idx_before_streak_started - TRAIN_MOVING_AVG_WINDOW + 1)\n",
        "                        historical_losses_for_ma_calc = all_train_losses_for_plot[historical_window_start : idx_before_streak_started + 1]\n",
        "    \n",
        "                        if len(historical_losses_for_ma_calc) >= TRAIN_MOVING_AVG_WINDOW // 2 : # Need at least half window\n",
        "                            historical_train_ma = sum(historical_losses_for_ma_calc) / len(historical_losses_for_ma_calc)\n",
        "                            # Improvement is positive if current_train_moving_avg is smaller\n",
        "                            percentage_improvement_vs_historical = (historical_train_ma - current_train_moving_avg) / abs(historical_train_ma + 1e-9) * 100\n",
        "                            print(f\"  Patience met. Current Train MA: {current_train_moving_avg:.6f}, Historical MA before streak ({len(historical_losses_for_ma_calc)} epochs): {historical_train_ma:.6f}. Improvement: {percentage_improvement_vs_historical:.2f}%\")\n",
        "                            if percentage_improvement_vs_historical < EARLY_STOPPING_PERCENTAGE:\n",
        "                                should_stop_early = True\n",
        "                                print(f\"Early stopping triggered: Improvement {percentage_improvement_vs_historical:.2f}% < threshold {EARLY_STOPPING_PERCENTAGE}%.\")\n",
        "                        else:\n",
        "                            print(f\"  Patience met, but not enough historical data ({len(historical_losses_for_ma_calc)} points out of {TRAIN_MOVING_AVG_WINDOW}) to reliably calculate percentage improvement for early stopping.\")\n",
        "                    else:\n",
        "                         print(f\"  Patience met, but not enough historical data (idx_before_streak_started = {idx_before_streak_started}) to compare.\")\n",
        "    \n",
        "        \n",
        "        if (current_epoch_num_for_log % SAVE_MODEL_EVERY == 0) or (epoch == NUM_EPOCHS - 1):\n",
        "            is_best_this_epoch = current_train_moving_avg == best_train_loss_ma # Check if current MA is the best overall\n",
        "            # Avoid saving regular checkpoint if it's also the best_train_loss epoch to prevent duplicate saves\n",
        "            if not (is_best_this_epoch and os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_train_loss.pth\") == previous_best_train_model_path):\n",
        "                 torch.save({\n",
        "                    'epoch': epoch, 'model_state_dict': denoiser.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(), 'loss': avg_train_loss,\n",
        "                    'val_loss': avg_val_loss, 'best_train_loss_ma': best_train_loss_ma, # Save current best_train_loss_ma\n",
        "                    'best_val_loss_ma': best_val_loss_ma # Also save best_val_loss_ma for regular epoch saves\n",
        "                }, os.path.join(config.CHECKPOINT_DIR, f\"denoiser_model_epoch_{current_epoch_num_for_log:04d}.pth\"))\n",
        "                 print(f\"Saved model checkpoint at epoch {current_epoch_num_for_log}\")\n",
        "        \n",
        "        final_epoch_completed = epoch # Update last completed epoch here\n",
        "        if should_stop_early: break\n",
        "    \n",
        "        if (current_epoch_num_for_log % SAMPLE_EVERY == 0) or (epoch == NUM_EPOCHS - 1) or should_stop_early:\n",
        "            print(f\"Epoch {current_epoch_num_for_log}: Generating multiple visualization samples...\")\n",
        "            denoiser.eval()\n",
        "            vis_wandb_log_data = {} # Accumulate images here for a single wandb.log call\n",
        "    \n",
        "            # --- 1. Fixed Sample ---\n",
        "            fixed_sample_idx = wandb_config.get('FIXED_VIS_SAMPLE_IDX', 0)\n",
        "            if fixed_sample_idx < len(val_dataset):\n",
        "                print(f\"  Generating fixed sample (index {fixed_sample_idx} from val_dataset)...\")\n",
        "                fixed_sample_data = val_dataset[fixed_sample_idx]\n",
        "                prev_obs_fixed, prev_act_fixed, gt_fixed_batch, gt_prev_frames_fixed_seq = prepare_single_sample_for_sampler(fixed_sample_data, DEVICE) # gt_fixed_batch is [1,C,H,W]\n",
        "                with torch.no_grad():\n",
        "                    generated_output_tuple_fixed = diffusion_sampler.sample(prev_obs=prev_obs_fixed, prev_act=prev_act_fixed)\n",
        "                \n",
        "                if generated_output_tuple_fixed:\n",
        "                    generated_image_batch_fixed = generated_output_tuple_fixed[0] # This is [1, C, H, W]\n",
        "                    if generated_image_batch_fixed.ndim == 4 and generated_image_batch_fixed.shape[0] == 1:\n",
        "                        generated_image_to_save_fixed = generated_image_batch_fixed[0] # Extract single image: [C, H, W]\n",
        "                    else:\n",
        "                        generated_image_to_save_fixed = generated_image_batch_fixed # Fallback, though should be 4D\n",
        "        \n",
        "                    gt_image_to_save_fixed = gt_fixed_batch[0] # Extract single GT image: [C, H, W]\n",
        "        \n",
        "                    vis_path_fixed = save_visualization_samples(\n",
        "                        generated_image_to_save_fixed, # Should be [C,H,W]\n",
        "                        gt_image_to_save_fixed,        # Should be [C,H,W]\n",
        "                        gt_prev_frames_fixed_seq,\n",
        "                        current_epoch_num_for_log,\n",
        "                        config.SAMPLE_DIR,\n",
        "                        prefix=f\"val_vis_fixed_idx{fixed_sample_idx}\"\n",
        "                    )\n",
        "                    if vis_path_fixed and wandb.run:\n",
        "                        vis_wandb_log_data[f\"validation_samples/fixed_idx_{fixed_sample_idx}\"] = wandb.Image(vis_path_fixed, caption=f\"Epoch {current_epoch_num_for_log} Fixed Sample (Val Idx {fixed_sample_idx})\")\n",
        "                else:\n",
        "                    print(\"  Warning: Sampler did not return output for fixed sample.\")\n",
        "            else:\n",
        "                print(f\"  Warning: FIXED_SAMPLE_IDX {fixed_sample_idx} is out of bounds for val_dataset (size {len(val_dataset)}). Skipping fixed sample.\")\n",
        "        \n",
        "            # --- 2. Random Stopped Sample (Action 0.0) ---\n",
        "            if len(val_stopped_subset) > 0:\n",
        "                print(\"  Generating random stopped sample...\")\n",
        "                random_stopped_idx_in_subset = random.randint(0, len(val_stopped_subset) - 1)\n",
        "                stopped_sample_data = val_stopped_subset[random_stopped_idx_in_subset]\n",
        "                prev_obs_stopped, prev_act_stopped, gt_stopped_batch, gt_prev_frames_stopped_seq = prepare_single_sample_for_sampler(stopped_sample_data, DEVICE) # gt_stopped_batch is [1,C,H,W]\n",
        "                with torch.no_grad():\n",
        "                    generated_output_tuple_stopped = diffusion_sampler.sample(prev_obs=prev_obs_stopped, prev_act=prev_act_stopped)\n",
        "                \n",
        "                if generated_output_tuple_stopped:\n",
        "                    generated_image_batch_stopped = generated_output_tuple_stopped[0] # This is [1, C, H, W]\n",
        "                    if generated_image_batch_stopped.ndim == 4 and generated_image_batch_stopped.shape[0] == 1:\n",
        "                        generated_image_to_save_stopped = generated_image_batch_stopped[0] # Extract single image: [C, H, W]\n",
        "                    else:\n",
        "                        generated_image_to_save_stopped = generated_image_batch_stopped\n",
        "        \n",
        "                    gt_image_to_save_stopped = gt_stopped_batch[0] # Extract single GT image: [C, H, W]\n",
        "        \n",
        "                    vis_path_stopped = save_visualization_samples(\n",
        "                        generated_image_to_save_stopped, # Should be [C,H,W]\n",
        "                        gt_image_to_save_stopped,        # Should be [C,H,W]\n",
        "                        gt_prev_frames_stopped_seq,\n",
        "                        current_epoch_num_for_log,\n",
        "                        config.SAMPLE_DIR,\n",
        "                        prefix=\"val_vis_stopped_random\"\n",
        "                    )\n",
        "                    if vis_path_stopped and wandb.run:\n",
        "                        vis_wandb_log_data[\"validation_samples/random_stopped\"] = wandb.Image(vis_path_stopped, caption=f\"Epoch {current_epoch_num_for_log} Random Stopped Sample\")\n",
        "                else:\n",
        "                    print(\"  Warning: Sampler did not return output for stopped sample.\")\n",
        "            else:\n",
        "                print(\"  Warning: No stopped (action 0.0) samples found in validation set. Skipping random stopped sample.\")\n",
        "        \n",
        "            # --- 3. Random Moving Sample ---\n",
        "            moving_action_val_vis = wandb_config.get('MOVING_ACTION_VALUE_FOR_VIS', 0.1)\n",
        "            if len(val_moving_subset) > 0:\n",
        "                print(f\"  Generating random moving sample (action {moving_action_val_vis})...\")\n",
        "                random_moving_idx_in_subset = random.randint(0, len(val_moving_subset) - 1)\n",
        "                moving_sample_data = val_moving_subset[random_moving_idx_in_subset]\n",
        "                prev_obs_moving, prev_act_moving, gt_moving_batch, gt_prev_frames_moving_seq = prepare_single_sample_for_sampler(moving_sample_data, DEVICE) # gt_moving_batch is [1,C,H,W]\n",
        "                with torch.no_grad():\n",
        "                    generated_output_tuple_moving = diffusion_sampler.sample(prev_obs=prev_obs_moving, prev_act=prev_act_moving)\n",
        "        \n",
        "                if generated_output_tuple_moving:\n",
        "                    generated_image_batch_moving = generated_output_tuple_moving[0] # This is [1, C, H, W]\n",
        "                    if generated_image_batch_moving.ndim == 4 and generated_image_batch_moving.shape[0] == 1:\n",
        "                        generated_image_to_save_moving = generated_image_batch_moving[0] # Extract single image: [C, H, W]\n",
        "                    else:\n",
        "                        generated_image_to_save_moving = generated_image_batch_moving\n",
        "        \n",
        "                    gt_image_to_save_moving = gt_moving_batch[0] # Extract single GT image: [C, H, W]\n",
        "        \n",
        "                    vis_path_moving = save_visualization_samples(\n",
        "                        generated_image_to_save_moving, # Should be [C,H,W]\n",
        "                        gt_image_to_save_moving,        # Should be [C,H,W]\n",
        "                        gt_prev_frames_moving_seq,\n",
        "                        current_epoch_num_for_log,\n",
        "                        config.SAMPLE_DIR,\n",
        "                        prefix=f\"val_vis_moving_act{str(moving_action_val_vis).replace('.', 'p')}_random\"\n",
        "                    )\n",
        "                    if vis_path_moving and wandb.run:\n",
        "                        vis_wandb_log_data[f\"validation_samples/random_moving_act{str(moving_action_val_vis).replace('.', 'p')}\"] = wandb.Image(vis_path_moving, caption=f\"Epoch {current_epoch_num_for_log} Random Moving Sample (Action {moving_action_val_vis})\")\n",
        "                else:\n",
        "                    print(\"  Warning: Sampler did not return output for moving sample.\")\n",
        "            else:\n",
        "                print(f\"  Warning: No moving (action {moving_action_val_vis}) samples found in validation set. Skipping random moving sample.\")\n",
        "            \n",
        "            denoiser.train() # Set model back to training mode\n",
        "            # Log all accumulated data for this epoch (losses + images)\n",
        "            if wandb.run:\n",
        "                wandb.log({**wandb_log_data, **vis_wandb_log_data})\n",
        "        elif wandb.run: # If not sampling, still log epoch metrics\n",
        "             wandb.log(wandb_log_data)\n",
        "    \n",
        "    \n",
        "        if (current_epoch_num_for_log % PLOT_EVERY == 0) or (epoch == NUM_EPOCHS - 1) or should_stop_early :\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(all_train_losses_for_plot, label=\"Avg Train Loss\")\n",
        "            plt.plot(all_val_losses_for_plot, label=\"Avg Validation Loss\")\n",
        "            if len(all_train_losses_for_plot) >= TRAIN_MOVING_AVG_WINDOW:\n",
        "                train_ma_plot = [sum(all_train_losses_for_plot[i-TRAIN_MOVING_AVG_WINDOW+1:i+1])/TRAIN_MOVING_AVG_WINDOW for i in range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot))]\n",
        "                plt.plot(range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot)), train_ma_plot, label=f'Train Loss MA ({TRAIN_MOVING_AVG_WINDOW} epochs)', linestyle=':')\n",
        "            if len(all_val_losses_for_plot) >= VAL_MOVING_AVG_WINDOW:\n",
        "                val_ma_plot = [sum(all_val_losses_for_plot[i-VAL_MOVING_AVG_WINDOW+1:i+1])/VAL_MOVING_AVG_WINDOW for i in range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot))]\n",
        "                plt.plot(range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot)), val_ma_plot, label=f'Val Loss MA ({VAL_MOVING_AVG_WINDOW} epochs)', linestyle='--')\n",
        "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Progress (Epoch {current_epoch_num_for_log})\")\n",
        "            plt.legend(); plt.grid(True)\n",
        "            plt.savefig(os.path.join(config.PLOT_DIR, f\"loss_plot_epoch_{current_epoch_num_for_log:04d}.png\"))\n",
        "            ### WANDB: Log epoch loss plot ###\n",
        "            wandb.log({\"epoch_loss_plot\": wandb.Image(plt, caption=f\"Loss Plot Epoch {current_epoch_num_for_log}\")})\n",
        "            plt.close()\n",
        "            print(f\"Saved loss plot up to epoch {current_epoch_num_for_log}\")\n",
        "    \n",
        "    overall_training_end_time = time.time()\n",
        "    total_training_duration_seconds = overall_training_end_time - overall_training_start_time\n",
        "    total_training_duration_formatted = str(datetime.timedelta(seconds=total_training_duration_seconds))\n",
        "    \n",
        "    # final_epoch_completed is the last epoch index that ran (0-indexed)\n",
        "    print(f\"--- Training Complete (Stopped after epoch {final_epoch_completed + 1}) ---\") \n",
        "    print(f\"Total training duration: {total_training_duration_formatted}\") \n",
        "    \n",
        "    # Final Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(all_train_losses_for_plot, label=\"Avg Train Loss\")\n",
        "    plt.plot(all_val_losses_for_plot, label=\"Avg Validation Loss\")\n",
        "    if len(all_train_losses_for_plot) >= TRAIN_MOVING_AVG_WINDOW:\n",
        "        train_ma_plot = [sum(all_train_losses_for_plot[i-TRAIN_MOVING_AVG_WINDOW+1:i+1])/TRAIN_MOVING_AVG_WINDOW for i in range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot))]\n",
        "        plt.plot(range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot)), train_ma_plot, label=f'Train Loss MA ({TRAIN_MOVING_AVG_WINDOW} epochs)', linestyle=':')\n",
        "    if len(all_val_losses_for_plot) >= VAL_MOVING_AVG_WINDOW:\n",
        "        val_ma_plot = [sum(all_val_losses_for_plot[i-VAL_MOVING_AVG_WINDOW+1:i+1])/VAL_MOVING_AVG_WINDOW for i in range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot))]\n",
        "        plt.plot(range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot)), val_ma_plot, label=f'Val Loss MA ({VAL_MOVING_AVG_WINDOW} epochs)', linestyle='--')\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Denoiser Final Training & Validation Loss (Up to Epoch {final_epoch_completed + 1})\")\n",
        "    plt.legend(); plt.grid(True)\n",
        "    final_loss_plot_path = os.path.join(config.PLOT_DIR, \"denoiser_final_loss_plot.png\")\n",
        "    plt.savefig(final_loss_plot_path)\n",
        "    # plt.show() # Usually not needed in script, but can be uncommented for interactive\n",
        "    print(f\"Final loss plot saved to {final_loss_plot_path}\")\n",
        "    \n",
        "    ### WANDB: Log final loss plot and finish run ###\n",
        "    wandb.log({\"final_loss_plot\": wandb.Image(final_loss_plot_path, caption=f\"Final Loss Plot up to Epoch {final_epoch_completed + 1}\")})\n",
        "    wandb.finish()\n",
        "    print(\"Wandb run finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae2abe1",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    _main_training()"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# coding: utf-8",
      "executable": "/usr/bin/env python",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
