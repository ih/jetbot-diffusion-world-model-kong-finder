{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c62f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0699dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# get_ipython().system('pip install wandb')\n",
    "\n",
    "# \n",
    "# \n",
    "# get_ipython().system('pip install --upgrade typing_extensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6d62c9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import datetime # For epoch timing and timestamping\n",
    "from torchvision import transforms\n",
    "from collections import deque # For moving average\n",
    "from dataclasses import dataclass \n",
    "from typing import List, Optional, Dict, Any \n",
    "import random\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import copy # Make sure to import copy at the top of the file\n",
    "\n",
    "import wandb # Will be initialized in _main_training\n",
    "\n",
    "# Your project's specific imports\n",
    "import config # Your config.py\n",
    "import models # Your models.py (which should import from diamond_models.ipynb)\n",
    "\n",
    "# Import dataset from your jetbot_dataset.ipynb\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from jetbot_dataset import JetbotDataset, filter_dataset_by_action \n",
    "\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "print(\"Imports successful.\")\n",
    "\n",
    "# DEVICE will be set in _main_training or used directly from config by other functions\n",
    "# Global config constants that might be used by imported functions like train_diamond_model\n",
    "# if they don't re-fetch from config themselves (they mostly do, but being safe).\n",
    "DM_IMG_CHANNELS = getattr(config, 'DM_IMG_CHANNELS', 3)\n",
    "DM_NUM_ACTIONS = getattr(config, 'DM_NUM_ACTIONS', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f06e05",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_dataset():\n",
    "    full_dataset = JetbotDataset(\n",
    "        csv_path=config.CSV_PATH,\n",
    "        data_dir=config.DATA_DIR,\n",
    "        image_size=config.IMAGE_SIZE,\n",
    "        num_prev_frames=config.NUM_PREV_FRAMES,\n",
    "        transform=config.TRANSFORM\n",
    "    )\n",
    "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "    split_file_path = os.path.join(config.OUTPUT_DIR, getattr(config, 'SPLIT_DATASET_FILENAME', 'dataset_split.pth'))\n",
    "    if os.path.exists(split_file_path):\n",
    "        print(f\"Loading dataset split from {split_file_path}\")\n",
    "        split_data = torch.load(split_file_path)\n",
    "        train_indices, val_indices = split_data['train_indices'], split_data['val_indices']\n",
    "        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    else:\n",
    "        print(\"Creating new train/val split...\")\n",
    "        total_size = len(full_dataset)\n",
    "        train_size = int(total_size * 0.9)\n",
    "        val_size = total_size - train_size\n",
    "        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size]) # Using torch.random_split by default\n",
    "        torch.save({\n",
    "            'train_indices': train_dataset.indices,\n",
    "            'val_indices': val_dataset.indices,\n",
    "        }, split_file_path)\n",
    "        print(f\"Saved new dataset split to {split_file_path}\")\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee323a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization helpers defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tensor_to_pil(tensor_img):\n",
    "    tensor_img = (tensor_img.clamp(-1, 1) + 1) / 2\n",
    "    tensor_img = tensor_img.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    if tensor_img.shape[2] == 1:\n",
    "        tensor_img = tensor_img.squeeze(2)\n",
    "    # Ensure array is writeable for PIL\n",
    "    if not tensor_img.flags.writeable:\n",
    "        tensor_img = np.ascontiguousarray(tensor_img)\n",
    "    if tensor_img.dtype != np.uint8: # This check might be problematic if tensor_img is already uint8\n",
    "        pil_img_array = (tensor_img * 255).astype(np.uint8)\n",
    "    else:\n",
    "        pil_img_array = tensor_img # Already uint8\n",
    "    pil_img = PILImage.fromarray(pil_img_array)\n",
    "    return pil_img\n",
    "\n",
    "def save_visualization_samples(generated_tensor, gt_current_tensor, gt_prev_frames_sequence, epoch, save_dir, prefix=\"val_vis\"):\n",
    "    \"\"\"\n",
    "    Saves a visualization comparing a single generated image, its corresponding GT current image,\n",
    "    and the sequence of GT previous frames.\n",
    "    - generated_tensor, gt_current_tensor: [C, H, W]\n",
    "    - gt_prev_frames_sequence: [NumPrev, C, H, W]\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    generated_tensor = generated_tensor.detach().cpu()\n",
    "    gt_current_tensor = gt_current_tensor.detach().cpu()\n",
    "    gt_prev_frames_sequence = gt_prev_frames_sequence.detach().cpu()\n",
    "\n",
    "    num_prev_frames = config.NUM_PREV_FRAMES # Get from global config\n",
    "\n",
    "    num_cols = num_prev_frames + 1  # N previous frames + 1 current GT\n",
    "    # Create a 2 rows, num_cols columns subplot\n",
    "    fig, axs = plt.subplots(2, num_cols, figsize=(num_cols * 3, 6), squeeze=False) # squeeze=False ensures axs is always 2D\n",
    "\n",
    "    try:\n",
    "        # Top row: Previous GT frames and Current GT frame\n",
    "        for i in range(num_prev_frames):\n",
    "            axs[0, i].imshow(tensor_to_pil(gt_prev_frames_sequence[i]))\n",
    "            axs[0, i].set_title(f\"GT Prev {i+1}\")\n",
    "            axs[0, i].axis('off')\n",
    "            axs[1, i].axis('off') # Keep bottom row empty under previous GT frames\n",
    "\n",
    "        axs[0, num_prev_frames].imshow(tensor_to_pil(gt_current_tensor))\n",
    "        axs[0, num_prev_frames].set_title(\"GT Current\")\n",
    "        axs[0, num_prev_frames].axis('off')\n",
    "\n",
    "        # Bottom row, last column: Generated frame (aligned under Current GT)\n",
    "        axs[1, num_prev_frames].imshow(tensor_to_pil(generated_tensor))\n",
    "        axs[1, num_prev_frames].set_title(\"Generated\")\n",
    "        axs[1, num_prev_frames].axis('off')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing image for prefix {prefix}, epoch {epoch}: {e}\")\n",
    "        # Clear figure and display error text\n",
    "        for r in range(axs.shape[0]):\n",
    "            for c in range(axs.shape[1]):\n",
    "                axs[r,c].axis('off')\n",
    "        fig.clear() \n",
    "        plt.text(0.5, 0.5, \"Error displaying image\", ha=\"center\", va=\"center\", transform=fig.transFigure)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f\"{prefix}_epoch_{epoch:04d}.png\") \n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "    return save_path\n",
    "    \n",
    "def prepare_single_sample_for_sampler(sample_data, device):\n",
    "    target_img, action_single, prev_frames_flat_unbatched = sample_data # prev_frames_flat_unbatched is [NumPrev*C, H, W]\n",
    "    \n",
    "    # Add batch dimension (B=1) and move to device\n",
    "    gt_current_frame_batch = target_img.unsqueeze(0).to(device) # Shape: [1, C, H, W]\n",
    "    action_single_batch = action_single.unsqueeze(0).to(device) # Shape: [1, 1]\n",
    "    # prev_frames_flat_for_sampler_input needs to be [B, NumPrev*C, H, W] for the view later if used directly by sampler\n",
    "    # but for DIAMOND sampler, prev_obs is [B, NumPrevFrames, C, H, W]\n",
    "    \n",
    "    num_prev_frames_const = config.NUM_PREV_FRAMES\n",
    "    img_channels_const = DM_IMG_CHANNELS # Assumes DM_IMG_CHANNELS is globally available or from config\n",
    "    img_h_const = config.IMAGE_SIZE\n",
    "    img_w_const = config.IMAGE_SIZE\n",
    "\n",
    "    # Reshape prev_frames_flat_unbatched for sampler input [1, NumPrev, C, H, W]\n",
    "    prev_obs_for_sampler_input_5d = prev_frames_flat_unbatched.view(\n",
    "        num_prev_frames_const,\n",
    "        img_channels_const,\n",
    "        img_h_const,\n",
    "        img_w_const\n",
    "    ).unsqueeze(0).to(device) # Add batch dim and send to device\n",
    "\n",
    "    action_sequence_for_sampler = action_single_batch.repeat(1, config.NUM_PREV_FRAMES).long()\n",
    "    \n",
    "    # For visualization, we want the GT previous frames, unbatched and sequenced: [NumPrev, C, H, W]\n",
    "    gt_prev_frames_seq_for_vis = prev_frames_flat_unbatched.view(\n",
    "        num_prev_frames_const,\n",
    "        img_channels_const,\n",
    "        img_h_const,\n",
    "        img_w_const\n",
    "    ) # This is already on CPU if sample_data came directly from dataset before .to(device)\n",
    "      # It will be detached and moved to CPU again in save_visualization_samples\n",
    "    \n",
    "    return prev_obs_for_sampler_input_5d, action_sequence_for_sampler, gt_current_frame_batch, gt_prev_frames_seq_for_vis\n",
    "\n",
    "print(\"Visualization helpers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b097310c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation epoch functions adapted for Batch object and Denoiser.forward.\n"
     ]
    }
   ],
   "source": [
    "def train_denoiser_epoch(\n",
    "    denoiser_model,\n",
    "    train_dl,\n",
    "    opt,\n",
    "    scheduler,\n",
    "    grad_clip_val,\n",
    "    device,\n",
    "    epoch_num_for_log,\n",
    "    num_train_batches_total,\n",
    "    num_val_batches_total,\n",
    "    train_step_start=0,\n",
    "):\n",
    "    \"\"\"Run one training epoch for the denoiser model.\"\"\"\n",
    "    denoiser_model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(range(len(train_dl)), desc=f\"Epoch {epoch_num_for_log} [Train]\", leave=False)\n",
    "\n",
    "    num_prev_frames = config.NUM_PREV_FRAMES\n",
    "    c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
    "    accumulation_steps = config.ACCUMULATION_STEPS\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    perf_table = None\n",
    "    if wandb.run:\n",
    "        perf_table = wandb.Table(\n",
    "            columns=[\"step\", \"data_fetch_sec\", \"batch_prep_sec\", \"fw_bw_sec\", \"opt_sched_sec\"]\n",
    "        )\n",
    "\n",
    "    train_iter = iter(train_dl)\n",
    "\n",
    "    for batch_idx in progress_bar:\n",
    "        step_time_start = time.perf_counter()\n",
    "\n",
    "        # ----- Data fetch -----\n",
    "        fetch_start = time.perf_counter()\n",
    "        batch = next(train_iter)\n",
    "        data_fetch_duration = time.perf_counter() - fetch_start\n",
    "\n",
    "        # ----- Batch preparation -----\n",
    "        prep_start = time.perf_counter()\n",
    "        if isinstance(batch, models.Batch):\n",
    "            current_batch_obj = batch.to(device)\n",
    "        else:\n",
    "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
    "            current_batch_size = target_img_batch.shape[0]\n",
    "            target_img_batch = target_img_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
    "\n",
    "            prev_frames_seq_batch = prev_frames_flat_batch.view(\n",
    "                current_batch_size, num_prev_frames, c, h, w\n",
    "            )\n",
    "            batch_obs_tensor = torch.cat(\n",
    "                (prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1\n",
    "            )\n",
    "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()\n",
    "            batch_mask_padding = torch.ones(\n",
    "                current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool\n",
    "            )\n",
    "\n",
    "            current_batch_obj = models.Batch(\n",
    "                obs=batch_obs_tensor,\n",
    "                act=batch_act_tensor,\n",
    "                mask_padding=batch_mask_padding,\n",
    "                info=[{}] * current_batch_size,\n",
    "            )\n",
    "        batch_prep_duration = time.perf_counter() - prep_start\n",
    "\n",
    "        # ----- Forward + backward -----\n",
    "        fw_bw_start = time.perf_counter()\n",
    "        loss, logs = denoiser_model(current_batch_obj)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        fw_bw_duration = time.perf_counter() - fw_bw_start\n",
    "\n",
    "        # ----- Optimizer / scheduler -----\n",
    "        opt_start = time.perf_counter()\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            if grad_clip_val > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(denoiser_model.parameters(), grad_clip_val)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            opt.zero_grad()\n",
    "        opt_sched_duration = time.perf_counter() - opt_start\n",
    "\n",
    "        step_duration = time.perf_counter() - step_time_start\n",
    "\n",
    "        print(f\"Batch {batch_idx} of size {len(batch)} took {step_duration} seconds\")\n",
    "        wandb.log({\"step_duration\": step_duration, \"batch_idx\": batch_idx})\n",
    "\n",
    "        if perf_table is not None:\n",
    "            perf_table.add_data(\n",
    "                train_step_start + batch_idx,\n",
    "                data_fetch_duration,\n",
    "                batch_prep_duration,\n",
    "                fw_bw_duration,\n",
    "                opt_sched_duration,\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item() * accumulation_steps, \"LR\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            train_step = train_step_start + batch_idx\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_batch_loss\": loss.item() * accumulation_steps,\n",
    "                    \"train_batch_denoising_loss\": logs.get(\"loss_denoising\"),\n",
    "                    \"train_step\": train_step,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if perf_table is not None:\n",
    "        wandb.log({\"train_epoch_perf\": perf_table})\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl) if len(train_dl) > 0 else 0.0\n",
    "    final_step = train_step_start + len(train_dl)\n",
    "    return avg_loss, final_step\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_denoiser_epoch(denoiser_model, val_dl, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, val_step_start=0):\n",
    "    denoiser_model.eval()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(val_dl, desc=f\"Epoch {epoch_num_for_log} [Valid]\", leave=False)\n",
    "    num_prev_frames = config.NUM_PREV_FRAMES\n",
    "    c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        if isinstance(batch, models.Batch):\n",
    "            current_batch_obj = batch.to(device)\n",
    "        else:\n",
    "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
    "            current_batch_size = target_img_batch.shape[0]\n",
    "            target_img_batch = target_img_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
    "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
    "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
    "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()\n",
    "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
    "        \n",
    "            # Corrected Batch instantiation\n",
    "            current_batch_obj = models.Batch(obs=batch_obs_tensor, act=batch_act_tensor, mask_padding=batch_mask_padding, info=[{}] * current_batch_size)\n",
    "        \n",
    "        loss, logs = denoiser_model(current_batch_obj)\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Val Loss\": loss.item()})\n",
    "        \n",
    "        # Restored wandb logging\n",
    "        if batch_idx % 10 == 0:\n",
    "            val_step = val_step_start + batch_idx\n",
    "            wandb.log({\n",
    "                \"val_batch_loss\": loss.item(),\n",
    "                \"val_batch_denoising_loss\": logs.get(\"loss_denoising\"),\n",
    "                \"val_step\": val_step,\n",
    "            })\n",
    "             \n",
    "    avg_loss = total_loss / len(val_dl) if len(val_dl) > 0 else 0.0\n",
    "    final_step = val_step_start + len(val_dl)\n",
    "    return avg_loss, final_step\n",
    "\n",
    "\n",
    "print(\"Training and validation epoch functions adapted for Batch object and Denoiser.forward.\")\n",
    "\n",
    "\n",
    "def train_diamond_model(train_loader, val_loader, fresh_dataset_size, start_checkpoint=None):\n",
    "    \"\"\"\n",
    "    Train a denoiser model with robust, step-based early stopping.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = config.DEVICE\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # --- Model and Optimizer Setup (remains the same) ---\n",
    "    inner_cfg = models.InnerModelConfig(\n",
    "        img_channels=config.DM_IMG_CHANNELS,\n",
    "        num_steps_conditioning=config.NUM_PREV_FRAMES,\n",
    "        cond_channels=config.DM_COND_CHANNELS,\n",
    "        depths=config.DM_UNET_DEPTHS,\n",
    "        channels=config.DM_UNET_CHANNELS,\n",
    "        attn_depths=config.DM_UNET_ATTN_DEPTHS,\n",
    "        num_actions=config.DM_NUM_ACTIONS,\n",
    "        is_upsampler=config.DM_IS_UPSAMPLER,\n",
    "    )\n",
    "    denoiser_cfg = models.DenoiserConfig(\n",
    "        inner_model=inner_cfg,\n",
    "        sigma_data=config.DM_SIGMA_DATA,\n",
    "        sigma_offset_noise=config.DM_SIGMA_OFFSET_NOISE,\n",
    "        noise_previous_obs=config.DM_NOISE_PREVIOUS_OBS,\n",
    "        upsampling_factor=config.DM_UPSAMPLING_FACTOR,\n",
    "    )\n",
    "    denoiser = models.Denoiser(cfg=denoiser_cfg).to(device)\n",
    "    sigma_cfg = models.SigmaDistributionConfig(\n",
    "        loc=config.DM_SIGMA_P_MEAN,\n",
    "        scale=config.DM_SIGMA_P_STD,\n",
    "        sigma_min=config.DM_SIGMA_MIN_TRAIN,\n",
    "        sigma_max=config.DM_SIGMA_MAX_TRAIN,\n",
    "    )\n",
    "    denoiser.setup_training(sigma_cfg)\n",
    "\n",
    "    start_step_offset = -1\n",
    "    if start_checkpoint and os.path.exists(start_checkpoint):\n",
    "        state = torch.load(start_checkpoint, map_location=device)\n",
    "        if 'model_state_dict' in state:\n",
    "            denoiser.load_state_dict(state['model_state_dict'])\n",
    "        start_step_offset = state.get('step', -1)\n",
    "        print(f\"Loaded from checkpoint {start_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Starting from fresh model\")\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        denoiser.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=config.LEARNING_RATE_WEIGHT_DECAY,\n",
    "        eps=config.LEARNING_RATE_EPS,\n",
    "    )\n",
    "\n",
    "    def lr_lambda(step: int):\n",
    "        warmup = config.LEARNING_RATE_WARMUP_STEPS\n",
    "        return float(step) / float(max(1, warmup)) if step < warmup else 1.0\n",
    "    scheduler = LambdaLR(opt, lr_lambda)\n",
    "    \n",
    "    # --- Robust Early Stopping & Checkpointing Setup ---\n",
    "    best_val_loss = float('inf')\n",
    "    steps_since_last_improvement = 0\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    def round_up_to_multiple(x: int, base:int ) -> int:\n",
    "        \"\"\"Smallest multiple of `base` â‰¥ x.\"\"\"\n",
    "        return int(((x + base - 1) // base) * base)\n",
    "\n",
    "    alpha                 = config.MIX_ALPHA                             # e.g. 0.2\n",
    "    raw_steps_per_fresh_data_epoch = fresh_dataset_size / (config.BATCH_SIZE * alpha)\n",
    "    steps_per_fresh_data_epoch = round_up_to_multiple(raw_steps_per_fresh_data_epoch, config.ACCUMULATION_STEPS)\n",
    "\n",
    "    num_steps = round_up_to_multiple(steps_per_fresh_data_epoch * config.NUM_EPOCHS, config.ACCUMULATION_STEPS)\n",
    "    patience_steps = round_up_to_multiple(steps_per_fresh_data_epoch * config.EARLY_STOPPING_PATIENCE, config.ACCUMULATION_STEPS)\n",
    "    \n",
    "    print(f\"Incremental training for at least {patience_steps} and at most {num_steps} with {steps_per_fresh_data_epoch} number of steps per fresh data epoch\")\n",
    "\n",
    "    validate_every = steps_per_fresh_data_epoch    \n",
    "    # Divergence Guard Setup\n",
    "    divergence_patience = getattr(config, 'TRAIN_DIVERGE_PATIENCE_CHECKS', 3)\n",
    "    divergence_threshold = getattr(config, 'TRAIN_DIVERGE_THRESHOLD', 0.05)\n",
    "    last_train_loss = float('inf')\n",
    "    divergence_counter = 0\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    val_step_count = 0\n",
    "    train_iter = iter(train_loader)\n",
    "    \n",
    "    pbar = tqdm(range(num_steps), desc=\"Incremental Training Steps\")\n",
    "\n",
    "    # Sampler for visualization (similar to _main_training)\n",
    "    sampler_cfg_vis = models.DiffusionSamplerConfig(\n",
    "        num_steps_denoising=config.SAMPLER_NUM_STEPS,\n",
    "        sigma_min=config.SAMPLER_SIGMA_MIN,\n",
    "        sigma_max=config.SAMPLER_SIGMA_MAX,\n",
    "        rho=config.SAMPLER_RHO,\n",
    "        order=config.SAMPLER_ORDER,\n",
    "        s_churn=config.SAMPLER_S_CHURN,\n",
    "        s_tmin=config.SAMPLER_S_TMIN,\n",
    "        s_tmax=config.SAMPLER_S_TMAX,\n",
    "        s_noise=config.SAMPLER_S_NOISE\n",
    "    )\n",
    "    diffusion_sampler_vis = models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg_vis)\n",
    "\n",
    "    # Prepare filtered validation subsets for visualization (similar to _main_training)\n",
    "    val_stopped_subset_inc = []\n",
    "    val_moving_subset_inc = []\n",
    "    if hasattr(val_loader, 'dataset') and len(val_loader.dataset) > 0:\n",
    "        val_dataset_for_filter = val_loader.dataset\n",
    "        val_stopped_subset_inc = filter_dataset_by_action(val_dataset_for_filter, target_actions=0.0)\n",
    "        moving_action_val_vis_inc = getattr(config, 'MOVING_ACTION_VALUE_FOR_VIS', 0.13)\n",
    "        val_moving_subset_inc = filter_dataset_by_action(val_dataset_for_filter, target_actions=moving_action_val_vis_inc)\n",
    "    \n",
    "    perf_table = None\n",
    "    if wandb.run:\n",
    "        perf_table = wandb.Table(\n",
    "            columns=[\"step\", \"data_fetch_sec\", \"batch_prep_sec\", \"fw_bw_sec\", \"opt_sched_sec\"]\n",
    "        )\n",
    "\n",
    "    for step in pbar:\n",
    "        step_time_start = time.perf_counter()\n",
    "\n",
    "        # ----- Data fetch -----\n",
    "        fetch_start = time.perf_counter()\n",
    "        try:\n",
    "            batch = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            batch = next(train_iter)\n",
    "        data_fetch_duration = time.perf_counter() - fetch_start\n",
    "\n",
    "        # ----- Batch preparation -----\n",
    "        prep_start = time.perf_counter()\n",
    "\n",
    "        # --- Standard Training Step (remains the same) ---\n",
    "\n",
    "        # (Code to prepare batch object `current_batch_obj` remains the same)\n",
    "        if isinstance(batch, models.Batch):\n",
    "            current_batch_obj = batch.to(device)\n",
    "        else:\n",
    "            # Unpack the batch from the DataLoader\n",
    "            target_img_batch, action_batch, prev_frames_flat_batch = batch\n",
    "\n",
    "            # Move tensors to the correct device\n",
    "            target_img_batch = target_img_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            prev_frames_flat_batch = prev_frames_flat_batch.to(device)\n",
    "\n",
    "            # Reconstruct the logic from train_denoiser_epoch to create the Batch object\n",
    "            current_batch_size = target_img_batch.shape[0]\n",
    "            num_prev_frames = config.NUM_PREV_FRAMES\n",
    "            c, h, w = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
    "\n",
    "            prev_frames_seq_batch = prev_frames_flat_batch.view(current_batch_size, num_prev_frames, c, h, w)\n",
    "            batch_obs_tensor = torch.cat((prev_frames_seq_batch, target_img_batch.unsqueeze(1)), dim=1)\n",
    "            batch_act_tensor = action_batch.repeat(1, num_prev_frames).long()  # Ensure this matches the expected action format for the model\n",
    "            batch_mask_padding = torch.ones(current_batch_size, num_prev_frames + 1, device=device, dtype=torch.bool)\n",
    "\n",
    "            current_batch_obj = models.Batch(\n",
    "                obs=batch_obs_tensor,\n",
    "                act=batch_act_tensor,\n",
    "                mask_padding=batch_mask_padding,\n",
    "                info=[{}] * current_batch_size\n",
    "            )\n",
    "\n",
    "        batch_prep_duration = time.perf_counter() - prep_start\n",
    "\n",
    "        # ----- Forward + backward -----\n",
    "        fw_bw_start = time.perf_counter()\n",
    "        denoiser.train()\n",
    "        loss, logs = denoiser(current_batch_obj)\n",
    "        train_loss_val = loss.item()\n",
    "        loss = loss / config.ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        fw_bw_duration = time.perf_counter() - fw_bw_start\n",
    "\n",
    "        # ----- Optimizer / scheduler -----\n",
    "        opt_start = time.perf_counter()\n",
    "        if (step + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "            if config.GRAD_CLIP_VALUE > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(denoiser.parameters(), config.GRAD_CLIP_VALUE)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            opt.zero_grad()\n",
    "        opt_sched_duration = time.perf_counter() - opt_start\n",
    "\n",
    "        step_duration = time.perf_counter() - step_time_start\n",
    "\n",
    "        # Logging to wandb (more frequently for steps)\n",
    "        train_step_count = start_step_offset + step + 1 # Define train_step_count here\n",
    "        if wandb.run and (step + 1) % 10 == 0: # Log every 10 steps\n",
    "            wandb.log({\n",
    "                \"incremental_step_train_loss\": train_loss_val,\n",
    "                \"incremental_step_denoising_loss\": logs.get(\"loss_denoising\"),\n",
    "                \"incremental_step_learning_rate\": scheduler.get_last_lr()[0],\n",
    "                \"incremental_step_data_fetch_sec\": data_fetch_duration,\n",
    "                \"incremental_step_duration_sec\": step_duration,\n",
    "                \"train_step\": train_step_count,\n",
    "            })\n",
    "\n",
    "        if perf_table is not None:\n",
    "            perf_table.add_data(\n",
    "                step + 1,\n",
    "                data_fetch_duration,\n",
    "                batch_prep_duration,\n",
    "                fw_bw_duration,\n",
    "                opt_sched_duration,\n",
    "            )\n",
    "        \n",
    "        # --- Validation, Early Stopping, and Divergence Check ---\n",
    "        if (step + 1) % validate_every == 0 or (step + 1) == num_steps:\n",
    "            val_step_start = val_step_count\n",
    "            val_time_start = time.time()\n",
    "            current_val_loss, val_step_count = validate_denoiser_epoch(\n",
    "                denoiser, val_loader, device, step + 1, 0, 0, val_step_start=val_step_start\n",
    "            )\n",
    "            val_duration = time.time() - val_time_start\n",
    "            print(f'Validation at step {step+1} took {val_duration:.2f}s')\n",
    "\n",
    "            # Log validation loss\n",
    "            if wandb.run:\n",
    "                wandb.log({\n",
    "                    \"incremental_eval_val_loss\": current_val_loss,\n",
    "                    \"incremental_validation_duration_sec\": val_duration,\n",
    "                    \"val_step\": val_step_start\n",
    "                })\n",
    "            \n",
    "            # Image Sampling (similar to _main_training, simplified for step-based)\n",
    "            # Tied to validation frequency for now.\n",
    "            if wandb.run and hasattr(config, 'SAMPLE_EVERY') and (step + 1) % config.SAMPLE_EVERY == 0 :\n",
    "                sample_time_start = time.time()\n",
    "                denoiser.eval()\n",
    "                vis_wandb_log_data_inc = {}\n",
    "                fixed_sample_idx_inc = getattr(config, 'FIXED_VIS_SAMPLE_IDX', 0)\n",
    "\n",
    "                if hasattr(val_loader, 'dataset') and fixed_sample_idx_inc < len(val_loader.dataset):\n",
    "                    fixed_sample_data_inc = val_loader.dataset[fixed_sample_idx_inc]\n",
    "                    # Ensure sample_data is a tuple (img, act, prev_frames_flat)\n",
    "                    if not (isinstance(fixed_sample_data_inc, tuple) and len(fixed_sample_data_inc) == 3):\n",
    "                         # Try to get it from .dataset if val_loader.dataset is a Subset\n",
    "                        if isinstance(val_loader.dataset, torch.utils.data.Subset):\n",
    "                            original_dataset = val_loader.dataset.dataset\n",
    "                            original_idx = val_loader.dataset.indices[fixed_sample_idx_inc]\n",
    "                            fixed_sample_data_inc = original_dataset[original_idx]\n",
    "                        else:\n",
    "                            print(f\"Skipping fixed sample visualization: data format error or direct access failed.\")\n",
    "                            fixed_sample_data_inc = None \n",
    "                            \n",
    "                    if fixed_sample_data_inc:\n",
    "                        prev_obs_fixed_inc, prev_act_fixed_inc, gt_fixed_batch_inc, gt_prev_frames_fixed_seq_inc = prepare_single_sample_for_sampler(fixed_sample_data_inc, device)\n",
    "                        with torch.no_grad():\n",
    "                            generated_output_tuple_fixed_inc = diffusion_sampler_vis.sample(prev_obs=prev_obs_fixed_inc, prev_act=prev_act_fixed_inc)\n",
    "                        if generated_output_tuple_fixed_inc:\n",
    "                            generated_image_to_save_fixed_inc = generated_output_tuple_fixed_inc[0][0]\n",
    "                            gt_image_to_save_fixed_inc = gt_fixed_batch_inc[0]\n",
    "                            vis_path_fixed_inc = save_visualization_samples(\n",
    "                                generated_image_to_save_fixed_inc, gt_image_to_save_fixed_inc, gt_prev_frames_fixed_seq_inc,\n",
    "                                step + 1, config.SAMPLE_DIR, prefix=f\"inc_vis_fixed_step{step+1}\"\n",
    "                            )\n",
    "                            vis_wandb_log_data_inc[f\"incremental_samples/fixed_idx_{fixed_sample_idx_inc}\"] = wandb.Image(vis_path_fixed_inc, caption=f\"Step {step+1} Fixed Sample\")\n",
    "\n",
    "                # Simplified: Add one random sample from val_stopped_subset_inc if available\n",
    "                if len(val_stopped_subset_inc) > 0:\n",
    "                    stopped_sample_data_inc = val_stopped_subset_inc[random.randint(0, len(val_stopped_subset_inc) - 1)]\n",
    "                    prev_obs_stop, prev_act_stop, gt_batch_stop, gt_prev_seq_stop = prepare_single_sample_for_sampler(stopped_sample_data_inc, device)\n",
    "                    with torch.no_grad():\n",
    "                        gen_out_stop = diffusion_sampler_vis.sample(prev_obs=prev_obs_stop, prev_act=prev_act_stop)\n",
    "                    if gen_out_stop:\n",
    "                        vis_path_stop = save_visualization_samples(gen_out_stop[0][0], gt_batch_stop[0], gt_prev_seq_stop, step+1, config.SAMPLE_DIR, prefix=f\"inc_vis_stopped_step{step+1}\")\n",
    "                        vis_wandb_log_data_inc[\"incremental_samples/random_stopped\"] = wandb.Image(vis_path_stop, caption=f\"Step {step+1} Random Stopped\")\n",
    "\n",
    "                # Simplified: Add one random sample from val_moving_subset_inc_subset_inc if available\n",
    "                if len(val_moving_subset_inc) > 0:\n",
    "                    moving_sample_data_inc = val_moving_subset_inc[random.randint(0, len(val_moving_subset_inc) - 1)]\n",
    "                    prev_obs_move, prev_act_move, gt_batch_move, gt_prev_seq_move = prepare_single_sample_for_sampler(moving_sample_data_inc, device)\n",
    "                    with torch.no_grad():\n",
    "                        gen_out_move = diffusion_sampler_vis.sample(prev_obs=prev_obs_move, prev_act=prev_act_move)\n",
    "                    if gen_out_move:\n",
    "                        vis_path_move = save_visualization_samples(gen_out_move[0][0], gt_batch_move[0], gt_prev_seq_move, step+1, config.SAMPLE_DIR, prefix=f\"inc_vis_moving_step{step+1}\")\n",
    "                        vis_wandb_log_data_inc[\"incremental_samples/random_moving\"] = wandb.Image(vis_path_move, caption=f\"Step {step+1} Random Moving\")\n",
    "                \n",
    "                if vis_wandb_log_data_inc:\n",
    "                    vis_wandb_log_data_inc[\"train_step\"] = train_step_count # Use the same train_step_count\n",
    "                    sample_duration = time.time() - sample_time_start\n",
    "                    print(f'Sampling at step {step+1} took {sample_duration:.2f}s')\n",
    "                    vis_wandb_log_data_inc[\"incremental_sampling_duration_sec\"] = sample_duration\n",
    "                    wandb.log(vis_wandb_log_data_inc)\n",
    "                denoiser.train() # Set back to train mode\n",
    "\n",
    "            # Check for improvement\n",
    "            if current_val_loss < best_val_loss:\n",
    "                best_val_loss = current_val_loss\n",
    "                steps_since_last_improvement = 0\n",
    "                best_model_state_dict = copy.deepcopy(denoiser.state_dict())\n",
    "                pbar.set_description(f\"New best val_loss: {best_val_loss:.4f}\")\n",
    "            else:\n",
    "                steps_since_last_improvement += validate_every\n",
    "                pbar.set_description(f\"Best val_loss: {best_val_loss:.4f} Steps since last improvement: {steps_since_last_improvement}\")\n",
    "            \n",
    "            # Check for training loss divergence\n",
    "            if train_loss_val > last_train_loss * (1 + divergence_threshold):\n",
    "                divergence_counter += 1\n",
    "            else:\n",
    "                divergence_counter = 0 # Reset if loss is stable\n",
    "            last_train_loss = train_loss_val\n",
    "\n",
    "            # Check stopping conditions\n",
    "            if steps_since_last_improvement >= patience_steps:\n",
    "                print(f\"ðŸ›‘ Early stopping triggered: No improvement in {patience_steps} steps.\")\n",
    "                if wandb.run: wandb.log({\"early_stop_reason\": \"patience_met\", \"early_stop_step\": step + 1})\n",
    "                break\n",
    "            \n",
    "            if divergence_counter >= divergence_patience:\n",
    "                print(f\"ðŸ›‘ Early stopping triggered: Training loss diverged for {divergence_patience} checks.\")\n",
    "                if wandb.run: wandb.log({\"early_stop_reason\": \"loss_diverged\", \"early_stop_step\": step + 1})\n",
    "                break\n",
    "\n",
    "        pbar.set_postfix({\"Train Loss\": f\"{train_loss_val:.4f}\", \"Best Val\": f\"{best_val_loss:.4f}\", \"Steps w/o Improve\": f\"{steps_since_last_improvement}\", \"Fetch\": f\"{data_fetch_duration:.2f}s\", \"Step Time\": f\"{step_duration:.2f}s\"})\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    if perf_table is not None:\n",
    "        wandb.log({\"incremental_perf\": perf_table})\n",
    "\n",
    "    # --- Restore Best Model and Save ---\n",
    "    if best_model_state_dict:\n",
    "        print(f\"âœ… Restoring model to best validation loss: {best_val_loss:.4f}\")\n",
    "        denoiser.load_state_dict(best_model_state_dict)\n",
    "    \n",
    "    # Save the final, best model for promotion testing\n",
    "    final_best_path = os.path.join(config.CHECKPOINT_DIR, \"tmp_incremental_best.pth\")\n",
    "    torch.save({\"model_state_dict\": denoiser.state_dict(), 'step': step + 1, 'val_loss': best_val_loss}, final_best_path)\n",
    "\n",
    "    return final_best_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e7d030",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _main_training(finetune_checkpoint: str | None = None):\n",
    "    print(\"--- Main Training Execution --- \")\n",
    "\n",
    "    print(\"--- Configuration ---\")\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Denoiser & InnerModel specific\n",
    "    DM_SIGMA_DATA = getattr(config, 'DM_SIGMA_DATA', 0.5)\n",
    "    DM_SIGMA_OFFSET_NOISE = getattr(config, 'DM_SIGMA_OFFSET_NOISE', 0.1)\n",
    "    DM_NOISE_PREVIOUS_OBS = getattr(config, 'DM_NOISE_PREVIOUS_OBS', True)\n",
    "    # DM_IMG_CHANNELS is global for prepare_single_sample_for_sampler\n",
    "    DM_NUM_STEPS_CONDITIONING = getattr(config, 'DM_NUM_STEPS_CONDITIONING', config.NUM_PREV_FRAMES)\n",
    "    DM_COND_CHANNELS = getattr(config, 'DM_COND_CHANNELS', 256)\n",
    "    DM_UNET_DEPTHS = getattr(config, 'DM_UNET_DEPTHS', [2, 2, 2, 2])\n",
    "    DM_UNET_CHANNELS = getattr(config, 'DM_UNET_CHANNELS', [128, 256, 512, 1024])\n",
    "    DM_UNET_ATTN_DEPTHS = getattr(config, 'DM_UNET_ATTN_DEPTHS', [False, False, True, True])\n",
    "    # DM_NUM_ACTIONS is global for prepare_single_sample_for_sampler\n",
    "    DM_IS_UPSAMPLER = getattr(config, 'DM_IS_UPSAMPLER', False)\n",
    "    DM_UPSAMPLING_FACTOR = getattr(config, 'DM_UPSAMPLING_FACTOR', None)\n",
    "\n",
    "    # Sampler specific (for inference/visualization)\n",
    "    SAMPLER_NUM_STEPS = getattr(config, 'SAMPLER_NUM_STEPS', 50)\n",
    "    SAMPLER_SIGMA_MIN = getattr(config, 'SAMPLER_SIGMA_MIN', 0.002)\n",
    "    SAMPLER_SIGMA_MAX = getattr(config, 'SAMPLER_SIGMA_MAX', 80.0)\n",
    "    SAMPLER_RHO = getattr(config, 'SAMPLER_RHO', 7.0)\n",
    "    SAMPLER_ORDER = getattr(config, 'SAMPLER_ORDER', 1)\n",
    "    SAMPLER_S_CHURN = getattr(config, 'SAMPLER_S_CHURN', 0.0)\n",
    "    SAMPLER_S_TMIN = getattr(config, 'SAMPLER_S_TMIN', 0.0)\n",
    "    SAMPLER_S_TMAX = getattr(config, 'SAMPLER_S_TMAX', float(\"inf\"))\n",
    "    SAMPLER_S_NOISE = getattr(config, 'SAMPLER_S_NOISE', 1.0)\n",
    "\n",
    "    # Training specific\n",
    "    BATCH_SIZE = config.BATCH_SIZE\n",
    "    LEARNING_RATE = config.LEARNING_RATE\n",
    "    NUM_EPOCHS = config.NUM_EPOCHS\n",
    "    SAVE_MODEL_EVERY = config.SAVE_MODEL_EVERY\n",
    "    SAMPLE_EVERY = config.SAMPLE_EVERY\n",
    "    PLOT_EVERY = config.PLOT_EVERY\n",
    "    GRAD_CLIP_VALUE = getattr(config, 'GRAD_CLIP_VALUE', 1.0)\n",
    "    DM_SIGMA_P_MEAN = getattr(config, 'DM_SIGMA_P_MEAN', -1.2)\n",
    "    DM_SIGMA_P_STD = getattr(config, 'DM_SIGMA_P_STD', 1.2)\n",
    "    DM_SIGMA_MIN_TRAIN = getattr(config, 'DM_SIGMA_MIN_TRAIN', 0.002)\n",
    "    DM_SIGMA_MAX_TRAIN = getattr(config, 'DM_SIGMA_MAX_TRAIN', 80.0)\n",
    "    EARLY_STOPPING_PATIENCE = getattr(config, 'EARLY_STOPPING_PATIENCE', 10)\n",
    "    EARLY_STOPPING_MIN_EPOCHS = getattr(config, 'MIN_EPOCHS', 20)\n",
    "    EARLY_STOPPING_PERCENTAGE = getattr(config, 'EARLY_STOPPING_PERCENTAGE', 0.1)\n",
    "    TRAIN_MOVING_AVG_WINDOW = getattr(config, 'TRAIN_MOVING_AVG_WINDOW', 10)\n",
    "    VAL_MOVING_AVG_WINDOW = getattr(config, 'VAL_MOVING_AVG_WINDOW', 5)\n",
    "    print(\"Configuration loaded for _main_training.\")\n",
    "\n",
    "    wandb_config = {\n",
    "        'DM_SIGMA_DATA': DM_SIGMA_DATA,\n",
    "        'DM_SIGMA_OFFSET_NOISE': DM_SIGMA_OFFSET_NOISE,\n",
    "        'DM_NOISE_PREVIOUS_OBS': DM_NOISE_PREVIOUS_OBS,\n",
    "        'DM_IMG_CHANNELS': DM_IMG_CHANNELS,\n",
    "        'DM_NUM_STEPS_CONDITIONING': DM_NUM_STEPS_CONDITIONING,\n",
    "        'DM_COND_CHANNELS': DM_COND_CHANNELS,\n",
    "        'DM_UNET_DEPTHS': DM_UNET_DEPTHS,\n",
    "        'DM_UNET_CHANNELS': DM_UNET_CHANNELS,\n",
    "        'DM_UNET_ATTN_DEPTHS': DM_UNET_ATTN_DEPTHS,\n",
    "        'DM_NUM_ACTIONS': DM_NUM_ACTIONS,\n",
    "        'DM_IS_UPSAMPLER': DM_IS_UPSAMPLER,\n",
    "        'DM_UPSAMPLING_FACTOR': DM_UPSAMPLING_FACTOR,\n",
    "        'SAMPLER_NUM_STEPS': SAMPLER_NUM_STEPS,\n",
    "        'SAMPLER_SIGMA_MIN': SAMPLER_SIGMA_MIN,\n",
    "        'SAMPLER_SIGMA_MAX': SAMPLER_SIGMA_MAX,\n",
    "        'SAMPLER_RHO': SAMPLER_RHO,\n",
    "        'SAMPLER_ORDER': SAMPLER_ORDER,\n",
    "        'SAMPLER_S_CHURN': SAMPLER_S_CHURN,\n",
    "        'SAMPLER_S_TMIN': SAMPLER_S_TMIN,\n",
    "        'SAMPLER_S_TMAX': SAMPLER_S_TMAX,\n",
    "        'SAMPLER_S_NOISE': SAMPLER_S_NOISE,\n",
    "        'BATCH_SIZE': BATCH_SIZE,\n",
    "        'LEARNING_RATE': LEARNING_RATE,\n",
    "        'NUM_EPOCHS': NUM_EPOCHS,\n",
    "        'GRAD_CLIP_VALUE': GRAD_CLIP_VALUE,\n",
    "        'DM_SIGMA_P_MEAN': DM_SIGMA_P_MEAN,\n",
    "        'DM_SIGMA_P_STD': DM_SIGMA_P_STD,\n",
    "        'DM_SIGMA_MIN_TRAIN': DM_SIGMA_MIN_TRAIN,\n",
    "        'DM_SIGMA_MAX_TRAIN': DM_SIGMA_MAX_TRAIN,\n",
    "        'EARLY_STOPPING_PATIENCE': EARLY_STOPPING_PATIENCE,\n",
    "        'EARLY_STOPPING_MIN_EPOCHS': EARLY_STOPPING_MIN_EPOCHS,\n",
    "        'EARLY_STOPPING_PERCENTAGE': EARLY_STOPPING_PERCENTAGE,\n",
    "        'TRAIN_MOVING_AVG_WINDOW': TRAIN_MOVING_AVG_WINDOW,\n",
    "        'VAL_MOVING_AVG_WINDOW': VAL_MOVING_AVG_WINDOW,\n",
    "        'IMAGE_SIZE': config.IMAGE_SIZE,\n",
    "        'NUM_PREV_FRAMES': config.NUM_PREV_FRAMES,\n",
    "        'PROJECT_NAME': getattr(config, 'PROJECT_NAME', 'jetbot-diamond-world-model'),\n",
    "        'FIXED_VIS_SAMPLE_IDX': getattr(config, 'FIXED_VIS_SAMPLE_IDX', 0),\n",
    "        'MOVING_ACTION_VALUE_FOR_VIS': getattr(config, 'MOVING_ACTION_VALUE_FOR_VIS', 0.13)\n",
    "    }\n",
    "    wandb.init(project=wandb_config['PROJECT_NAME'], config=wandb_config)\n",
    "    print(\"Wandb initialized for _main_training.\")\n",
    "\n",
    "    print(\"--- Initializing Models for _main_training ---\")\n",
    "    try:\n",
    "        inner_model_config = models.InnerModelConfig(\n",
    "            img_channels=DM_IMG_CHANNELS,\n",
    "            num_steps_conditioning=DM_NUM_STEPS_CONDITIONING,\n",
    "            cond_channels=DM_COND_CHANNELS,\n",
    "            depths=DM_UNET_DEPTHS,\n",
    "            channels=DM_UNET_CHANNELS,\n",
    "            attn_depths=DM_UNET_ATTN_DEPTHS,\n",
    "            num_actions=DM_NUM_ACTIONS,\n",
    "            is_upsampler=DM_IS_UPSAMPLER\n",
    "        )\n",
    "        # inner_model_instance = models.InnerModel(inner_model_config).to(DEVICE) # Not strictly needed if only denoiser is used\n",
    "        # print(f\"InnerModelImpl parameter count: {sum(p.numel() for p in inner_model_instance.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "        denoiser_cfg = models.DenoiserConfig(\n",
    "            inner_model=inner_model_config, \n",
    "            sigma_data=DM_SIGMA_DATA,\n",
    "            sigma_offset_noise=DM_SIGMA_OFFSET_NOISE,\n",
    "            noise_previous_obs=DM_NOISE_PREVIOUS_OBS,\n",
    "            upsampling_factor=DM_UPSAMPLING_FACTOR\n",
    "        )\n",
    "        denoiser = models.Denoiser(cfg=denoiser_cfg).to(DEVICE)\n",
    "        sigma_dist_train_cfg = models.SigmaDistributionConfig(\n",
    "            loc=DM_SIGMA_P_MEAN, scale=DM_SIGMA_P_STD,\n",
    "            sigma_min=DM_SIGMA_MIN_TRAIN, sigma_max=DM_SIGMA_MAX_TRAIN\n",
    "        )\n",
    "        denoiser.setup_training(sigma_dist_train_cfg)\n",
    "        print(f\"Denoiser model created for _main_training. Total parameter count: {sum(p.numel() for p in denoiser.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "        sampler_cfg = models.DiffusionSamplerConfig(\n",
    "            num_steps_denoising=SAMPLER_NUM_STEPS, sigma_min=SAMPLER_SIGMA_MIN,\n",
    "            sigma_max=SAMPLER_SIGMA_MAX, rho=SAMPLER_RHO, order=SAMPLER_ORDER,\n",
    "            s_churn=SAMPLER_S_CHURN, s_tmin=SAMPLER_S_TMIN,\n",
    "            s_tmax=SAMPLER_S_TMAX, s_noise=SAMPLER_S_NOISE\n",
    "        )\n",
    "        diffusion_sampler = models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg)\n",
    "        print(\"DiffusionSampler created for visualization in _main_training.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing models in _main_training: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"--- Setting up Optimizer and Scheduler for _main_training ---\")\n",
    "    lr_for_optimizer = LEARNING_RATE\n",
    "    if finetune_checkpoint:\n",
    "        print(f\"Finetuning from checkpoint: {finetune_checkpoint}\")\n",
    "        if os.path.exists(finetune_checkpoint):\n",
    "            ckpt = torch.load(finetune_checkpoint, map_location=DEVICE)\n",
    "            if \"model_state_dict\" in ckpt:\n",
    "                denoiser.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "                lr_for_optimizer = LEARNING_RATE / 5\n",
    "                print(f\"Loaded weights for fine-tuning. LR set to {lr_for_optimizer}\")\n",
    "            else:\n",
    "                print(f\"Warning: no model_state_dict in {finetune_checkpoint}. Starting fresh\")\n",
    "        else:\n",
    "            print(f\"Warning: finetune checkpoint {finetune_checkpoint} not found. Starting fresh\")\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        denoiser.parameters(), lr=lr_for_optimizer,\n",
    "        weight_decay=config.LEARNING_RATE_WEIGHT_DECAY, eps=config.LEARNING_RATE_EPS\n",
    "    )\n",
    "    print(f\"Optimizer: AdamW with LR={lr_for_optimizer}\")\n",
    "    def lr_lambda_main(current_step: int):\n",
    "        if current_step < config.LEARNING_RATE_WARMUP_STEPS:\n",
    "            return float(current_step) / float(max(1, config.LEARNING_RATE_WARMUP_STEPS))\n",
    "        return 1.0\n",
    "    lr_scheduler = LambdaLR(optimizer, lr_lambda_main)\n",
    "    print(f\"LR Scheduler: LambdaLR with {config.LEARNING_RATE_WARMUP_STEPS} warmup steps.\")\n",
    "    wandb.watch(denoiser, log=\"all\", log_freq=100)\n",
    "    print(\"Wandb watching denoiser model.\")\n",
    "\n",
    "    START_EPOCH = 0\n",
    "    BEST_VAL_LOSS_MA_FROM_CKPT = float('inf')\n",
    "    PREVIOUS_BEST_VAL_MODEL_PATH = None\n",
    "\n",
    "    load_path_config_main = None if finetune_checkpoint else config.LOAD_CHECKPOINT\n",
    "    best_val_loss_model_default_path_main = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\")\n",
    "    load_path_main = None\n",
    "    if not finetune_checkpoint:\n",
    "        load_path_main = load_path_config_main\n",
    "        if load_path_main:\n",
    "            print(f\"Attempting to load checkpoint from config.LOAD_CHECKPOINT: {load_path_main}\")\n",
    "        elif os.path.exists(best_val_loss_model_default_path_main):\n",
    "            load_path_main = best_val_loss_model_default_path_main\n",
    "            print(f\"Using existing best_val_loss model: {load_path_main}\")\n",
    "\n",
    "        if load_path_main and os.path.exists(load_path_main):\n",
    "            print(f\"Loading checkpoint for _main_training from: {load_path_main}\")\n",
    "            try:\n",
    "                checkpoint = torch.load(load_path_main, map_location=DEVICE)\n",
    "                denoiser.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "                START_EPOCH = checkpoint.get(\"epoch\", 0) + 1\n",
    "                BEST_VAL_LOSS_MA_FROM_CKPT = checkpoint.get(\"best_val_loss_ma\", float(\"inf\"))\n",
    "                if load_path_main.endswith(\"denoiser_model_best_val_loss.pth\"):\n",
    "                    PREVIOUS_BEST_VAL_MODEL_PATH = load_path_main\n",
    "                print(f\"Resuming _main_training from epoch {START_EPOCH}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint in _main_training: {e}. Starting fresh.\")\n",
    "                START_EPOCH = 0\n",
    "        else:\n",
    "            print(\"No checkpoint found or specified for _main_training. Starting fresh.\")\n",
    "    train_dataset, val_dataset = split_dataset()\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=False, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=False, drop_last=False)\n",
    "    print(f\"Training dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")\n",
    "    \n",
    "    val_stopped_subset, val_moving_subset = [], []\n",
    "    if len(val_dataset) > 0:\n",
    "        print(\"Preparing filtered validation subsets for visualization...\")\n",
    "        val_stopped_subset = filter_dataset_by_action(val_dataset, target_actions=0.0)\n",
    "        moving_action_val = wandb_config['MOVING_ACTION_VALUE_FOR_VIS']\n",
    "        val_moving_subset = filter_dataset_by_action(val_dataset, target_actions=moving_action_val)\n",
    "        print(f\"Found {len(val_stopped_subset)} stopped and {len(val_moving_subset)} moving samples.\")\n",
    "    else:\n",
    "        from torch.utils.data import Subset # Ensure Subset is available if val_dataset is empty\n",
    "        val_stopped_subset = Subset(val_dataset, [])\n",
    "        val_moving_subset = Subset(val_dataset, [])\n",
    "    \n",
    "    print(\"--- Starting Training Process in _main_training ---\")\n",
    "    overall_training_start_time = time.time()\n",
    "    all_train_losses_for_plot, all_val_losses_for_plot = [], []\n",
    "    train_loss_moving_avg_q = deque(maxlen=TRAIN_MOVING_AVG_WINDOW)\n",
    "    val_loss_moving_avg_q = deque(maxlen=VAL_MOVING_AVG_WINDOW)\n",
    "    best_val_loss_ma = BEST_VAL_LOSS_MA_FROM_CKPT\n",
    "    epochs_without_improvement_val = 0\n",
    "    previous_best_val_model_path = PREVIOUS_BEST_VAL_MODEL_PATH\n",
    "    final_epoch_completed = START_EPOCH - 1\n",
    "    num_train_batches = len(train_dataloader)\n",
    "    num_val_batches = len(val_dataloader)\n",
    "    \n",
    "    train_step_count = 0\n",
    "    val_step_count = 0\n",
    "    for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        current_epoch_num_for_log = epoch + 1\n",
    "        avg_train_loss, train_step_count = train_denoiser_epoch(\n",
    "            denoiser_model=denoiser, train_dl=train_dataloader, opt=optimizer,\n",
    "            scheduler=lr_scheduler, grad_clip_val=GRAD_CLIP_VALUE, device=DEVICE,\n",
    "            epoch_num_for_log=current_epoch_num_for_log,\n",
    "            num_train_batches_total=num_train_batches, num_val_batches_total=num_val_batches,\n",
    "            train_step_start=train_step_count\n",
    "        )\n",
    "        \n",
    "        all_train_losses_for_plot.append(avg_train_loss)\n",
    "        train_loss_moving_avg_q.append(avg_train_loss)\n",
    "        current_train_moving_avg = sum(train_loss_moving_avg_q) / len(train_loss_moving_avg_q) if train_loss_moving_avg_q else float('inf')\n",
    "    \n",
    "        val_time_start = time.time()\n",
    "        avg_val_loss, val_step_count = validate_denoiser_epoch(\n",
    "            denoiser_model=denoiser, \n",
    "            val_dl=val_dataloader, \n",
    "            device=DEVICE, \n",
    "            epoch_num_for_log=current_epoch_num_for_log,\n",
    "            num_train_batches_total=num_train_batches, \n",
    "            num_val_batches_total=num_val_batches,\n",
    "            val_step_start=val_step_count      \n",
    "        )\n",
    "        val_duration = time.time() - val_time_start\n",
    "        print(f'Validation for epoch {current_epoch_num_for_log} took {val_duration:.2f}s')\n",
    "        all_val_losses_for_plot.append(avg_val_loss)\n",
    "        val_loss_moving_avg_q.append(avg_val_loss) \n",
    "        current_val_moving_avg = sum(val_loss_moving_avg_q) / len(val_loss_moving_avg_q) if val_loss_moving_avg_q else float('inf')\n",
    "    \n",
    "        epoch_duration_seconds = time.time() - epoch_start_time\n",
    "        epoch_duration_formatted = str(datetime.timedelta(seconds=epoch_duration_seconds))\n",
    "    \n",
    "        print(f\"Epoch {current_epoch_num_for_log}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f} (MA: {current_train_moving_avg:.4f}), Val Loss: {avg_val_loss:.4f} (MA: {current_val_moving_avg:.4f}), Duration: {epoch_duration_formatted}\")\n",
    "    \n",
    "        ### WANDB: Log epoch-level metrics ###\n",
    "        \n",
    "        wandb_log_data = {\n",
    "            \"epoch\": current_epoch_num_for_log,\n",
    "            \"avg_train_loss\": avg_train_loss,\n",
    "            \"train_loss_ma\": current_train_moving_avg,\n",
    "            \"avg_val_loss\": avg_val_loss,\n",
    "            \"val_loss_ma\": current_val_moving_avg,\n",
    "            \"best_val_loss_ma_so_far\": best_val_loss_ma, # Log best val loss MA so far\n",
    "            \"epoch_duration_sec\": epoch_duration_seconds,\n",
    "            \"validation_duration_sec\": val_duration,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        \n",
    "        if lr_scheduler: lr_scheduler.step(avg_val_loss if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) else None)\n",
    "    \n",
    "        # Save model based on Validation Loss MA\n",
    "        if current_val_moving_avg < best_val_loss_ma:\n",
    "            improvement_val_over_absolute_best = (best_val_loss_ma - current_val_moving_avg) / abs(best_val_loss_ma + 1e-9) * 100\n",
    "            print(f\"  Val Loss MA improved to {current_val_moving_avg:.6f} from {best_val_loss_ma:.6f} ({improvement_val_over_absolute_best:.2f}% improvement).\")\n",
    "            best_val_loss_ma = current_val_moving_avg\n",
    "            new_best_val_model_path = os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': denoiser.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'best_val_loss_ma': best_val_loss_ma\n",
    "            }, new_best_val_model_path)\n",
    "            print(f\"  Saved new best model (val loss MA) at epoch {current_epoch_num_for_log}\")\n",
    "            if previous_best_val_model_path and previous_best_val_model_path != new_best_val_model_path and os.path.exists(previous_best_val_model_path):\n",
    "                try:\n",
    "                    os.remove(previous_best_val_model_path)\n",
    "                    print(f\"  Deleted previous best val model: {previous_best_val_model_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"  Warning: Could not delete previous best val model '{previous_best_val_model_path}': {e}\")\n",
    "            previous_best_val_model_path = new_best_val_model_path\n",
    "    \n",
    "        should_stop_early = False\n",
    "        # Early stopping based on validation loss moving average\n",
    "        if current_epoch_num_for_log > EARLY_STOPPING_MIN_EPOCHS:\n",
    "            if current_val_moving_avg < best_val_loss_ma:\n",
    "                epochs_without_improvement_val = 0\n",
    "            else:\n",
    "                epochs_without_improvement_val += 1\n",
    "                print(f\"  No improvement in val loss MA for {epochs_without_improvement_val} epoch(s). Best MA: {best_val_loss_ma:.6f}, Current MA: {current_val_moving_avg:.6f}\")\n",
    "                if epochs_without_improvement_val >= EARLY_STOPPING_PATIENCE:\n",
    "                    should_stop_early = True\n",
    "                    print(\"Early stopping triggered due to validation loss stagnation.\")\n",
    "        if (current_epoch_num_for_log % SAVE_MODEL_EVERY == 0) or (epoch == NUM_EPOCHS - 1):\n",
    "            is_best_this_epoch = current_val_moving_avg == best_val_loss_ma\n",
    "            # Avoid saving regular checkpoint if it's also the best_val_loss epoch to prevent duplicate saves\n",
    "            if not (is_best_this_epoch and os.path.join(config.CHECKPOINT_DIR, \"denoiser_model_best_val_loss.pth\") == previous_best_val_model_path):\n",
    "                 torch.save({\n",
    "                    'epoch': epoch, 'model_state_dict': denoiser.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(), 'loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'best_val_loss_ma': best_val_loss_ma\n",
    "                }, os.path.join(config.CHECKPOINT_DIR, f\"denoiser_model_epoch_{current_epoch_num_for_log:04d}.pth\"))\n",
    "                 print(f\"Saved model checkpoint at epoch {current_epoch_num_for_log}\")\n",
    "        \n",
    "        final_epoch_completed = epoch # Update last completed epoch here\n",
    "        if should_stop_early: break\n",
    "    \n",
    "        if (current_epoch_num_for_log % SAMPLE_EVERY == 0) or (epoch == NUM_EPOCHS - 1) or should_stop_early:\n",
    "            print(f\"Epoch {current_epoch_num_for_log}: Generating multiple visualization samples...\")\n",
    "            sample_time_start = time.time()\n",
    "            denoiser.eval()\n",
    "            vis_wandb_log_data = {} # Accumulate images here for a single wandb.log call\n",
    "    \n",
    "            # --- 1. Fixed Sample ---\n",
    "            fixed_sample_idx = wandb_config.get('FIXED_VIS_SAMPLE_IDX', 0)\n",
    "            if fixed_sample_idx < len(val_dataset):\n",
    "                print(f\"  Generating fixed sample (index {fixed_sample_idx} from val_dataset)...\")\n",
    "                fixed_sample_data = val_dataset[fixed_sample_idx]\n",
    "                prev_obs_fixed, prev_act_fixed, gt_fixed_batch, gt_prev_frames_fixed_seq = prepare_single_sample_for_sampler(fixed_sample_data, DEVICE) # gt_fixed_batch is [1,C,H,W]\n",
    "                with torch.no_grad():\n",
    "                    generated_output_tuple_fixed = diffusion_sampler.sample(prev_obs=prev_obs_fixed, prev_act=prev_act_fixed)\n",
    "                \n",
    "                if generated_output_tuple_fixed:\n",
    "                    generated_image_batch_fixed = generated_output_tuple_fixed[0] # This is [1, C, H, W]\n",
    "                    if generated_image_batch_fixed.ndim == 4 and generated_image_batch_fixed.shape[0] == 1:\n",
    "                        generated_image_to_save_fixed = generated_image_batch_fixed[0] # Extract single image: [C, H, W]\n",
    "                    else:\n",
    "                        generated_image_to_save_fixed = generated_image_batch_fixed # Fallback, though should be 4D\n",
    "        \n",
    "                    gt_image_to_save_fixed = gt_fixed_batch[0] # Extract single GT image: [C, H, W]\n",
    "        \n",
    "                    vis_path_fixed = save_visualization_samples(\n",
    "                        generated_image_to_save_fixed, # Should be [C,H,W]\n",
    "                        gt_image_to_save_fixed,        # Should be [C,H,W]\n",
    "                        gt_prev_frames_fixed_seq,\n",
    "                        current_epoch_num_for_log,\n",
    "                        config.SAMPLE_DIR,\n",
    "                        prefix=f\"val_vis_fixed_idx{fixed_sample_idx}\"\n",
    "                    )\n",
    "                    if vis_path_fixed and wandb.run:\n",
    "                        vis_wandb_log_data[f\"validation_samples/fixed_idx_{fixed_sample_idx}\"] = wandb.Image(vis_path_fixed, caption=f\"Epoch {current_epoch_num_for_log} Fixed Sample (Val Idx {fixed_sample_idx})\")\n",
    "                else:\n",
    "                    print(\"  Warning: Sampler did not return output for fixed sample.\")\n",
    "            else:\n",
    "                print(f\"  Warning: FIXED_SAMPLE_IDX {fixed_sample_idx} is out of bounds for val_dataset (size {len(val_dataset)}). Skipping fixed sample.\")\n",
    "        \n",
    "            # --- 2. Random Stopped Sample (Action 0.0) ---\n",
    "            if len(val_stopped_subset) > 0:\n",
    "                print(\"  Generating random stopped sample...\")\n",
    "                random_stopped_idx_in_subset = random.randint(0, len(val_stopped_subset) - 1)\n",
    "                stopped_sample_data = val_stopped_subset[random_stopped_idx_in_subset]\n",
    "                prev_obs_stopped, prev_act_stopped, gt_stopped_batch, gt_prev_frames_stopped_seq = prepare_single_sample_for_sampler(stopped_sample_data, DEVICE) # gt_stopped_batch is [1,C,H,W]\n",
    "                with torch.no_grad():\n",
    "                    generated_output_tuple_stopped = diffusion_sampler.sample(prev_obs=prev_obs_stopped, prev_act=prev_act_stopped)\n",
    "                \n",
    "                if generated_output_tuple_stopped:\n",
    "                    generated_image_batch_stopped = generated_output_tuple_stopped[0] # This is [1, C, H, W]\n",
    "                    if generated_image_batch_stopped.ndim == 4 and generated_image_batch_stopped.shape[0] == 1:\n",
    "                        generated_image_to_save_stopped = generated_image_batch_stopped[0] # Extract single image: [C, H, W]\n",
    "                    else:\n",
    "                        generated_image_to_save_stopped = generated_image_batch_stopped\n",
    "        \n",
    "                    gt_image_to_save_stopped = gt_stopped_batch[0] # Extract single GT image: [C, H, W]\n",
    "        \n",
    "                    vis_path_stopped = save_visualization_samples(\n",
    "                        generated_image_to_save_stopped, # Should be [C,H,W]\n",
    "                        gt_image_to_save_stopped,        # Should be [C,H,W]\n",
    "                        gt_prev_frames_stopped_seq,\n",
    "                        current_epoch_num_for_log,\n",
    "                        config.SAMPLE_DIR,\n",
    "                        prefix=\"val_vis_stopped_random\"\n",
    "                    )\n",
    "                    if vis_path_stopped and wandb.run:\n",
    "                        vis_wandb_log_data[\"validation_samples/random_stopped\"] = wandb.Image(vis_path_stopped, caption=f\"Epoch {current_epoch_num_for_log} Random Stopped Sample\")\n",
    "                else:\n",
    "                    print(\"  Warning: Sampler did not return output for stopped sample.\")\n",
    "            else:\n",
    "                print(\"  Warning: No stopped (action 0.0) samples found in validation set. Skipping random stopped sample.\")\n",
    "        \n",
    "            # --- 3. Random Moving Sample ---\n",
    "            moving_action_val_vis = wandb_config.get('MOVING_ACTION_VALUE_FOR_VIS', 0.1)\n",
    "            if len(val_moving_subset) > 0:\n",
    "                print(f\"  Generating random moving sample (action {moving_action_val_vis})...\")\n",
    "                random_moving_idx_in_subset = random.randint(0, len(val_moving_subset) - 1)\n",
    "                moving_sample_data = val_moving_subset[random_moving_idx_in_subset]\n",
    "                prev_obs_moving, prev_act_moving, gt_moving_batch, gt_prev_frames_moving_seq = prepare_single_sample_for_sampler(moving_sample_data, DEVICE) # gt_moving_batch is [1,C,H,W]\n",
    "                with torch.no_grad():\n",
    "                    generated_output_tuple_moving = diffusion_sampler.sample(prev_obs=prev_obs_moving, prev_act=prev_act_moving)\n",
    "        \n",
    "                if generated_output_tuple_moving:\n",
    "                    generated_image_batch_moving = generated_output_tuple_moving[0] # This is [1, C, H, W]\n",
    "                    if generated_image_batch_moving.ndim == 4 and generated_image_batch_moving.shape[0] == 1:\n",
    "                        generated_image_to_save_moving = generated_image_batch_moving[0] # Extract single image: [C, H, W]\n",
    "                    else:\n",
    "                        generated_image_to_save_moving = generated_image_batch_moving\n",
    "        \n",
    "                    gt_image_to_save_moving = gt_moving_batch[0] # Extract single GT image: [C, H, W]\n",
    "        \n",
    "                    vis_path_moving = save_visualization_samples(\n",
    "                        generated_image_to_save_moving, # Should be [C,H,W]\n",
    "                        gt_image_to_save_moving,        # Should be [C,H,W]\n",
    "                        gt_prev_frames_moving_seq,\n",
    "                        current_epoch_num_for_log,\n",
    "                        config.SAMPLE_DIR,\n",
    "                        prefix=f\"val_vis_moving_act{str(moving_action_val_vis).replace('.', 'p')}_random\"\n",
    "                    )\n",
    "                    if vis_path_moving and wandb.run:\n",
    "                        vis_wandb_log_data[f\"validation_samples/random_moving_act{str(moving_action_val_vis).replace('.', 'p')}\"] = wandb.Image(vis_path_moving, caption=f\"Epoch {current_epoch_num_for_log} Random Moving Sample (Action {moving_action_val_vis})\")\n",
    "                else:\n",
    "                    print(\"  Warning: Sampler did not return output for moving sample.\")\n",
    "            else:\n",
    "                print(f\"  Warning: No moving (action {moving_action_val_vis}) samples found in validation set. Skipping random moving sample.\")\n",
    "            \n",
    "            denoiser.train() # Set model back to training mode\n",
    "            sample_duration = time.time() - sample_time_start\n",
    "            print(f'Sampling for epoch {current_epoch_num_for_log} took {sample_duration:.2f}s')\n",
    "            # Log all accumulated data for this epoch (losses + images)\n",
    "            if wandb.run:\n",
    "                wandb.log({**wandb_log_data, **vis_wandb_log_data, 'sampling_duration_sec': sample_duration})\n",
    "        elif wandb.run: # If not sampling, still log epoch metrics\n",
    "             wandb.log(wandb_log_data)\n",
    "    \n",
    "    \n",
    "        if (current_epoch_num_for_log % PLOT_EVERY == 0) or (epoch == NUM_EPOCHS - 1) or should_stop_early :\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(all_train_losses_for_plot, label=\"Avg Train Loss\")\n",
    "            plt.plot(all_val_losses_for_plot, label=\"Avg Validation Loss\")\n",
    "            if len(all_train_losses_for_plot) >= TRAIN_MOVING_AVG_WINDOW:\n",
    "                train_ma_plot = [sum(all_train_losses_for_plot[i-TRAIN_MOVING_AVG_WINDOW+1:i+1])/TRAIN_MOVING_AVG_WINDOW for i in range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot))]\n",
    "                plt.plot(range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot)), train_ma_plot, label=f'Train Loss MA ({TRAIN_MOVING_AVG_WINDOW} epochs)', linestyle=':')\n",
    "            if len(all_val_losses_for_plot) >= VAL_MOVING_AVG_WINDOW:\n",
    "                val_ma_plot = [sum(all_val_losses_for_plot[i-VAL_MOVING_AVG_WINDOW+1:i+1])/VAL_MOVING_AVG_WINDOW for i in range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot))]\n",
    "                plt.plot(range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot)), val_ma_plot, label=f'Val Loss MA ({VAL_MOVING_AVG_WINDOW} epochs)', linestyle='--')\n",
    "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Progress (Epoch {current_epoch_num_for_log})\")\n",
    "            plt.legend(); plt.grid(True)\n",
    "            plt.savefig(os.path.join(config.PLOT_DIR, f\"loss_plot_epoch_{current_epoch_num_for_log:04d}.png\"))\n",
    "            ### WANDB: Log epoch loss plot ###\n",
    "            wandb.log({\"epoch_loss_plot\": wandb.Image(plt, caption=f\"Loss Plot Epoch {current_epoch_num_for_log}\")})\n",
    "            plt.close()\n",
    "            print(f\"Saved loss plot up to epoch {current_epoch_num_for_log}\")\n",
    "    \n",
    "    overall_training_end_time = time.time()\n",
    "    total_training_duration_seconds = overall_training_end_time - overall_training_start_time\n",
    "    total_training_duration_formatted = str(datetime.timedelta(seconds=total_training_duration_seconds))\n",
    "    \n",
    "    # final_epoch_completed is the last epoch index that ran (0-indexed)\n",
    "    print(f\"--- Training Complete (Stopped after epoch {final_epoch_completed + 1}) ---\") \n",
    "    print(f\"Total training duration: {total_training_duration_formatted}\") \n",
    "    \n",
    "    # Final Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_train_losses_for_plot, label=\"Avg Train Loss\")\n",
    "    plt.plot(all_val_losses_for_plot, label=\"Avg Validation Loss\")\n",
    "    if len(all_train_losses_for_plot) >= TRAIN_MOVING_AVG_WINDOW:\n",
    "        train_ma_plot = [sum(all_train_losses_for_plot[i-TRAIN_MOVING_AVG_WINDOW+1:i+1])/TRAIN_MOVING_AVG_WINDOW for i in range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot))]\n",
    "        plt.plot(range(TRAIN_MOVING_AVG_WINDOW-1, len(all_train_losses_for_plot)), train_ma_plot, label=f'Train Loss MA ({TRAIN_MOVING_AVG_WINDOW} epochs)', linestyle=':')\n",
    "    if len(all_val_losses_for_plot) >= VAL_MOVING_AVG_WINDOW:\n",
    "        val_ma_plot = [sum(all_val_losses_for_plot[i-VAL_MOVING_AVG_WINDOW+1:i+1])/VAL_MOVING_AVG_WINDOW for i in range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot))]\n",
    "        plt.plot(range(VAL_MOVING_AVG_WINDOW-1, len(all_val_losses_for_plot)), val_ma_plot, label=f'Val Loss MA ({VAL_MOVING_AVG_WINDOW} epochs)', linestyle='--')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Denoiser Final Training & Validation Loss (Up to Epoch {final_epoch_completed + 1})\")\n",
    "    plt.legend(); plt.grid(True)\n",
    "    final_loss_plot_path = os.path.join(config.PLOT_DIR, \"denoiser_final_loss_plot.png\")\n",
    "    plt.savefig(final_loss_plot_path)\n",
    "    # plt.show() # Usually not needed in script, but can be uncommented for interactive\n",
    "    print(f\"Final loss plot saved to {final_loss_plot_path}\")\n",
    "    \n",
    "    ### WANDB: Log final loss plot and finish run ###\n",
    "    wandb.log({\"final_loss_plot\": wandb.Image(final_loss_plot_path, caption=f\"Final Loss Plot up to Epoch {final_epoch_completed + 1}\")})\n",
    "    wandb.finish()\n",
    "    print(\"Wandb run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae2abe1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Main Training Execution --- \n",
      "--- Configuration ---\n",
      "Using device: cuda\n",
      "Configuration loaded for _main_training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: irvin-hwang (irvin-hwang-simulacra-systems) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\jetbot-diffusion-world-model-kong-finder\\wandb\\run-20250721_081326-aih0944l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model/runs/aih0944l' target=\"_blank\">lyric-firefly-41</a></strong> to <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model/runs/aih0944l' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diamond-world-model/runs/aih0944l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb initialized for _main_training.\n",
      "--- Initializing Models for _main_training ---\n",
      "Denoiser model created for _main_training. Total parameter count: 330,464,131\n",
      "DiffusionSampler created for visualization in _main_training.\n",
      "--- Setting up Optimizer and Scheduler for _main_training ---\n",
      "Optimizer: AdamW with LR=0.0001\n",
      "LR Scheduler: LambdaLR with 100 warmup steps.\n",
      "Wandb watching denoiser model.\n",
      "No checkpoint found or specified for _main_training. Starting fresh.\n",
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n",
      "Full dataset size: 591\n",
      "Creating new train/val split...\n",
      "Saved new dataset split to C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\output_model_2hz_DIAMOND_laundry_nonincremental_test\\dataset_split.pth\n",
      "Training dataset size: 531, Validation dataset size: 60\n",
      "Preparing filtered validation subsets for visualization...\n",
      "Filtering dataset with 60 samples for actions: [0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4439907fa44b3183a1e9cea52a2bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering Dataset:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered down to 23 samples.\n",
      "Filtering dataset with 60 samples for actions: [0.13]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7dbf23c4c74251a9740ccdcfb55414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering Dataset:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered down to 37 samples.\n",
      "Found 23 stopped and 37 moving samples.\n",
      "--- Starting Training Process in _main_training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5985e1fcaba40a08c576d3b1ccf7af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/265 [00:14<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of size 3 took 3.2756783962249756 seconds\n",
      "Batch 1 of size 3 took 0.0870051383972168 seconds\n",
      "Batch 2 of size 3 took 0.09890341758728027 seconds\n",
      "Batch 3 of size 3 took 0.07392191886901855 seconds\n",
      "Batch 4 of size 3 took 0.0745077133178711 seconds\n",
      "Batch 5 of size 3 took 0.07424378395080566 seconds\n",
      "Batch 6 of size 3 took 0.06859827041625977 seconds\n",
      "Batch 7 of size 3 took 0.07200741767883301 seconds\n",
      "Batch 8 of size 3 took 0.07000017166137695 seconds\n",
      "Batch 9 of size 3 took 0.07436227798461914 seconds\n",
      "Batch 10 of size 3 took 0.0699472427368164 seconds\n",
      "Batch 11 of size 3 took 0.07294249534606934 seconds\n",
      "Batch 12 of size 3 took 0.06981921195983887 seconds\n",
      "Batch 13 of size 3 took 0.07068061828613281 seconds\n",
      "Batch 14 of size 3 took 0.09752774238586426 seconds\n",
      "Batch 15 of size 3 took 0.06808638572692871 seconds\n",
      "Batch 16 of size 3 took 0.07108092308044434 seconds\n",
      "Batch 17 of size 3 took 0.07436752319335938 seconds\n",
      "Batch 18 of size 3 took 0.09587621688842773 seconds\n",
      "Batch 19 of size 3 took 0.07367944717407227 seconds\n",
      "Batch 20 of size 3 took 0.09688067436218262 seconds\n",
      "Batch 21 of size 3 took 0.07482481002807617 seconds\n",
      "Batch 22 of size 3 took 0.0702970027923584 seconds\n",
      "Batch 23 of size 3 took 0.07224726676940918 seconds\n",
      "Batch 24 of size 3 took 0.09834814071655273 seconds\n",
      "Batch 25 of size 3 took 0.07469987869262695 seconds\n",
      "Batch 26 of size 3 took 0.06999969482421875 seconds\n",
      "Batch 27 of size 3 took 0.09747672080993652 seconds\n",
      "Batch 28 of size 3 took 0.06824851036071777 seconds\n",
      "Batch 29 of size 3 took 0.09730815887451172 seconds\n",
      "Batch 30 of size 3 took 0.06784772872924805 seconds\n",
      "Batch 31 of size 3 took 0.07044196128845215 seconds\n",
      "Batch 32 of size 3 took 0.07429075241088867 seconds\n",
      "Batch 33 of size 3 took 0.09723091125488281 seconds\n",
      "Batch 34 of size 3 took 0.07467126846313477 seconds\n",
      "Batch 35 of size 3 took 0.09651041030883789 seconds\n",
      "Batch 36 of size 3 took 0.09585833549499512 seconds\n",
      "Batch 37 of size 3 took 0.09600567817687988 seconds\n",
      "Batch 38 of size 3 took 0.07299637794494629 seconds\n",
      "Batch 39 of size 3 took 0.0962514877319336 seconds\n",
      "Batch 40 of size 3 took 0.06752204895019531 seconds\n",
      "Batch 41 of size 3 took 0.0672156810760498 seconds\n",
      "Batch 42 of size 3 took 0.07061529159545898 seconds\n",
      "Batch 43 of size 3 took 0.07358336448669434 seconds\n",
      "Batch 44 of size 3 took 0.0696408748626709 seconds\n",
      "Batch 45 of size 3 took 0.0980532169342041 seconds\n",
      "Batch 46 of size 3 took 0.06803536415100098 seconds\n",
      "Batch 47 of size 3 took 0.06890368461608887 seconds\n",
      "Batch 48 of size 3 took 0.07307100296020508 seconds\n",
      "Batch 49 of size 3 took 0.0737466812133789 seconds\n",
      "Batch 50 of size 3 took 0.07395005226135254 seconds\n",
      "Batch 51 of size 3 took 0.06929612159729004 seconds\n",
      "Batch 52 of size 3 took 0.07161235809326172 seconds\n",
      "Batch 53 of size 3 took 0.09721803665161133 seconds\n",
      "Batch 54 of size 3 took 0.06722331047058105 seconds\n",
      "Batch 55 of size 3 took 0.07657265663146973 seconds\n",
      "Batch 56 of size 3 took 0.07194161415100098 seconds\n",
      "Batch 57 of size 3 took 0.07327604293823242 seconds\n",
      "Batch 58 of size 3 took 0.06835365295410156 seconds\n",
      "Batch 59 of size 3 took 0.07439851760864258 seconds\n",
      "Batch 60 of size 3 took 0.07118916511535645 seconds\n",
      "Batch 61 of size 3 took 0.07000517845153809 seconds\n",
      "Batch 62 of size 3 took 0.09483647346496582 seconds\n",
      "Batch 63 of size 3 took 0.24762988090515137 seconds\n",
      "Batch 64 of size 3 took 0.08889079093933105 seconds\n",
      "Batch 65 of size 3 took 0.07601666450500488 seconds\n",
      "Batch 66 of size 3 took 0.06880903244018555 seconds\n",
      "Batch 67 of size 3 took 0.07508397102355957 seconds\n",
      "Batch 68 of size 3 took 0.07057619094848633 seconds\n",
      "Batch 69 of size 3 took 0.07411050796508789 seconds\n",
      "Batch 70 of size 3 took 0.0733194351196289 seconds\n",
      "Batch 71 of size 3 took 0.09682846069335938 seconds\n",
      "Batch 72 of size 3 took 0.06831049919128418 seconds\n",
      "Batch 73 of size 3 took 0.07239103317260742 seconds\n",
      "Batch 74 of size 3 took 0.07567119598388672 seconds\n",
      "Batch 75 of size 3 took 0.07131505012512207 seconds\n",
      "Batch 76 of size 3 took 0.09728336334228516 seconds\n",
      "Batch 77 of size 3 took 0.07470035552978516 seconds\n",
      "Batch 78 of size 3 took 0.07545161247253418 seconds\n",
      "Batch 79 of size 3 took 0.09787273406982422 seconds\n",
      "Batch 80 of size 3 took 0.06920576095581055 seconds\n",
      "Batch 81 of size 3 took 0.06784963607788086 seconds\n",
      "Batch 82 of size 3 took 0.07076215744018555 seconds\n",
      "Batch 83 of size 3 took 0.07482385635375977 seconds\n",
      "Batch 84 of size 3 took 0.07334470748901367 seconds\n",
      "Batch 85 of size 3 took 0.07735300064086914 seconds\n",
      "Batch 86 of size 3 took 0.07734918594360352 seconds\n",
      "Batch 87 of size 3 took 0.0780036449432373 seconds\n",
      "Batch 88 of size 3 took 0.07900691032409668 seconds\n",
      "Batch 89 of size 3 took 0.07750892639160156 seconds\n",
      "Batch 90 of size 3 took 0.0770573616027832 seconds\n",
      "Batch 91 of size 3 took 0.07829809188842773 seconds\n",
      "Batch 92 of size 3 took 0.07780218124389648 seconds\n",
      "Batch 93 of size 3 took 0.07780122756958008 seconds\n",
      "Batch 94 of size 3 took 0.09046721458435059 seconds\n",
      "Batch 95 of size 3 took 0.07696771621704102 seconds\n",
      "Batch 96 of size 3 took 0.0900721549987793 seconds\n",
      "Batch 97 of size 3 took 0.08200550079345703 seconds\n",
      "Batch 98 of size 3 took 0.08960962295532227 seconds\n",
      "Batch 99 of size 3 took 2.369371175765991 seconds\n",
      "Batch 100 of size 3 took 0.07749724388122559 seconds\n",
      "Batch 101 of size 3 took 0.0888369083404541 seconds\n",
      "Batch 102 of size 3 took 0.0749199390411377 seconds\n",
      "Batch 103 of size 3 took 0.07630372047424316 seconds\n",
      "Batch 104 of size 3 took 0.07599949836730957 seconds\n",
      "Batch 105 of size 3 took 0.07734131813049316 seconds\n",
      "Batch 106 of size 3 took 0.07736015319824219 seconds\n",
      "Batch 107 of size 3 took 0.07704758644104004 seconds\n",
      "Batch 108 of size 3 took 0.07871866226196289 seconds\n",
      "Batch 109 of size 3 took 0.07677602767944336 seconds\n",
      "Batch 110 of size 3 took 0.07655167579650879 seconds\n",
      "Batch 111 of size 3 took 0.07651066780090332 seconds\n",
      "Batch 112 of size 3 took 0.07776570320129395 seconds\n",
      "Batch 113 of size 3 took 0.09025430679321289 seconds\n",
      "Batch 114 of size 3 took 0.0911860466003418 seconds\n",
      "Batch 115 of size 3 took 0.06924176216125488 seconds\n",
      "Batch 116 of size 3 took 0.07729887962341309 seconds\n",
      "Batch 117 of size 3 took 0.0778656005859375 seconds\n",
      "Batch 118 of size 3 took 0.07724213600158691 seconds\n",
      "Batch 119 of size 3 took 0.07874727249145508 seconds\n",
      "Batch 120 of size 3 took 0.07756614685058594 seconds\n",
      "Batch 121 of size 3 took 0.07750892639160156 seconds\n",
      "Batch 122 of size 3 took 0.07717370986938477 seconds\n",
      "Batch 123 of size 3 took 0.06982851028442383 seconds\n",
      "Batch 124 of size 3 took 0.0766592025756836 seconds\n",
      "Batch 125 of size 3 took 0.08179879188537598 seconds\n",
      "Batch 126 of size 3 took 0.07473349571228027 seconds\n",
      "Batch 127 of size 3 took 0.08676314353942871 seconds\n",
      "Batch 128 of size 3 took 0.08426809310913086 seconds\n",
      "Batch 129 of size 3 took 0.06886577606201172 seconds\n",
      "Batch 130 of size 3 took 0.07619714736938477 seconds\n",
      "Batch 131 of size 3 took 0.06978678703308105 seconds\n",
      "Batch 132 of size 3 took 0.06966686248779297 seconds\n",
      "Batch 133 of size 3 took 0.07700204849243164 seconds\n",
      "Batch 134 of size 3 took 0.0756082534790039 seconds\n",
      "Batch 135 of size 3 took 0.07631587982177734 seconds\n",
      "Batch 136 of size 3 took 0.07593297958374023 seconds\n",
      "Batch 137 of size 3 took 0.0767374038696289 seconds\n",
      "Batch 138 of size 3 took 0.08896517753601074 seconds\n",
      "Batch 139 of size 3 took 0.0771794319152832 seconds\n",
      "Batch 140 of size 3 took 0.0738983154296875 seconds\n",
      "Batch 141 of size 3 took 0.0766592025756836 seconds\n",
      "Batch 142 of size 3 took 0.07657122611999512 seconds\n",
      "Batch 143 of size 3 took 0.07433485984802246 seconds\n",
      "Batch 144 of size 3 took 0.07627725601196289 seconds\n",
      "Batch 145 of size 3 took 0.07618832588195801 seconds\n",
      "Batch 146 of size 3 took 0.08891010284423828 seconds\n",
      "Batch 147 of size 3 took 0.0768117904663086 seconds\n",
      "Batch 148 of size 3 took 0.06781649589538574 seconds\n",
      "Batch 149 of size 3 took 0.07702445983886719 seconds\n",
      "Batch 150 of size 3 took 0.07500076293945312 seconds\n",
      "Batch 151 of size 3 took 0.07406210899353027 seconds\n",
      "Batch 152 of size 3 took 0.07696008682250977 seconds\n",
      "Batch 153 of size 3 took 0.06697750091552734 seconds\n",
      "Batch 154 of size 3 took 0.07570648193359375 seconds\n",
      "Batch 155 of size 3 took 0.07499527931213379 seconds\n",
      "Batch 156 of size 3 took 0.07566142082214355 seconds\n",
      "Batch 157 of size 3 took 0.07718682289123535 seconds\n",
      "Batch 158 of size 3 took 0.07462096214294434 seconds\n",
      "Batch 159 of size 3 took 0.0769968032836914 seconds\n",
      "Batch 160 of size 3 took 0.07630085945129395 seconds\n",
      "Batch 161 of size 3 took 0.07698345184326172 seconds\n",
      "Batch 162 of size 3 took 0.07616758346557617 seconds\n",
      "Batch 163 of size 3 took 0.07674455642700195 seconds\n",
      "Batch 164 of size 3 took 0.0761871337890625 seconds\n",
      "Batch 165 of size 3 took 0.0735480785369873 seconds\n",
      "Batch 166 of size 3 took 0.07660317420959473 seconds\n",
      "Batch 167 of size 3 took 0.07724595069885254 seconds\n",
      "Batch 168 of size 3 took 0.07521891593933105 seconds\n",
      "Batch 169 of size 3 took 0.07774114608764648 seconds\n",
      "Batch 170 of size 3 took 0.07627511024475098 seconds\n",
      "Batch 171 of size 3 took 0.07671666145324707 seconds\n",
      "Batch 172 of size 3 took 0.07559680938720703 seconds\n",
      "Batch 173 of size 3 took 0.07805824279785156 seconds\n",
      "Batch 174 of size 3 took 0.0718834400177002 seconds\n",
      "Batch 175 of size 3 took 0.07628417015075684 seconds\n",
      "Batch 176 of size 3 took 0.07394194602966309 seconds\n",
      "Batch 177 of size 3 took 0.07619953155517578 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43m_main_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 232\u001b[0m, in \u001b[0;36m_main_training\u001b[1;34m(finetune_checkpoint)\u001b[0m\n\u001b[0;32m    230\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    231\u001b[0m current_epoch_num_for_log \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 232\u001b[0m avg_train_loss, train_step_count \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_denoiser_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenoiser_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGRAD_CLIP_VALUE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_num_for_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_epoch_num_for_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_batches_total\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_val_batches_total\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_val_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_step_count\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m all_train_losses_for_plot\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n\u001b[0;32m    241\u001b[0m train_loss_moving_avg_q\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m, in \u001b[0;36mtrain_denoiser_epoch\u001b[1;34m(denoiser_model, train_dl, opt, scheduler, grad_clip_val, device, epoch_num_for_log, num_train_batches_total, num_val_batches_total, train_step_start)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss, logs \u001b[38;5;241m=\u001b[39m denoiser_model(current_batch_obj)\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[1;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_clip_val \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main_training()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
