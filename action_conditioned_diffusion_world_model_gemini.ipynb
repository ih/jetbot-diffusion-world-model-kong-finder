{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b40e0f-8c29-4a90-9e82-5995ef5a0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import config\n",
    "from torch.utils.data import random_split\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from jetbot_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e89e9ec-9f79-4a17-800e-243b0864cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diffusion Helpers ---\n",
    "def linear_beta_schedule(timesteps, beta_start, beta_end):\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, betas, alphas_cumprod, device=\"cpu\"):\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(torch.sqrt(alphas_cumprod), t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        torch.sqrt(1. - alphas_cumprod), t, x_0.shape\n",
    "    )\n",
    "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise, noise\n",
    "\n",
    "# --- U-Net Model ---\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t, ):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        h = h + time_emb\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, image_channels=3, down_channels=(64, 128, 256), up_channels=(256, 128, 64), time_emb_dim=32, num_prev_frames=4):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        self.conv0 = nn.Conv2d(image_channels * (num_prev_frames + 1), down_channels[0], 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim + 1) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim + 1, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], image_channels, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, timestep, action, prev_frames):\n",
    "        x = torch.cat([x, prev_frames], dim=1)\n",
    "\n",
    "        t = self.time_mlp(timestep)\n",
    "        if action is not None:\n",
    "            action = action.float()\n",
    "            if len(action.shape) == 1:\n",
    "                action = action.unsqueeze(1)\n",
    "            t = torch.cat([t, action], dim=1)\n",
    "\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train(model, dataloader, optimizer, betas, alphas_cumprod, start_epoch, num_epochs,\n",
    "          device, save_every, sample_every, checkpoint_dir, sample_dir, plot_dir,\n",
    "          plot_every, use_fp16, accumulation_steps, num_prev_frames,\n",
    "          early_stopping_patience, early_stopping_percentage, min_epochs):\n",
    "    \"\"\"\n",
    "    Trains the diffusion model with early stopping and best model saving/deletion.\n",
    "    \"\"\"\n",
    "\n",
    "    all_losses = []\n",
    "    start_time = time.time()\n",
    "    last_plot_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    epochs_without_improvement = 0\n",
    "    moving_avg_window = 10\n",
    "    moving_avg_losses = []\n",
    "    previous_best_model_path = None  # Keep track of the previous best model's path\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for step, (images, actions, prev_frames) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            actions = actions.to(device)\n",
    "            prev_frames = prev_frames.to(device)\n",
    "            t = torch.randint(0, config.NUM_TIMESTEPS, (images.shape[0],), device=device).long()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_fp16):\n",
    "                x_noisy, noise = forward_diffusion_sample(images, t, betas, alphas_cumprod, device)\n",
    "                predicted_noise = model(x_noisy, t, actions, prev_frames)\n",
    "                loss = F.mse_loss(noise, predicted_noise)\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_losses.append(loss.item() * accumulation_steps)\n",
    "            pbar.set_postfix({\"Loss\": loss.item() * accumulation_steps})\n",
    "\n",
    "        if optimizer.param_groups[0]['params'][0].grad is not None:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        all_losses.append(avg_epoch_loss)\n",
    "\n",
    "        moving_avg_losses.append(avg_epoch_loss)\n",
    "        if len(moving_avg_losses) > moving_avg_window:\n",
    "            moving_avg_losses.pop(0)\n",
    "        current_moving_avg = sum(moving_avg_losses) / len(moving_avg_losses)\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_epoch_loss,\n",
    "            }, os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\"))\n",
    "            print(f\"Saved model checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "        if (epoch + 1) % sample_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                random_idx = torch.randint(0, len(dataset), (1,)).item()\n",
    "                real_current_frame, action, real_prev_frames = dataset[random_idx]\n",
    "                real_current_frame = real_current_frame.unsqueeze(0).to(device)\n",
    "                real_prev_frames = real_prev_frames.unsqueeze(0).to(device)\n",
    "                action = action.to(device)\n",
    "\n",
    "                t_sample = torch.tensor([config.NUM_TIMESTEPS - 1], device=device, dtype=torch.long)\n",
    "                x_noisy, _ = forward_diffusion_sample(real_current_frame, t_sample, betas, alphas_cumprod, device)\n",
    "                x = x_noisy\n",
    "\n",
    "                for i in reversed(range(1, config.NUM_TIMESTEPS)):\n",
    "                    t = (torch.ones(1) * i).long().to(device)\n",
    "                    with torch.cuda.amp.autocast(enabled=use_fp16):\n",
    "                        predicted_noise = model(x, t, action, real_prev_frames)\n",
    "\n",
    "                    alpha = alphas[t][:, None, None, None]\n",
    "                    alpha_hat = alphas_cumprod[t][:, None, None, None]\n",
    "                    beta = betas[t][:, None, None, None]\n",
    "\n",
    "                    if i > 1:\n",
    "                        noise = torch.randn_like(x)\n",
    "                    else:\n",
    "                        noise = torch.zeros_like(x)\n",
    "                    x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "                predicted_next_frame = (x.clamp(-1, 1) + 1) / 2\n",
    "                predicted_next_frame = (predicted_next_frame * 255).type(torch.uint8)\n",
    "                prev_images = []\n",
    "\n",
    "                for i in range(num_prev_frames):\n",
    "                    frame = real_prev_frames[0, (i * 3):(i + 1) * 3, :, :]\n",
    "                    frame = (frame.clamp(-1, 1) + 1) / 2\n",
    "                    frame = (frame * 255).type(torch.uint8)\n",
    "                    prev_images.append(transforms.ToPILImage()(frame))\n",
    "\n",
    "                current_image = transforms.ToPILImage()((real_current_frame[0].clamp(-1, 1) + 1) / 2 * 255).convert(\"RGB\") #Fixed\n",
    "                predicted_image = transforms.ToPILImage()(predicted_next_frame[0]).convert(\"RGB\")\n",
    "\n",
    "                total_width = (num_prev_frames + 2) * config.IMAGE_SIZE\n",
    "                max_height = config.IMAGE_SIZE\n",
    "                new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "                x_offset = 0\n",
    "                for image in prev_images:\n",
    "                    new_im.paste(image, (x_offset,0))\n",
    "                    x_offset += config.IMAGE_SIZE\n",
    "                new_im.paste(current_image, (x_offset, 0))\n",
    "                x_offset += config.IMAGE_SIZE\n",
    "                new_im.paste(predicted_image, (x_offset, 0))\n",
    "\n",
    "                new_im.save(os.path.join(sample_dir, f\"sample_epoch_{epoch+1}.png\"))\n",
    "                print(f\"Saved sample image at epoch {epoch+1}\")\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if (epoch + 1) % plot_every == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            formatted_time = str(datetime.timedelta(seconds=elapsed_time))\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "            axes[0].plot(all_losses)\n",
    "            axes[0].set_xlabel(\"Epoch\")\n",
    "            axes[0].set_ylabel(\"Loss\")\n",
    "            axes[0].set_title(f\"Training Loss from Start (Time: {formatted_time})\")\n",
    "            axes[0].grid(True)\n",
    "\n",
    "            axes[1].plot(range(last_plot_epoch + 1, epoch + 2), all_losses[last_plot_epoch:])\n",
    "            axes[1].set_xlabel(\"Epoch\")\n",
    "            axes[1].set_ylabel(\"Loss\")\n",
    "            axes[1].set_title(f\"Loss Since Last Plot (Epoch {last_plot_epoch + 1})\")\n",
    "            axes[1].grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_dir, f\"loss_plot_epoch_{epoch+1}.png\"))\n",
    "            plt.close()\n",
    "            print(f\"Epoch {epoch+1}: Loss = {avg_epoch_loss:.4f}, Time = {formatted_time}\")\n",
    "\n",
    "            last_plot_epoch = epoch\n",
    "\n",
    "        # --- Early Stopping (Dynamic Threshold) and Best Model Saving/Deletion---\n",
    "        if early_stopping_patience is not None and epoch + 1 > min_epochs:\n",
    "            if current_moving_avg < best_loss:\n",
    "                best_loss = current_moving_avg\n",
    "                best_epoch = epoch + 1\n",
    "                epochs_without_improvement = 0\n",
    "\n",
    "                # Save the *best* model\n",
    "                new_best_model_path = os.path.join(checkpoint_dir, f\"model_best_epoch_{best_epoch}.pth\")\n",
    "                torch.save({\n",
    "                    'epoch': best_epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_loss,  # Save the best loss\n",
    "                }, new_best_model_path)\n",
    "                print(f\"Saved best model at epoch {best_epoch} with loss {best_loss}\")\n",
    "\n",
    "                # Delete the *previous* best model (if it exists)\n",
    "                if previous_best_model_path and os.path.exists(previous_best_model_path):\n",
    "                    os.remove(previous_best_model_path)\n",
    "                    print(f\"Deleted previous best model: {previous_best_model_path}\")\n",
    "                previous_best_model_path = new_best_model_path # Update the path\n",
    "\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                if len(moving_avg_losses) == moving_avg_window:\n",
    "                    prev_moving_avg = sum(moving_avg_losses[:-1]) / (moving_avg_window - 1)\n",
    "                    improvement = (prev_moving_avg - current_moving_avg) / prev_moving_avg * 100\n",
    "                if improvement < early_stopping_percentage:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.  Improvement: {improvement:.2f}%\")\n",
    "                    break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    formatted_time = str(datetime.timedelta(seconds=total_time))\n",
    "    print(f\"Total training time: {formatted_time}\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_epoch_loss,\n",
    "    }, os.path.join(checkpoint_dir, \"model_last.pth\"))\n",
    "    print(f\"Saved last model at epoch {epoch+1} with loss {avg_epoch_loss}\")\n",
    "\n",
    "    return all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356d5ca4-d8af-471b-afce-8023111e6f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n",
      "Total rows in CSV: 23081, Valid sequence start indices: 23037\n",
      "Dataset split file not found, creating a new split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a34f33892c4de2bc176fd65f835c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb9fb43e8646cda47e6e1a3a3b99c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604f8b33360c4978a167de83029cfad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bd7cd69fa943bda164051faa98d164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11366287ba9948678e9497d161efd528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 5\n",
      "Saved sample image at epoch 5\n",
      "Epoch 5: Loss = 0.0103, Time = 0:28:15.005614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972e565f11cd44a8a29fcbe06f9046ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 6 with loss 0.025121140641881878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bc5605ab8648998384ba91b17ed7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 7 with loss 0.02272802283702067\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9225eb6df89c4ba3bafa5259bc2428d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 8 with loss 0.02070872366505859\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_7.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798a4fd4ad534444b9ff6014ef36ab6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 9 with loss 0.01910940636241214\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_8.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e78d9209e440d2adc66958be73e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 10\n",
      "Saved sample image at epoch 10\n",
      "Epoch 10: Loss = 0.0061, Time = 0:53:59.690766\n",
      "Saved best model at epoch 10 with loss 0.01780402315868059\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_9.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639696bf7d4a46c5baf041a8b7eff1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 11 with loss 0.010155675140448593\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_10.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4440b47c1546e08b0897bd22e7488a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 12 with loss 0.008603343254568093\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_11.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fb128538804e21b4c612e0aa1b6466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 13 with loss 0.007488475360761035\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_12.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43453071ccb245ed83df35e976368cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model at epoch 14 with loss 0.006638822444635808\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_13.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5f71e77359408c8b5d353235231532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/15:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint at epoch 15\n",
      "Saved sample image at epoch 15\n",
      "Epoch 15: Loss = 0.0044, Time = 1:19:40.021436\n",
      "Saved best model at epoch 15 with loss 0.006042996834935366\n",
      "Deleted previous best model: output_two_action_datset_refactor_test\\checkpoints\\model_best_epoch_14.pth\n",
      "Total training time: 1:19:40.373441\n",
      "Saved last model at epoch 15 with loss 0.004382078121043125\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Data Transforms ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # --- Create Dataset and DataLoader ---\n",
    "    dataset = JetbotDataset(config.CSV_PATH, config.DATA_DIR, config.IMAGE_SIZE, config.NUM_PREV_FRAMES, transform=transform)\n",
    "    \n",
    "    # Try to load existing split\n",
    "    train_dataset, test_dataset = load_train_test_split(dataset, config.SPLIT_DATASET_FILENAME)\n",
    "    \n",
    "    if train_dataset is None or test_dataset is None:\n",
    "        print(\"Dataset split file not found, creating a new split...\")\n",
    "        train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "        save_existing_split(train_dataset, test_dataset, config.SPLIT_DATASET_FILENAME)\n",
    "    else:\n",
    "        print(\"Loaded existing dataset split.\")\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False) # Batch size 1 for easier evaluation\n",
    "    \n",
    "    # --- Calculate Betas and Alphas ---\n",
    "    betas = linear_beta_schedule(config.NUM_TIMESTEPS, config.BETA_START, config.BETA_END).to(config.DEVICE)\n",
    "    #betas = cosine_beta_schedule(NUM_TIMESTEPS).to(DEVICE) # Alternative\n",
    "    \n",
    "    alphas = (1. - betas).to(config.DEVICE)\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0).to(config.DEVICE)\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0).to(config.DEVICE)\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(config.DEVICE)\n",
    "    \n",
    "    # --- Create Model and Optimizer ---\n",
    "    model = SimpleUNet(image_channels=3, down_channels=(64, 128, 256), up_channels=(256, 128, 64), time_emb_dim=32, num_prev_frames=config.NUM_PREV_FRAMES).to(config.DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    # --- Load Checkpoint (if available) ---\n",
    "    if config.LOAD_CHECKPOINT:\n",
    "        checkpoint = torch.load(config.LOAD_CHECKPOINT)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        START_EPOCH = checkpoint['epoch']\n",
    "        print(f\"Loaded checkpoint from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        START_EPOCH = 0\n",
    "    \n",
    "    # --- Train the Model ---\n",
    "    losses = train(model, train_dataloader, optimizer, betas, alphas_cumprod, START_EPOCH, config.NUM_EPOCHS, config.DEVICE,\n",
    "                   config.SAVE_MODEL_EVERY, config.SAMPLE_EVERY, config.CHECKPOINT_DIR, config.SAMPLE_DIR, config.PLOT_DIR, config.PLOT_EVERY, config.USE_FP16,\n",
    "                   config.ACCUMULATION_STEPS, config.NUM_PREV_FRAMES, early_stopping_patience=config.EARLY_STOPPING_PATIENCE, early_stopping_percentage=config.EARLY_STOPPING_PERCENTAGE, min_epochs=config.MIN_EPOCHS)\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # --- Final Loss Plot ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(config.PLOT_DIR, \"loss_plot_final.png\"))  # Save to plot dir\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16933dd4-020b-40b6-9c67-b420f3ed3ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.SAMPLE_EVERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69705aac-316a-4771-aa0f-ccab7b3d9eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d410bf-97c6-44f0-a38c-4e378b0659ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
