{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b40e0f-8c29-4a90-9e82-5995ef5a0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import config\n",
    "from torch.utils.data import random_split\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from jetbot_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e89e9ec-9f79-4a17-800e-243b0864cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diffusion Helpers ---\n",
    "def linear_beta_schedule(timesteps, beta_start, beta_end):\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, betas, alphas_cumprod, device=\"cpu\"):\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(torch.sqrt(alphas_cumprod), t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        torch.sqrt(1. - alphas_cumprod), t, x_0.shape\n",
    "    )\n",
    "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise, noise\n",
    "\n",
    "# --- U-Net Model ---\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # Ensure output matches the embedding dim even if input dim is odd\n",
    "        if self.dim % 2 == 1:\n",
    "             embeddings = F.pad(embeddings, (0, 1))\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        # Adjusted time_mlp input dimension\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            # Adjusted Conv2d input channels for concatenation\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t_emb): # Modified to accept pre-computed embedding\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding - Project and add\n",
    "        time_emb_proj = self.relu(self.time_mlp(t_emb))\n",
    "        time_emb_proj = time_emb_proj[(..., ) + (None, ) * 2] # Reshape for spatial broadcast\n",
    "        h = h + time_emb_proj # Add time embedding\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "        \n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, image_channels=3, time_emb_dim=32, num_prev_frames=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Increased Channel Depth and Added Layer ---\n",
    "        down_channels = (128, 256, 512, 512) # Increased channels, added a level\n",
    "        up_channels = (512, 512, 256, 128)   # Increased channels, added a level\n",
    "\n",
    "        # Input channels = current frame + previous frames\n",
    "        in_img_channels = image_channels * (num_prev_frames + 1)\n",
    "        action_dim = 1 # Single motor action\n",
    "\n",
    "        # Effective embedding dimension including action\n",
    "        effective_time_emb_dim = time_emb_dim + action_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # --- Model Layers ---\n",
    "        self.conv0 = nn.Conv2d(in_img_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(down_channels)-1):\n",
    "            self.downs.append(Block(down_channels[i], down_channels[i+1], effective_time_emb_dim))\n",
    "\n",
    "        # Bottleneck (implicitly defined by channel changes)\n",
    "        # No extra bottleneck block needed here, just the transition\n",
    "\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in range(len(up_channels)-1):\n",
    "            self.ups.append(Block(up_channels[i], up_channels[i+1], effective_time_emb_dim, up=True))\n",
    "\n",
    "        # Final output layer (outputs noise prediction, same channels as original image)\n",
    "        self.output = nn.Conv2d(up_channels[-1], image_channels, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, timestep, action, prev_frames):\n",
    "        # x: noisy next frame (batch, 3, H, W)\n",
    "        # timestep: (batch,)\n",
    "        # action: (batch, 1)\n",
    "        # prev_frames: (batch, num_prev_frames * 3, H, W)\n",
    "\n",
    "        # Concatenate the current noisy image with the previous frames\n",
    "        x = torch.cat([x, prev_frames], dim=1) # Shape: (batch, (N+1)*C, H, W)\n",
    "\n",
    "        # --- Prepare Time and Action Embedding ---\n",
    "        t_emb = self.time_mlp(timestep) # Shape: (batch, time_emb_dim)\n",
    "        if action is not None:\n",
    "            action = action.float()\n",
    "            if len(action.shape) == 1:\n",
    "                 action = action.unsqueeze(1) # Ensure shape is (batch, 1)\n",
    "            # Concatenate time embedding and action embedding\n",
    "            t_action_emb = torch.cat([t_emb, action], dim=1) # Shape: (batch, time_emb_dim + 1)\n",
    "        else:\n",
    "            # Handle cases where action might be None (e.g., unconditional generation if needed later)\n",
    "            # Pad action dimensions if needed - adjust padding based on your effective_time_emb_dim\n",
    "            padding = torch.zeros(t_emb.shape[0], 1, device=t_emb.device)\n",
    "            t_action_emb = torch.cat([t_emb, padding], dim=1)\n",
    "        # --- End Embedding Prep ---\n",
    "\n",
    "        # --- U-Net Architecture ---\n",
    "        x = self.conv0(x) # Initial processing of combined input\n",
    "        residual_inputs = []\n",
    "        # Downsampling path\n",
    "        for i, down_block in enumerate(self.downs):\n",
    "            x = down_block(x, t_action_emb) # Pass combined embedding to blocks\n",
    "            residual_inputs.append(x)\n",
    "\n",
    "        # Upsampling path\n",
    "        for i, up_block in enumerate(self.ups):\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1) # Concatenate skip connection\n",
    "            x = up_block(x, t_action_emb) # Pass combined embedding to blocks\n",
    "\n",
    "        return self.output(x) # Predict noise\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train(model, dataloader, optimizer, betas, alphas_cumprod, start_epoch, num_epochs,\n",
    "          device, save_every, sample_every, checkpoint_dir, sample_dir, plot_dir,\n",
    "          plot_every, use_fp16, accumulation_steps, num_prev_frames,\n",
    "          early_stopping_patience, early_stopping_percentage, min_epochs):\n",
    "    \"\"\"\n",
    "    Trains the diffusion model with early stopping and best model saving/deletion.\n",
    "    \"\"\"\n",
    "\n",
    "    all_losses = []\n",
    "    start_time = time.time()\n",
    "    last_plot_epoch = start_epoch - 1\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = start_epoch\n",
    "    epochs_without_improvement = 0\n",
    "    moving_avg_window = 10\n",
    "    moving_avg_losses = []\n",
    "    previous_best_model_path = None  # Keep track of the previous best model's path\n",
    "\n",
    "    # --- Load previous best model path if resuming ---\n",
    "    # Find the highest epoch 'best' model if resuming, to delete the correct one later\n",
    "    if start_epoch > 0:\n",
    "        try:\n",
    "            existing_best = [f for f in os.listdir(checkpoint_dir) if f.startswith('model_best_epoch_')]\n",
    "            if existing_best:\n",
    "                existing_best.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]), reverse=True)\n",
    "                previous_best_model_path = os.path.join(checkpoint_dir, existing_best[0])\n",
    "                # Extract best_loss from the loaded best model checkpoint if desired\n",
    "                # best_checkpoint = torch.load(previous_best_model_path, map_location='cpu')\n",
    "                # best_loss = best_checkpoint.get('loss', float('inf')) # Restore best loss\n",
    "                print(f\"Found previous best model: {previous_best_model_path}\") # Verify best loss restoration if needed\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not determine previous best model path: {e}\")\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for step, (images, actions, prev_frames) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            actions = actions.to(device)\n",
    "            prev_frames = prev_frames.to(device)\n",
    "            t = torch.randint(0, config.NUM_TIMESTEPS, (images.shape[0],), device=device).long()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_fp16):\n",
    "                x_noisy, noise = forward_diffusion_sample(images, t, betas, alphas_cumprod, device)\n",
    "                predicted_noise = model(x_noisy, t, actions, prev_frames)\n",
    "                loss = F.mse_loss(noise, predicted_noise)\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_losses.append(loss.item() * accumulation_steps)\n",
    "            pbar.set_postfix({\"Loss\": loss.item() * accumulation_steps})\n",
    "\n",
    "        if optimizer.param_groups[0]['params'][0].grad is not None:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)  if epoch_losses else float('nan') # Handle empty epoch_losses\n",
    "        if np.isnan(avg_epoch_loss):\n",
    "            print(f\"Warning: NaN loss detected for epoch {epoch+1}. Skipping update/plot.\")\n",
    "            # Optionally: break or handle NaN case differently\n",
    "            continue\n",
    "        all_losses.append(avg_epoch_loss)\n",
    "\n",
    "        moving_avg_losses.append(avg_epoch_loss)\n",
    "        if len(moving_avg_losses) > moving_avg_window:\n",
    "            moving_avg_losses.pop(0)\n",
    "        current_moving_avg = sum(moving_avg_losses) / len(moving_avg_losses)\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_epoch_loss,\n",
    "            }, os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\"))\n",
    "            print(f\"Saved model checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "        if (epoch + 1) % sample_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                random_idx = torch.randint(0, len(dataset), (1,)).item()\n",
    "                real_current_frame, action, real_prev_frames = dataset[random_idx]\n",
    "                real_current_frame = real_current_frame.unsqueeze(0).to(device)\n",
    "                real_prev_frames = real_prev_frames.unsqueeze(0).to(device)\n",
    "                action = action.to(device)\n",
    "\n",
    "                t_sample = torch.tensor([config.NUM_TIMESTEPS - 1], device=device, dtype=torch.long)\n",
    "                x_noisy, _ = forward_diffusion_sample(real_current_frame, t_sample, betas, alphas_cumprod, device)\n",
    "                x = x_noisy\n",
    "\n",
    "                for i in reversed(range(1, config.NUM_TIMESTEPS)):\n",
    "                    t = (torch.ones(1) * i).long().to(device)\n",
    "                    with torch.cuda.amp.autocast(enabled=use_fp16):\n",
    "                        predicted_noise = model(x, t, action, real_prev_frames)\n",
    "\n",
    "                    alpha = alphas[t][:, None, None, None]\n",
    "                    alpha_hat = alphas_cumprod[t][:, None, None, None]\n",
    "                    beta = betas[t][:, None, None, None]\n",
    "\n",
    "                    if i > 1:\n",
    "                        noise = torch.randn_like(x)\n",
    "                    else:\n",
    "                        noise = torch.zeros_like(x)\n",
    "                    x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "                predicted_next_frame = (x.clamp(-1, 1) + 1) / 2\n",
    "                predicted_next_frame = (predicted_next_frame * 255).type(torch.uint8)\n",
    "                prev_images = []\n",
    "\n",
    "                for i in range(num_prev_frames):\n",
    "                    frame = real_prev_frames[0, (i * 3):(i + 1) * 3, :, :]\n",
    "                    frame = (frame.clamp(-1, 1) + 1) / 2\n",
    "                    frame = (frame * 255).type(torch.uint8)\n",
    "                    prev_images.append(transforms.ToPILImage()(frame))\n",
    "\n",
    "                current_tensor = (real_current_frame[0].clamp(-1, 1) + 1) / 2 * 255\n",
    "                current_image = transforms.ToPILImage()(current_tensor.type(torch.uint8)).convert(\"RGB\")\n",
    "                predicted_image = transforms.ToPILImage()(predicted_next_frame[0]).convert(\"RGB\")\n",
    "\n",
    "                total_width = (num_prev_frames + 2) * config.IMAGE_SIZE\n",
    "                max_height = config.IMAGE_SIZE\n",
    "                new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "                x_offset = 0\n",
    "                for image in prev_images:\n",
    "                    new_im.paste(image, (x_offset,0))\n",
    "                    x_offset += config.IMAGE_SIZE\n",
    "                new_im.paste(current_image, (x_offset, 0))\n",
    "                x_offset += config.IMAGE_SIZE\n",
    "                new_im.paste(predicted_image, (x_offset, 0))\n",
    "\n",
    "                new_im.save(os.path.join(sample_dir, f\"sample_epoch_{epoch+1}.png\"))\n",
    "                print(f\"Saved sample image at epoch {epoch+1}\")\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Step {step}:\")\n",
    "            print(f\"  Mem Allocated: {torch.cuda.memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "            print(f\"  Max Mem Allocated: {torch.cuda.max_memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "            print(f\"  Mem Reserved: {torch.cuda.memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "            print(f\"  Max Mem Reserved: {torch.cuda.max_memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % plot_every == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            formatted_time = str(datetime.timedelta(seconds=elapsed_time))\n",
    "    \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "            # --- Plot 1: Loss from START_EPOCH of this run ---\n",
    "            # X-axis: Absolute epoch numbers (start_epoch + 1 up to current epoch + 1)\n",
    "            # Y-axis: Losses collected *in this run* (all_losses indices 0 up to current)\n",
    "            current_run_epochs_plotted = range(start_epoch + 1, epoch + 2)\n",
    "            axes[0].plot(current_run_epochs_plotted, all_losses)\n",
    "            axes[0].set_xlabel(\"Epoch\")\n",
    "            axes[0].set_ylabel(\"Loss\")\n",
    "            axes[0].set_title(f\"Loss Since Start (Epoch {start_epoch+1}, Time: {formatted_time})\")\n",
    "            axes[0].grid(True)\n",
    "    \n",
    "            # --- Plot 2: Loss since last plot ---\n",
    "            # X-axis: Absolute epoch numbers for the segment\n",
    "            x_values_ax1 = range(last_plot_epoch + 2, epoch + 2)\n",
    "    \n",
    "            # Y-axis: Slice all_losses using indices relative to this run's start\n",
    "            # Calculate indices corresponding to the absolute epoch numbers\n",
    "            start_slice_index = (last_plot_epoch + 1) - start_epoch # Index in all_losses for epoch last_plot_epoch+1\n",
    "            end_slice_index = (epoch + 1) - start_epoch           # Index in all_losses for epoch epoch+1 (exclusive)\n",
    "            y_values_ax1 = all_losses[start_slice_index : end_slice_index]\n",
    "    \n",
    "            if x_values_ax1 and y_values_ax1:\n",
    "                if len(x_values_ax1) != len(y_values_ax1):\n",
    "                     # This check should ideally not be needed with correct logic, but good safeguard\n",
    "                     print(f\"!!! ERROR: Mismatch detected plotting axes[1]: len(x)={len(x_values_ax1)}, len(y)={len(y_values_ax1)}\")\n",
    "                else:\n",
    "                    axes[1].plot(x_values_ax1, y_values_ax1)\n",
    "                    axes[1].set_xlabel(\"Epoch\")\n",
    "                    axes[1].set_ylabel(\"Loss\")\n",
    "                    axes[1].set_title(f\"Loss Since Epoch {last_plot_epoch + 1}\")\n",
    "                    axes[1].grid(True)\n",
    "            else:\n",
    "                 axes[1].set_title(f\"Loss Since Epoch {last_plot_epoch + 1} (No new data)\")\n",
    "                 axes[1].grid(True)\n",
    "    \n",
    "    \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_dir, f\"loss_plot_epoch_{epoch+1}.png\"))\n",
    "            plt.close()\n",
    "            print(f\"Epoch {epoch+1}: Avg Loss = {avg_epoch_loss:.6f}, Moving Avg = {current_moving_avg:.6f}, Time = {formatted_time}\")\n",
    "    \n",
    "            last_plot_epoch = epoch # Update absolute last plot epoch index\n",
    "\n",
    "        # --- Early Stopping (Dynamic Threshold) and Best Model Saving/Deletion---\n",
    "        if early_stopping_patience is not None and epoch + 1 > min_epochs:\n",
    "            should_stop = False\n",
    "            improvement_calculated = False\n",
    "            calculated_improvement = None # Define outside the inner ifs\n",
    "\n",
    "            if current_moving_avg < best_loss:\n",
    "                best_loss = current_moving_avg\n",
    "                best_epoch = epoch + 1\n",
    "                epochs_without_improvement = 0\n",
    "\n",
    "                # Save the *best* model\n",
    "                new_best_model_path = os.path.join(checkpoint_dir, f\"model_best_epoch_{best_epoch}.pth\")\n",
    "                torch.save({\n",
    "                    'epoch': best_epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_loss, # Save the best moving average loss\n",
    "                }, new_best_model_path)\n",
    "                print(f\"Saved best model at epoch {best_epoch} (Moving Avg Loss: {best_loss:.6f})\")\n",
    "\n",
    "                # Delete the *previous* best model\n",
    "                if previous_best_model_path and os.path.exists(previous_best_model_path):\n",
    "                    try: # Add try-except for deletion\n",
    "                        os.remove(previous_best_model_path)\n",
    "                        print(f\"Deleted previous best model: {previous_best_model_path}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"Warning: Could not delete previous best model '{previous_best_model_path}': {e}\")\n",
    "                previous_best_model_path = new_best_model_path\n",
    "\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "                # Check if patience is exceeded\n",
    "                if epochs_without_improvement >= early_stopping_patience:\n",
    "                    # Try to calculate improvement percentage ONLY if patience is met\n",
    "                    if len(moving_avg_losses) >= moving_avg_window: # Use >= for window check\n",
    "                        # Calculate previous average using the window *before* the non-improvement streak started\n",
    "                        # Ensure indices are valid\n",
    "                        if len(all_losses) > epochs_without_improvement:\n",
    "                            # Index of the loss just before the non-improvement streak started\n",
    "                            comparison_idx = len(all_losses) - epochs_without_improvement - 1\n",
    "                            # Average of the window ending at that point\n",
    "                            start_comparison_window = max(0, comparison_idx - moving_avg_window + 1)\n",
    "                            prev_window_losses = all_losses[start_comparison_window : comparison_idx + 1]\n",
    "\n",
    "                            if prev_window_losses:\n",
    "                                prev_moving_avg = sum(prev_window_losses) / len(prev_window_losses)\n",
    "                                if prev_moving_avg > 1e-9: # Check for non-zero denominator\n",
    "                                    calculated_improvement = (prev_moving_avg - current_moving_avg) / prev_moving_avg * 100\n",
    "                                    improvement_calculated = True\n",
    "                                    print(f\"  Epochs w/o improvement: {epochs_without_improvement}, Current MA: {current_moving_avg:.6f}, Prev MA: {prev_moving_avg:.6f}, Improvement: {calculated_improvement:.2f}%\")\n",
    "                                else:\n",
    "                                    print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Previous MA near zero.\")\n",
    "                            else:\n",
    "                                print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Not enough history for prev MA.\")\n",
    "                        else:\n",
    "                            print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Not enough history for prev MA.\")\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Waiting for full moving avg window to check percentage.\")\n",
    "\n",
    "                    # Decide whether to stop based on calculated improvement (if available)\n",
    "                    # Stop if patience is met AND (improvement wasn't calculated OR improvement is too small)\n",
    "                    if not improvement_calculated or (improvement_calculated and calculated_improvement < early_stopping_percentage):\n",
    "                         should_stop = True\n",
    "                         stop_reason = f\"Improvement {calculated_improvement:.2f}% < {early_stopping_percentage}%\" if improvement_calculated else \"Patience reached, improvement could not be reliably calculated.\"\n",
    "                         print(f\"Early stopping triggered at epoch {epoch+1}. Reason: {stop_reason}\")\n",
    "\n",
    "\n",
    "            # Break the loop *outside* the nested conditions if stop flag is set\n",
    "            if should_stop:\n",
    "                break\n",
    "                \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    formatted_time = str(datetime.timedelta(seconds=total_time))\n",
    "    print(f\"Total training time: {formatted_time}\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_epoch_loss,\n",
    "    }, os.path.join(checkpoint_dir, \"model_last.pth\"))\n",
    "    print(f\"Saved last model at epoch {epoch+1} with loss {avg_epoch_loss}\")\n",
    "\n",
    "    return all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356d5ca4-d8af-471b-afce-8023111e6f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n",
      "Total rows in CSV: 23081, Valid sequence start indices: 23037\n",
      "Loaded existing dataset split.\n",
      "Loaded checkpoint from epoch 58\n",
      "--- Training Configuration ---\n",
      "Model Parameters: 34,956,835\n",
      "  Mem Allocated: 559.88 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "--------------------------\n",
      "Found previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_58.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d60cb72500490b9c762bff4b6672c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 59/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 59\n",
      "Epoch 59, Step 4607:\n",
      "  Mem Allocated: 562.88 MB\n",
      "  Max Mem Allocated: 1208.07 MB\n",
      "  Mem Reserved: 1380.00 MB\n",
      "  Max Mem Reserved: 1380.00 MB\n",
      "Saved best model at epoch 59 (Moving Avg Loss: 0.002325)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_58.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1217b6bf8ea14c3e84d3ac1a5cc03b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 60/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 60\n",
      "Epoch 60, Step 4607:\n",
      "  Mem Allocated: 562.88 MB\n",
      "  Max Mem Allocated: 1209.67 MB\n",
      "  Mem Reserved: 1380.00 MB\n",
      "  Max Mem Reserved: 1380.00 MB\n",
      "Epoch 60: Avg Loss = 0.002542, Moving Avg = 0.002434, Time = 0:12:38.633682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb791da4472746e2bff549c23a803699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 61/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 61\n",
      "Epoch 61, Step 4607:\n",
      "  Mem Allocated: 562.88 MB\n",
      "  Max Mem Allocated: 1209.67 MB\n",
      "  Mem Reserved: 1380.00 MB\n",
      "  Max Mem Reserved: 1380.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1299d88d191f42d9852becbc81185c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 62/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 62\n",
      "Epoch 62, Step 4607:\n",
      "  Mem Allocated: 562.88 MB\n",
      "  Max Mem Allocated: 1209.67 MB\n",
      "  Mem Reserved: 1380.00 MB\n",
      "  Max Mem Reserved: 1380.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71266a9a9c59424ab404fb66b8ae8465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 63/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 63\n",
      "Epoch 63, Step 4607:\n",
      "  Mem Allocated: 562.88 MB\n",
      "  Max Mem Allocated: 1209.67 MB\n",
      "  Mem Reserved: 1380.00 MB\n",
      "  Max Mem Reserved: 1380.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c781d047ff4cd899b736a4d45a16ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 64/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 64\n",
      "Epoch 64, Step 4607:\n",
      "  Mem Allocated: 562.88 MB\n",
      "  Max Mem Allocated: 1209.67 MB\n",
      "  Mem Reserved: 1380.00 MB\n",
      "  Max Mem Reserved: 1380.00 MB\n",
      "  Epochs w/o improvement: 5. Waiting for full moving avg window to check percentage.\n",
      "Early stopping triggered at epoch 64. Reason: Patience reached, improvement could not be reliably calculated.\n",
      "Total training time: 0:38:04.120099\n",
      "Saved last model at epoch 64 with loss 0.0027634814869909657\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Data Transforms ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # --- Create Dataset and DataLoader ---\n",
    "    dataset = JetbotDataset(config.CSV_PATH, config.DATA_DIR, config.IMAGE_SIZE, config.NUM_PREV_FRAMES, transform=transform)\n",
    "    \n",
    "    # Try to load existing split\n",
    "    train_dataset, test_dataset = load_train_test_split(dataset, config.SPLIT_DATASET_FILENAME)\n",
    "    \n",
    "    if train_dataset is None or test_dataset is None:\n",
    "        print(\"Dataset split file not found, creating a new split...\")\n",
    "        train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "        save_existing_split(train_dataset, test_dataset, config.SPLIT_DATASET_FILENAME)\n",
    "    else:\n",
    "        print(\"Loaded existing dataset split.\")\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False) # Batch size 1 for easier evaluation\n",
    "    \n",
    "    # --- Calculate Betas and Alphas ---\n",
    "    betas = linear_beta_schedule(config.NUM_TIMESTEPS, config.BETA_START, config.BETA_END).to(config.DEVICE)\n",
    "    #betas = cosine_beta_schedule(NUM_TIMESTEPS).to(DEVICE) # Alternative\n",
    "    \n",
    "    alphas = (1. - betas).to(config.DEVICE)\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0).to(config.DEVICE)\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0).to(config.DEVICE)\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(config.DEVICE)\n",
    "    \n",
    "    # --- Create Model and Optimizer ---\n",
    "    model = SimpleUNet(image_channels=3, time_emb_dim=32, num_prev_frames=config.NUM_PREV_FRAMES).to(config.DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    # --- Load Checkpoint (if available) ---\n",
    "    if config.LOAD_CHECKPOINT:\n",
    "        checkpoint = torch.load(config.LOAD_CHECKPOINT)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        START_EPOCH = checkpoint['epoch']\n",
    "        print(f\"Loaded checkpoint from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        START_EPOCH = 0\n",
    "\n",
    "    print(f\"--- Training Configuration ---\")\n",
    "    # Print model parameter count\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print(f\"  Mem Allocated: {torch.cuda.memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Max Mem Allocated: {torch.cuda.max_memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Mem Reserved: {torch.cuda.memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Max Mem Reserved: {torch.cuda.max_memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")    \n",
    "    print(f\"--------------------------\")    \n",
    "    # --- Train the Model ---\n",
    "    losses = train(model, train_dataloader, optimizer, betas, alphas_cumprod, START_EPOCH, config.NUM_EPOCHS, config.DEVICE,\n",
    "                   config.SAVE_MODEL_EVERY, config.SAMPLE_EVERY, config.CHECKPOINT_DIR, config.SAMPLE_DIR, config.PLOT_DIR, config.PLOT_EVERY, config.USE_FP16,\n",
    "                   config.ACCUMULATION_STEPS, config.NUM_PREV_FRAMES, early_stopping_patience=config.EARLY_STOPPING_PATIENCE, early_stopping_percentage=config.EARLY_STOPPING_PERCENTAGE, min_epochs=config.MIN_EPOCHS)\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # --- Final Loss Plot ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(config.PLOT_DIR, \"loss_plot_final.png\"))  # Save to plot dir\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16933dd4-020b-40b6-9c67-b420f3ed3ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.SAMPLE_EVERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69705aac-316a-4771-aa0f-ccab7b3d9eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d410bf-97c6-44f0-a38c-4e378b0659ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
