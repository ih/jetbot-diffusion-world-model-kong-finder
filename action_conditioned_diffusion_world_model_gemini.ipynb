{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b40e0f-8c29-4a90-9e82-5995ef5a0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import config\n",
    "from torch.utils.data import random_split\n",
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from jetbot_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e89e9ec-9f79-4a17-800e-243b0864cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diffusion Helpers ---\n",
    "def linear_beta_schedule(timesteps, beta_start, beta_end):\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, betas, alphas_cumprod, device=\"cpu\"):\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(torch.sqrt(alphas_cumprod), t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        torch.sqrt(1. - alphas_cumprod), t, x_0.shape\n",
    "    )\n",
    "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise, noise\n",
    "\n",
    "# --- U-Net Model ---\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # Ensure output matches the embedding dim even if input dim is odd\n",
    "        if self.dim % 2 == 1:\n",
    "             embeddings = F.pad(embeddings, (0, 1))\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        # Adjusted time_mlp input dimension\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            # Adjusted Conv2d input channels for concatenation\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t_emb): # Modified to accept pre-computed embedding\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding - Project and add\n",
    "        time_emb_proj = self.relu(self.time_mlp(t_emb))\n",
    "        time_emb_proj = time_emb_proj[(..., ) + (None, ) * 2] # Reshape for spatial broadcast\n",
    "        h = h + time_emb_proj # Add time embedding\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "        \n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, image_channels=3, time_emb_dim=32, num_prev_frames=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Increased Channel Depth and Added Layer ---\n",
    "        down_channels = (128, 256, 512, 512) # Increased channels, added a level\n",
    "        up_channels = (512, 512, 256, 128)   # Increased channels, added a level\n",
    "\n",
    "        # Input channels = current frame + previous frames\n",
    "        in_img_channels = image_channels * (num_prev_frames + 1)\n",
    "        action_dim = 1 # Single motor action\n",
    "\n",
    "        # Effective embedding dimension including action\n",
    "        effective_time_emb_dim = time_emb_dim + action_dim\n",
    "\n",
    "        # Time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # --- Model Layers ---\n",
    "        self.conv0 = nn.Conv2d(in_img_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(down_channels)-1):\n",
    "            self.downs.append(Block(down_channels[i], down_channels[i+1], effective_time_emb_dim))\n",
    "\n",
    "        # Bottleneck (implicitly defined by channel changes)\n",
    "        # No extra bottleneck block needed here, just the transition\n",
    "\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in range(len(up_channels)-1):\n",
    "            self.ups.append(Block(up_channels[i], up_channels[i+1], effective_time_emb_dim, up=True))\n",
    "\n",
    "        # Final output layer (outputs noise prediction, same channels as original image)\n",
    "        self.output = nn.Conv2d(up_channels[-1], image_channels, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, timestep, action, prev_frames):\n",
    "        # x: noisy next frame (batch, 3, H, W)\n",
    "        # timestep: (batch,)\n",
    "        # action: (batch, 1)\n",
    "        # prev_frames: (batch, num_prev_frames * 3, H, W)\n",
    "\n",
    "        # Concatenate the current noisy image with the previous frames\n",
    "        x = torch.cat([x, prev_frames], dim=1) # Shape: (batch, (N+1)*C, H, W)\n",
    "\n",
    "        # --- Prepare Time and Action Embedding ---\n",
    "        t_emb = self.time_mlp(timestep) # Shape: (batch, time_emb_dim)\n",
    "        if action is not None:\n",
    "            action = action.float()\n",
    "            if len(action.shape) == 1:\n",
    "                 action = action.unsqueeze(1) # Ensure shape is (batch, 1)\n",
    "            # Concatenate time embedding and action embedding\n",
    "            t_action_emb = torch.cat([t_emb, action], dim=1) # Shape: (batch, time_emb_dim + 1)\n",
    "        else:\n",
    "            # Handle cases where action might be None (e.g., unconditional generation if needed later)\n",
    "            # Pad action dimensions if needed - adjust padding based on your effective_time_emb_dim\n",
    "            padding = torch.zeros(t_emb.shape[0], 1, device=t_emb.device)\n",
    "            t_action_emb = torch.cat([t_emb, padding], dim=1)\n",
    "        # --- End Embedding Prep ---\n",
    "\n",
    "        # --- U-Net Architecture ---\n",
    "        x = self.conv0(x) # Initial processing of combined input\n",
    "        residual_inputs = []\n",
    "        # Downsampling path\n",
    "        for i, down_block in enumerate(self.downs):\n",
    "            x = down_block(x, t_action_emb) # Pass combined embedding to blocks\n",
    "            residual_inputs.append(x)\n",
    "\n",
    "        # Upsampling path\n",
    "        for i, up_block in enumerate(self.ups):\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1) # Concatenate skip connection\n",
    "            x = up_block(x, t_action_emb) # Pass combined embedding to blocks\n",
    "\n",
    "        return self.output(x) # Predict noise\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train(model, dataloader, optimizer, betas, alphas_cumprod, start_epoch, num_epochs,\n",
    "          device, save_every, sample_every, checkpoint_dir, sample_dir, plot_dir,\n",
    "          plot_every, use_fp16, accumulation_steps, num_prev_frames,\n",
    "          early_stopping_patience, early_stopping_percentage, min_epochs):\n",
    "    \"\"\"\n",
    "    Trains the diffusion model with early stopping and best model saving/deletion.\n",
    "    \"\"\"\n",
    "\n",
    "    all_losses = []\n",
    "    start_time = time.time()\n",
    "    last_plot_epoch = start_epoch - 1\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = start_epoch\n",
    "    epochs_without_improvement = 0\n",
    "    moving_avg_window = 10\n",
    "    moving_avg_losses = []\n",
    "    previous_best_model_path = None  # Keep track of the previous best model's path\n",
    "\n",
    "    # --- Load previous best model path if resuming ---\n",
    "    # Find the highest epoch 'best' model if resuming, to delete the correct one later\n",
    "    if start_epoch > 0:\n",
    "        try:\n",
    "            existing_best = [f for f in os.listdir(checkpoint_dir) if f.startswith('model_best_epoch_')]\n",
    "            if existing_best:\n",
    "                existing_best.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]), reverse=True)\n",
    "                previous_best_model_path = os.path.join(checkpoint_dir, existing_best[0])\n",
    "                # Extract best_loss from the loaded best model checkpoint if desired\n",
    "                best_checkpoint = torch.load(previous_best_model_path, map_location='cpu')\n",
    "                best_loss = best_checkpoint.get('loss', float('inf')) # Restore best loss\n",
    "                print(f\"Found previous best model: {previous_best_model_path}\") # Verify best loss restoration if needed\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not determine previous best model path: {e}\")\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for step, (images, actions, prev_frames) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            actions = actions.to(device)\n",
    "            prev_frames = prev_frames.to(device)\n",
    "            t = torch.randint(0, config.NUM_TIMESTEPS, (images.shape[0],), device=device).long()\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_fp16):\n",
    "                x_noisy, noise = forward_diffusion_sample(images, t, betas, alphas_cumprod, device)\n",
    "                predicted_noise = model(x_noisy, t, actions, prev_frames)\n",
    "                loss = F.mse_loss(noise, predicted_noise)\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_losses.append(loss.item() * accumulation_steps)\n",
    "            pbar.set_postfix({\"Loss\": loss.item() * accumulation_steps})\n",
    "\n",
    "        if optimizer.param_groups[0]['params'][0].grad is not None:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)  if epoch_losses else float('nan') # Handle empty epoch_losses\n",
    "        if np.isnan(avg_epoch_loss):\n",
    "            print(f\"Warning: NaN loss detected for epoch {epoch+1}. Skipping update/plot.\")\n",
    "            # Optionally: break or handle NaN case differently\n",
    "            continue\n",
    "        all_losses.append(avg_epoch_loss)\n",
    "\n",
    "        moving_avg_losses.append(avg_epoch_loss)\n",
    "        if len(moving_avg_losses) > moving_avg_window:\n",
    "            moving_avg_losses.pop(0)\n",
    "        current_moving_avg = sum(moving_avg_losses) / len(moving_avg_losses)\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_epoch_loss,\n",
    "            }, os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\"))\n",
    "            print(f\"Saved model checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "        if (epoch + 1) % sample_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                random_idx = torch.randint(0, len(dataset), (1,)).item()\n",
    "                real_current_frame, action, real_prev_frames = dataset[random_idx]\n",
    "                real_current_frame = real_current_frame.unsqueeze(0).to(device)\n",
    "                real_prev_frames = real_prev_frames.unsqueeze(0).to(device)\n",
    "                action = action.to(device)\n",
    "\n",
    "                t_sample = torch.tensor([config.NUM_TIMESTEPS - 1], device=device, dtype=torch.long)\n",
    "                x_noisy, _ = forward_diffusion_sample(real_current_frame, t_sample, betas, alphas_cumprod, device)\n",
    "                x = x_noisy\n",
    "\n",
    "                for i in reversed(range(1, config.NUM_TIMESTEPS)):\n",
    "                    t = (torch.ones(1) * i).long().to(device)\n",
    "                    with torch.cuda.amp.autocast(enabled=use_fp16):\n",
    "                        predicted_noise = model(x, t, action, real_prev_frames)\n",
    "\n",
    "                    alpha = alphas[t][:, None, None, None]\n",
    "                    alpha_hat = alphas_cumprod[t][:, None, None, None]\n",
    "                    beta = betas[t][:, None, None, None]\n",
    "\n",
    "                    if i > 1:\n",
    "                        noise = torch.randn_like(x)\n",
    "                    else:\n",
    "                        noise = torch.zeros_like(x)\n",
    "                    x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "                predicted_next_frame = (x.clamp(-1, 1) + 1) / 2\n",
    "                predicted_next_frame = (predicted_next_frame * 255).type(torch.uint8)\n",
    "                prev_images = []\n",
    "\n",
    "                for i in range(num_prev_frames):\n",
    "                    frame = real_prev_frames[0, (i * 3):(i + 1) * 3, :, :]\n",
    "                    frame = (frame.clamp(-1, 1) + 1) / 2\n",
    "                    frame = (frame * 255).type(torch.uint8)\n",
    "                    prev_images.append(transforms.ToPILImage()(frame))\n",
    "\n",
    "                current_tensor = (real_current_frame[0].clamp(-1, 1) + 1) / 2 * 255\n",
    "                current_image = transforms.ToPILImage()(current_tensor.type(torch.uint8)).convert(\"RGB\")\n",
    "                predicted_image = transforms.ToPILImage()(predicted_next_frame[0]).convert(\"RGB\")\n",
    "\n",
    "                total_width = (num_prev_frames + 2) * config.IMAGE_SIZE\n",
    "                max_height = config.IMAGE_SIZE\n",
    "                new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "                x_offset = 0\n",
    "                for image in prev_images:\n",
    "                    new_im.paste(image, (x_offset,0))\n",
    "                    x_offset += config.IMAGE_SIZE\n",
    "                new_im.paste(current_image, (x_offset, 0))\n",
    "                x_offset += config.IMAGE_SIZE\n",
    "                new_im.paste(predicted_image, (x_offset, 0))\n",
    "\n",
    "                new_im.save(os.path.join(sample_dir, f\"sample_epoch_{epoch+1}.png\"))\n",
    "                print(f\"Saved sample image at epoch {epoch+1}\")\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Step {step}:\")\n",
    "            print(f\"  Mem Allocated: {torch.cuda.memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "            print(f\"  Max Mem Allocated: {torch.cuda.max_memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "            print(f\"  Mem Reserved: {torch.cuda.memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "            print(f\"  Max Mem Reserved: {torch.cuda.max_memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % plot_every == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            formatted_time = str(datetime.timedelta(seconds=elapsed_time))\n",
    "    \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "            # --- Plot 1: Loss from START_EPOCH of this run ---\n",
    "            # X-axis: Absolute epoch numbers (start_epoch + 1 up to current epoch + 1)\n",
    "            # Y-axis: Losses collected *in this run* (all_losses indices 0 up to current)\n",
    "            current_run_epochs_plotted = range(start_epoch + 1, epoch + 2)\n",
    "            axes[0].plot(current_run_epochs_plotted, all_losses)\n",
    "            axes[0].set_xlabel(\"Epoch\")\n",
    "            axes[0].set_ylabel(\"Loss\")\n",
    "            axes[0].set_title(f\"Loss Since Start (Epoch {start_epoch+1}, Time: {formatted_time})\")\n",
    "            axes[0].grid(True)\n",
    "    \n",
    "            # --- Plot 2: Loss since last plot ---\n",
    "            # X-axis: Absolute epoch numbers for the segment\n",
    "            x_values_ax1 = range(last_plot_epoch + 2, epoch + 2)\n",
    "    \n",
    "            # Y-axis: Slice all_losses using indices relative to this run's start\n",
    "            # Calculate indices corresponding to the absolute epoch numbers\n",
    "            start_slice_index = (last_plot_epoch + 1) - start_epoch # Index in all_losses for epoch last_plot_epoch+1\n",
    "            end_slice_index = (epoch + 1) - start_epoch           # Index in all_losses for epoch epoch+1 (exclusive)\n",
    "            y_values_ax1 = all_losses[start_slice_index : end_slice_index]\n",
    "    \n",
    "            if x_values_ax1 and y_values_ax1:\n",
    "                if len(x_values_ax1) != len(y_values_ax1):\n",
    "                     # This check should ideally not be needed with correct logic, but good safeguard\n",
    "                     print(f\"!!! ERROR: Mismatch detected plotting axes[1]: len(x)={len(x_values_ax1)}, len(y)={len(y_values_ax1)}\")\n",
    "                else:\n",
    "                    axes[1].plot(x_values_ax1, y_values_ax1)\n",
    "                    axes[1].set_xlabel(\"Epoch\")\n",
    "                    axes[1].set_ylabel(\"Loss\")\n",
    "                    axes[1].set_title(f\"Loss Since Epoch {last_plot_epoch + 1}\")\n",
    "                    axes[1].grid(True)\n",
    "            else:\n",
    "                 axes[1].set_title(f\"Loss Since Epoch {last_plot_epoch + 1} (No new data)\")\n",
    "                 axes[1].grid(True)\n",
    "    \n",
    "    \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plot_dir, f\"loss_plot_epoch_{epoch+1}.png\"))\n",
    "            plt.close()\n",
    "            print(f\"Epoch {epoch+1}: Avg Loss = {avg_epoch_loss:.6f}, Moving Avg = {current_moving_avg:.6f}, Time = {formatted_time}\")\n",
    "    \n",
    "            last_plot_epoch = epoch # Update absolute last plot epoch index\n",
    "\n",
    "        # --- Early Stopping (Dynamic Threshold) and Best Model Saving/Deletion---\n",
    "        if early_stopping_patience is not None and epoch + 1 > min_epochs:\n",
    "            should_stop = False\n",
    "            improvement_calculated = False\n",
    "            calculated_improvement = None # Define outside the inner ifs\n",
    "\n",
    "            if current_moving_avg < best_loss:\n",
    "                best_loss = current_moving_avg\n",
    "                best_epoch = epoch + 1\n",
    "                epochs_without_improvement = 0\n",
    "\n",
    "                # Save the *best* model\n",
    "                new_best_model_path = os.path.join(checkpoint_dir, f\"model_best_epoch_{best_epoch}.pth\")\n",
    "                torch.save({\n",
    "                    'epoch': best_epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_loss, # Save the best moving average loss\n",
    "                }, new_best_model_path)\n",
    "                print(f\"Saved best model at epoch {best_epoch} (Moving Avg Loss: {best_loss:.6f})\")\n",
    "\n",
    "                # Delete the *previous* best model\n",
    "                if previous_best_model_path and os.path.exists(previous_best_model_path):\n",
    "                    try: # Add try-except for deletion\n",
    "                        os.remove(previous_best_model_path)\n",
    "                        print(f\"Deleted previous best model: {previous_best_model_path}\")\n",
    "                    except OSError as e:\n",
    "                        print(f\"Warning: Could not delete previous best model '{previous_best_model_path}': {e}\")\n",
    "                previous_best_model_path = new_best_model_path\n",
    "\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "                # Check if patience is exceeded\n",
    "                if epochs_without_improvement >= early_stopping_patience:\n",
    "                    # Try to calculate improvement percentage ONLY if patience is met\n",
    "                    if len(moving_avg_losses) >= moving_avg_window: # Use >= for window check\n",
    "                        # Calculate previous average using the window *before* the non-improvement streak started\n",
    "                        # Ensure indices are valid\n",
    "                        if len(all_losses) > epochs_without_improvement:\n",
    "                            # Index of the loss just before the non-improvement streak started\n",
    "                            comparison_idx = len(all_losses) - epochs_without_improvement - 1\n",
    "                            # Average of the window ending at that point\n",
    "                            start_comparison_window = max(0, comparison_idx - moving_avg_window + 1)\n",
    "                            prev_window_losses = all_losses[start_comparison_window : comparison_idx + 1]\n",
    "\n",
    "                            if prev_window_losses:\n",
    "                                prev_moving_avg = sum(prev_window_losses) / len(prev_window_losses)\n",
    "                                if prev_moving_avg > 1e-9: # Check for non-zero denominator\n",
    "                                    calculated_improvement = (prev_moving_avg - current_moving_avg) / prev_moving_avg * 100\n",
    "                                    improvement_calculated = True\n",
    "                                    print(f\"  Epochs w/o improvement: {epochs_without_improvement}, Current MA: {current_moving_avg:.6f}, Prev MA: {prev_moving_avg:.6f}, Improvement: {calculated_improvement:.2f}%\")\n",
    "                                else:\n",
    "                                    print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Previous MA near zero.\")\n",
    "                            else:\n",
    "                                print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Not enough history for prev MA.\")\n",
    "                        else:\n",
    "                            print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Not enough history for prev MA.\")\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(f\"  Epochs w/o improvement: {epochs_without_improvement}. Waiting for full moving avg window to check percentage.\")\n",
    "\n",
    "                    # Decide whether to stop based on calculated improvement (if available)\n",
    "                    # Stop if patience is met AND (improvement wasn't calculated OR improvement is too small)\n",
    "                    if not improvement_calculated or (improvement_calculated and calculated_improvement < early_stopping_percentage):\n",
    "                         should_stop = True\n",
    "                         stop_reason = f\"Improvement {calculated_improvement:.2f}% < {early_stopping_percentage}%\" if improvement_calculated else \"Patience reached, improvement could not be reliably calculated.\"\n",
    "                         print(f\"Early stopping triggered at epoch {epoch+1}. Reason: {stop_reason}\")\n",
    "\n",
    "\n",
    "            # Break the loop *outside* the nested conditions if stop flag is set\n",
    "            if should_stop:\n",
    "                break\n",
    "                \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    formatted_time = str(datetime.timedelta(seconds=total_time))\n",
    "    print(f\"Total training time: {formatted_time}\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_epoch_loss,\n",
    "    }, os.path.join(checkpoint_dir, \"model_last.pth\"))\n",
    "    print(f\"Saved last model at epoch {epoch+1} with loss {avg_epoch_loss}\")\n",
    "\n",
    "    return all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356d5ca4-d8af-471b-afce-8023111e6f15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n",
      "Total rows in CSV: 23081, Valid sequence start indices: 23037\n",
      "Loaded existing dataset split.\n",
      "Loaded checkpoint from epoch 32\n",
      "--- Training Configuration ---\n",
      "Model Parameters: 34,956,835\n",
      "  Mem Allocated: 537.50 MB\n",
      "  Max Mem Allocated: 537.50 MB\n",
      "  Mem Reserved: 554.00 MB\n",
      "  Max Mem Reserved: 554.00 MB\n",
      "--------------------------\n",
      "Found previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_32.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d487d29e461b48b79ddfde8a191ad241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 33/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 33\n",
      "Epoch 33, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1201.32 MB\n",
      "  Mem Reserved: 1250.00 MB\n",
      "  Max Mem Reserved: 1250.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec54356fb424b018172c0a1bb210486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 34/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 34\n",
      "Epoch 34, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643dc249a71840c880e4ca75e624bce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 35/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 35\n",
      "Epoch 35, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363d746eb4fb43ee90f76641d60b359b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 36/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 36\n",
      "Epoch 36, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9155162644a94472b086919cd1db69fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 37/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 37\n",
      "Epoch 37, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 37 (Moving Avg Loss: 0.002840)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_32.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31813dc6ad3e4290b41087b229989fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 38/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 38\n",
      "Epoch 38, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 38 (Moving Avg Loss: 0.002820)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_37.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dda0a69010d49189444d07c323d78b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 39/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 39\n",
      "Epoch 39, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 39 (Moving Avg Loss: 0.002817)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_38.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9051c529f00b43e9b9a300c2aa73896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 40/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 40\n",
      "Epoch 40, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Epoch 40: Avg Loss = 0.002720, Moving Avg = 0.002805, Time = 0:50:08.460164\n",
      "Saved best model at epoch 40 (Moving Avg Loss: 0.002805)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_39.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76e1bc208a64dbe9e830f514bca4759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 41/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 41\n",
      "Epoch 41, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 41 (Moving Avg Loss: 0.002804)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_40.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716fcd8d0d4a4f52baf2e1183023bf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 42/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 42\n",
      "Epoch 42, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 42 (Moving Avg Loss: 0.002791)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_41.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe11fba0cdb4d7896bdbcbc33b655be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 43/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 43\n",
      "Epoch 43, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 43 (Moving Avg Loss: 0.002756)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_42.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb7b85f26d74d3598f76476e371e971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 44/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 44\n",
      "Epoch 44, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4354bf4b61cd4246a7439763d9a7b411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 45/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 45\n",
      "Epoch 45, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 45 (Moving Avg Loss: 0.002750)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_43.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17864cbe7644e63a8d7effa9f678203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 46/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 46\n",
      "Epoch 46, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 46 (Moving Avg Loss: 0.002739)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_45.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8781e981307437b88d6f0be50949879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 47/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 47\n",
      "Epoch 47, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 47 (Moving Avg Loss: 0.002725)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_46.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf50956bd89456fbef609836a6c1288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 48/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 48\n",
      "Epoch 48, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 48 (Moving Avg Loss: 0.002703)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_47.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8d6511adec445988fcb66e9e700656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 49/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 49\n",
      "Epoch 49, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 49 (Moving Avg Loss: 0.002676)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_48.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c6bb5506944075b46664954a485f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 50/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 50\n",
      "Epoch 50, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Epoch 50: Avg Loss = 0.002595, Moving Avg = 0.002664, Time = 1:52:47.542474\n",
      "Saved best model at epoch 50 (Moving Avg Loss: 0.002664)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_49.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c3f86bb7ce49208aea92913096366d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 51/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 51\n",
      "Epoch 51, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 51 (Moving Avg Loss: 0.002654)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_50.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d008a9746b3428da5ff760e75143121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 52/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 52\n",
      "Epoch 52, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 52 (Moving Avg Loss: 0.002643)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_51.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daf4adad5b44cfe914140bd96840192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 53/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 53\n",
      "Epoch 53, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 53 (Moving Avg Loss: 0.002626)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_52.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a36522ed754e54a90c2c3d645cebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 54/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 54\n",
      "Epoch 54, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 54 (Moving Avg Loss: 0.002603)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_53.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec8450c4fca4570bbd9528de333aebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 55/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 55\n",
      "Epoch 55, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 55 (Moving Avg Loss: 0.002587)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_54.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb0ee746457443e8aba17b4a2d79af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 56/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 56\n",
      "Epoch 56, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 56 (Moving Avg Loss: 0.002581)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_55.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ed88f45dc740cf86067fcf6e464b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 57/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 57\n",
      "Epoch 57, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 57 (Moving Avg Loss: 0.002570)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_56.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21785e593a6490fb2d0d3f2dba63deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 58/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image at epoch 58\n",
      "Epoch 58, Step 4607:\n",
      "  Mem Allocated: 556.75 MB\n",
      "  Max Mem Allocated: 1202.92 MB\n",
      "  Mem Reserved: 1252.00 MB\n",
      "  Max Mem Reserved: 1252.00 MB\n",
      "Saved best model at epoch 58 (Moving Avg Loss: 0.002570)\n",
      "Deleted previous best model: output_two_action_model_v2_128image\\checkpoints\\model_best_epoch_57.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b6263e696445739c2bee15745f9345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 59/1000:   0%|          | 0/4608 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# --- Train the Model ---\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphas_cumprod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m               \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAVE_MODEL_EVERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAMPLE_EVERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAMPLE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPLOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPLOT_EVERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUSE_FP16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m               \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACCUMULATION_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_PREV_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEARLY_STOPPING_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEARLY_STOPPING_PERCENTAGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMIN_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# --- Final Loss Plot ---\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 193\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, betas, alphas_cumprod, start_epoch, num_epochs, device, save_every, sample_every, checkpoint_dir, sample_dir, plot_dir, plot_every, use_fp16, accumulation_steps, num_prev_frames, early_stopping_patience, early_stopping_percentage, min_epochs)\u001b[0m\n\u001b[0;32m    190\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    191\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 193\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_frames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mC:\\Projects\\jetbot-diffusion-world-model-kong-finder\\jetbot_dataset.ipynb:101\u001b[0m, in \u001b[0;36mJetbotDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Construct path relative to the main data_dir\u001b[39;00m\n\u001b[0;32m    100\u001b[0m current_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, current_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 101\u001b[0m current_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_image_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m current_action \u001b[38;5;241m=\u001b[39m current_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    104\u001b[0m prev_frames \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mC:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\PIL\\Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Data Transforms ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # --- Create Dataset and DataLoader ---\n",
    "    dataset = JetbotDataset(config.CSV_PATH, config.DATA_DIR, config.IMAGE_SIZE, config.NUM_PREV_FRAMES, transform=transform)\n",
    "    \n",
    "    # Try to load existing split\n",
    "    train_dataset, test_dataset = load_train_test_split(dataset, config.SPLIT_DATASET_FILENAME)\n",
    "    \n",
    "    if train_dataset is None or test_dataset is None:\n",
    "        print(\"Dataset split file not found, creating a new split...\")\n",
    "        train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "        save_existing_split(train_dataset, test_dataset, config.SPLIT_DATASET_FILENAME)\n",
    "    else:\n",
    "        print(\"Loaded existing dataset split.\")\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False) # Batch size 1 for easier evaluation\n",
    "    \n",
    "    # --- Calculate Betas and Alphas ---\n",
    "    betas = linear_beta_schedule(config.NUM_TIMESTEPS, config.BETA_START, config.BETA_END).to(config.DEVICE)\n",
    "    #betas = cosine_beta_schedule(NUM_TIMESTEPS).to(DEVICE) # Alternative\n",
    "    \n",
    "    alphas = (1. - betas).to(config.DEVICE)\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0).to(config.DEVICE)\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0).to(config.DEVICE)\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas).to(config.DEVICE)\n",
    "    \n",
    "    # --- Create Model and Optimizer ---\n",
    "    model = SimpleUNet(image_channels=3, time_emb_dim=32, num_prev_frames=config.NUM_PREV_FRAMES).to(config.DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    # --- Load Checkpoint (if available) ---\n",
    "    if config.LOAD_CHECKPOINT:\n",
    "        checkpoint = torch.load(config.LOAD_CHECKPOINT)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        START_EPOCH = checkpoint['epoch']\n",
    "        print(f\"Loaded checkpoint from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        START_EPOCH = 0\n",
    "\n",
    "    print(f\"--- Training Configuration ---\")\n",
    "    # Print model parameter count\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    print(f\"  Mem Allocated: {torch.cuda.memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Max Mem Allocated: {torch.cuda.max_memory_allocated(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Mem Reserved: {torch.cuda.memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")\n",
    "    print(f\"  Max Mem Reserved: {torch.cuda.max_memory_reserved(config.DEVICE) / 1024**2:.2f} MB\")    \n",
    "    print(f\"--------------------------\")    \n",
    "    # --- Train the Model ---\n",
    "    losses = train(model, train_dataloader, optimizer, betas, alphas_cumprod, START_EPOCH, config.NUM_EPOCHS, config.DEVICE,\n",
    "                   config.SAVE_MODEL_EVERY, config.SAMPLE_EVERY, config.CHECKPOINT_DIR, config.SAMPLE_DIR, config.PLOT_DIR, config.PLOT_EVERY, config.USE_FP16,\n",
    "                   config.ACCUMULATION_STEPS, config.NUM_PREV_FRAMES, early_stopping_patience=config.EARLY_STOPPING_PATIENCE, early_stopping_percentage=config.EARLY_STOPPING_PERCENTAGE, min_epochs=config.MIN_EPOCHS)\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # --- Final Loss Plot ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(config.PLOT_DIR, \"loss_plot_final.png\"))  # Save to plot dir\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16933dd4-020b-40b6-9c67-b420f3ed3ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.SAMPLE_EVERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69705aac-316a-4771-aa0f-ccab7b3d9eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d410bf-97c6-44f0-a38c-4e378b0659ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
