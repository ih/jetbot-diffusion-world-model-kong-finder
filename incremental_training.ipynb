{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "72d5ec15",
      "metadata": {},
      "source": [
        "# Incremental Training with Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b840f8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from importnb import Notebook\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
        "import random\n",
        "from diamond_world_model_trainer import train_diamond_model\n",
        "import pickle\n",
        "\n",
        "import config\n",
        "with Notebook():\n",
        "    from jetbot_dataset import JetbotDataset\n",
        "    from combine_session_data import combine_sessions_append, gather_new_sessions_only\n",
        "    from compare_diamond_models import load_sampler, evaluate_models_alternating\n",
        "\n",
        "import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5f5312",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer(Dataset):\n",
        "    \"\"\"A simple replay buffer storing dataset indices.\"\"\"\n",
        "    def __init__(self, dataset, max_size=50000, index_path=None):\n",
        "        self.dataset = dataset\n",
        "        self.max_size = max_size\n",
        "        self.index_path = index_path\n",
        "        if index_path and os.path.exists(index_path):\n",
        "            with open(index_path, 'rb') as f:\n",
        "                self.indices = pickle.load(f)\n",
        "        else:\n",
        "            self.indices = list(range(len(dataset)))[:max_size]\n",
        "            if index_path:\n",
        "                with open(index_path, 'wb') as f:\n",
        "                    pickle.dump(self.indices, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[self.indices[idx]]\n",
        "\n",
        "    def sample(self, k):\n",
        "        idxs = random.sample(self.indices, min(k, len(self.indices)))\n",
        "        return [self.dataset[i] for i in idxs]\n",
        "\n",
        "    def add_episode(self, new_idx):\n",
        "        self.indices = list(new_idx) + self.indices\n",
        "        self.indices = self.indices[:self.max_size]\n",
        "        if self.index_path:\n",
        "            with open(self.index_path, 'wb') as f:\n",
        "                pickle.dump(self.indices, f)\n",
        "\n",
        "class MixedDataset(IterableDataset):\n",
        "    def __init__(self, fresh_ds, replay_buffer, alpha=0.2):\n",
        "        self.fresh_ds = fresh_ds\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if random.random() < self.alpha and len(self.fresh_ds) > 0:\n",
        "                idx = random.randint(0, len(self.fresh_ds) - 1)\n",
        "                yield self.fresh_ds[idx]\n",
        "            else:\n",
        "                yield self.replay_buffer.sample(1)[0]\n",
        "\n",
        "\n",
        "def build_batch(samples):\n",
        "    \"Collate function building a models.Batch from dataset samples.\"\n",
        "    imgs, acts, prevs = zip(*samples)\n",
        "    imgs  = torch.stack(imgs, 0)\n",
        "    acts  = torch.stack(acts, 0)\n",
        "    prevs = torch.stack(prevs, 0)\n",
        "    b        = len(samples)\n",
        "    num_prev = config.NUM_PREV_FRAMES\n",
        "    c, h, w  = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
        "    prev_seq = prevs.view(b, num_prev, c, h, w)\n",
        "    obs      = torch.cat((prev_seq, imgs.unsqueeze(1)), dim=1)\n",
        "    act_seq  = acts.repeat(1, num_prev).long()\n",
        "    mask     = torch.ones(b, num_prev + 1, dtype=torch.bool, device=imgs.device)\n",
        "    return models.Batch(obs=obs, act=act_seq, mask_padding=mask, info=[{}] * b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89e0423",
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "\\n",
        "    gather_new_sessions_only(\n",
        "        config.SESSION_DATA_DIR,\n",
        "        config.CSV_PATH,\n",
        "        config.NEW_IMAGE_DIR,\n",
        "        config.NEW_CSV_PATH,\n",
        "    )\n",
        "    fresh_ds = JetbotDataset(\\n",
        "        config.NEW_CSV_PATH,\\n",
        "        config.NEW_DATA_DIR,\\n",
        "        config.IMAGE_SIZE,\\n",
        "        config.NUM_PREV_FRAMES,\\n",
        "        transform=config.TRANSFORM,\\n",
        "    ) if os.path.exists(config.NEW_CSV_PATH) else []\\n",
        "\\n",
        "    full_ds = JetbotDataset(\\n",
        "        config.CSV_PATH,\\n",
        "        config.DATA_DIR,\\n",
        "        config.IMAGE_SIZE,\\n",
        "        config.NUM_PREV_FRAMES,\\n",
        "        transform=config.TRANSFORM,\\n",
        "    )\\n",
        "    replay_ds = ReplayBuffer(full_ds, max_size=50000, index_path=config.REPLAY_INDEX_PATH)\\n",
        "\\n",
        "    mixed_dataset = MixedDataset(fresh_ds, replay_ds, alpha=0.2)\\n",
        "    train_loader = DataLoader(\\n",
        "        mixed_dataset,\\n",
        "        batch_size=config.BATCH_SIZE,\\n",
        "        collate_fn=build_batch,\\n",
        "        num_workers=4,\\n",
        "        pin_memory=True,\\n",
        "        drop_last=True,\\n",
        "    )\\n",
        "\\n",
        "    val_dataset = JetbotDataset(\\n",
        "        config.HOLDOUT_CSV_PATH,\\n",
        "        config.HOLDOUT_DATA_DIR,\\n",
        "        config.IMAGE_SIZE,\\n",
        "        config.NUM_PREV_FRAMES,\\n",
        "        transform=config.TRANSFORM,\\n",
        "    )\\n",
        "    val_loader = DataLoader(\\n",
        "        val_dataset,\\n",
        "        batch_size=config.BATCH_SIZE,\\n",
        "        shuffle=False,\\n",
        "        collate_fn=build_batch,\\n",
        "        num_workers=4,\\n",
        "        pin_memory=True,\\n",
        "    )\\n",
        "\\n",
        "    # Step 2: train a new model starting from the last best checkpoint\\n",
        "    ckpt_path = os.path.join(config.CHECKPOINT_DIR, 'denoiser_model_best_val_loss.pth')\\n",
        "    new_ckpt = train_diamond_model(\\n",
        "        train_loader,\\n",
        "        val_loader,\\n",
        "        start_checkpoint=ckpt_path,\\n",
        "        max_steps=config.NUM_TRAIN_STEPS,\\n",
        "    )\\n",
        "\\n",
        "    # Step 3: compare old best with the newly trained checkpoint\\n",
        "    if os.path.exists(ckpt_path):\\n",
        "        sampler_a = load_sampler(ckpt_path, config.DEVICE)\\n",
        "        sampler_b = load_sampler(new_ckpt, config.DEVICE)\\n",
        "        dataset_holdout = JetbotDataset(\\n",
        "            config.HOLDOUT_CSV_PATH,\\n",
        "            config.HOLDOUT_DATA_DIR,\\n",
        "            config.IMAGE_SIZE,\\n",
        "            config.NUM_PREV_FRAMES,\\n",
        "            transform=config.TRANSFORM,\\n",
        "        )\\n",
        "        dl_holdout = DataLoader(dataset_holdout, batch_size=1, shuffle=False)\\n",
        "        results = evaluate_models_alternating(\\n",
        "            sampler_a, sampler_b, dl_holdout, config.DEVICE, config.NUM_PREV_FRAMES\\n",
        "        )\\n",
        "        if results['B']['avg_mse'] < results['A']['avg_mse']:\\n",
        "            os.replace(new_ckpt, ckpt_path)\\n",
        "        else:\\n",
        "            os.remove(new_ckpt)\\n",
        "    else:\\n",
        "        os.replace(new_ckpt, ckpt_path)\\n",
        "\\n",
        "    # After training, permanently add new sessions to the full dataset\\n",
        "    old_len = len(full_ds)\\n",
        "    combine_sessions_append(config.SESSION_DATA_DIR, config.IMAGE_DIR, config.CSV_PATH)\\n",
        "    updated_ds = JetbotDataset(config.CSV_PATH, config.DATA_DIR, config.IMAGE_SIZE, config.NUM_PREV_FRAMES, transform=config.TRANSFORM)\\n",
        "    new_indices = range(old_len, len(updated_ds))\\n",
        "    replay_ds.dataset = updated_ds\\n",
        "    replay_ds.add_episode(new_indices)\\n",
        "\\n",
        "if __name__ == '__main__':\\n",
        "    main()\\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
