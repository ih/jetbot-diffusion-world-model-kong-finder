{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee5d92",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b39856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\\n",
    "from importnb import Notebook\\n",
    "import torch\\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, Subset\\n",
    "import random\\n",
    "import pickle\\n",
    "import numpy as np\\n",
    "import wandb\\n",
    "import time\\n",
    "import datetime\\n",
    "import logging\\n",
    "logging.basicConfig(level=logging.INFO)\\n",
    "logger = logging.getLogger(\\"IncrementalTraining\\")\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\\n",
    "with Notebook():\\n",
    "    from jetbot_dataset import JetbotDataset\\n",
    "    from combine_session_data import combine_sessions_append, gather_new_sessions_only\\n",
    "    from compare_diamond_models import load_sampler, evaluate_models_alternating\\n",
    "from diamond_world_model_trainer import train_diamond_model, split_dataset, log_gpu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70959a96",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "max-holdout",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HOLDOUT = 250\\n",
    "EVAL_SEED = 42\\n",
    "EPS_MSE  = 0.995   # ≥ 0.5 % relative MSE improvement\\n",
    "EPS_SSIM = 0.002   # ≥ 0.002 absolute SSIM gain (~0.2 %)\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6f6af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\\n",
    "import numpy as np\\n",
    "\\n",
    "class MixedMapDataset(torch.utils.data.Dataset):\\n",
    "    \"\"\"\\n",
    "    A high-performance map-style dataset that replaces the slow IterableDataset.\\n",
    "\\n",
    "    It pre-calculates a mixed-and-shuffled index mapping from a \\"fresh\\"\\n",
    "    and a \\"replay\\" dataset based on a desired mixing ratio. This allows\\n",
    "    the PyTorch DataLoader to use its optimized, batched fetching strategy.\\n",
    "    \"\"\"\\n",
    "    def __init__(self, fresh_ds, replay_ds, virtual_epoch_size, fresh_ratio=0.2):\\n",
    "        super().__init__()\\n",
    "        self.fresh_ds = fresh_ds\\n",
    "        self.replay_ds = replay_ds\\n",
    "        self.virtual_epoch_size = virtual_epoch_size\\n",
    "\\n",
    "        num_fresh = int(virtual_epoch_size * fresh_ratio)\\n",
    "\\n",
    "        # If there is no fresh data, all samples will come from the replay buffer\\n",
    "        if not self.fresh_ds or len(self.fresh_ds) == 0:\\n",
    "            num_fresh = 0\\n",
    "        \\n",
    "        num_replay = virtual_epoch_size - num_fresh\\n",
    "\\n",
    "        # Create pointers to samples: (is_fresh_bool, index_in_source_dataset)\\n",
    "        # We sample with replacement to fill the desired virtual epoch size\\n",
    "        fresh_pointers = []\\n",
    "        if num_fresh > 0:\\n",
    "            fresh_indices = np.random.choice(len(self.fresh_ds), size=num_fresh, replace=True)\\n",
    "            fresh_pointers = [(True, idx) for idx in fresh_indices]\\n",
    "\\n",
    "        replay_indices = np.random.choice(len(self.replay_ds), size=num_replay, replace=True)\\n",
    "        replay_pointers = [(False, idx) for idx in replay_indices]\\n",
    "\\n",
    "        # Combine and shuffle the pointers to mix the data\\n",
    "        self.mapping = fresh_pointers + replay_pointers\\n",
    "        np.random.shuffle(self.mapping)\\n",
    "\\n",
    "    def __len__(self):\\n",
    "        return self.virtual_epoch_size\\n",
    "\\n",
    "    def __getitem__(self, idx):\\n",
    "        is_fresh, source_idx = self.mapping[idx]\\n",
    "        if is_fresh:\\n",
    "            return self.fresh_ds[source_idx]\\n",
    "        else:\\n",
    "            return self.replay_ds[source_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeea1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(samples):\\n",
    "    \"\"\"Collate function building a ``models.Batch`` from dataset samples.\"\"\"\\n",
    "    imgs, acts, prevs = zip(*samples)\\n",
    "    imgs  = torch.stack(imgs, 0)\\n",
    "    acts  = torch.stack(acts, 0)\\n",
    "    prevs = torch.stack(prevs, 0)\\n",
    "\\n",
    "    b        = len(samples)\\n",
    "    num_prev = config.NUM_PREV_FRAMES\\n",
    "    c, h, w  = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\\n",
    "    prev_seq = prevs.view(b, num_prev, c, h, w)\\n",
    "    obs      = torch.cat((prev_seq, imgs.unsqueeze(1)), dim=1)\\n",
    "    act_seq  = acts.repeat(1, num_prev).long()\\n",
    "    mask     = torch.ones(b, num_prev + 1, dtype=torch.bool, device=imgs.device)\\n",
    "    return models.Batch(obs=obs, act=act_seq, mask_padding=mask, info=[{}] * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff659086",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\\n",
    "    # Step 1: gather only new sessions into a temporary dataset\\n",
    "    gather_new_sessions_only(\\n",
    "        config.SESSION_DATA_DIR,\\n",
    "        config.CSV_PATH,\\n",
    "        config.NEW_IMAGE_DIR,\\n",
    "        config.NEW_CSV_PATH,\\n",
    "    )\\n",
    "\\n",
    "    fresh_ds = JetbotDataset(\\n",
    "        config.NEW_CSV_PATH,\\n",
    "        config.NEW_DATA_DIR,\\n",
    "        config.IMAGE_SIZE,\\n",
    "        config.NUM_PREV_FRAMES,\\n",
    "        transform=config.TRANSFORM,\\n",
    "    ) if os.path.exists(config.NEW_CSV_PATH) else []\\n",
    "\\n",
    "    print(f\\"Fresh dataset size is {len(fresh_ds)}.\\")\\n",
    "\\n",
    "    # --- Step 2: Set up datasets (MODIFIED) ---\\n",
    "    print(f\\"Fresh dataset size is {len(fresh_ds)}.\\")\\n",
    "    train_ds, val_ds = split_dataset()\\n",
    "    \\n",
    "    # The original replay buffer can be replaced by the training split directly\\n",
    "    replay_ds = train_ds \\n",
    "\\n",
    "    # === START: MODIFICATION TO FIX SLOWDOWN ===\\n",
    "    \\n",
    "    # 1. Define a \\"virtual epoch size\\" based on your training logic\\n",
    "    # This determines how many samples are loaded before the dataloader shuffles again.\\n",
    "    # A larger number is fine; 20,000 is a reasonable default.\\n",
    "    VIRTUAL_EPOCH_SIZE = 20000\\n",
    "\\n",
    "    # 2. Create the new high-performance dataset\\n",
    "    mixed_dataset = MixedMapDataset(\\n",
    "        fresh_ds=fresh_ds,\\n",
    "        replay_ds=replay_ds,\\n",
    "        virtual_epoch_size=VIRTUAL_EPOCH_SIZE,\\n",
    "        fresh_ratio=config.MIX_ALPHA  # Use MIX_ALPHA from your config\\n",
    "    )\\n",
    "    \\n",
    "    # 3. Create the DataLoader. It now uses shuffle=True because it's a map-style dataset.\\n",
    "    train_loader = DataLoader(\\n",
    "        mixed_dataset,\\n",
    "        batch_size=config.BATCH_SIZE,\\n",
    "        collate_fn=build_batch,\\n",
    "        num_workers=4,\\n",
    "        pin_memory=False,\\n",
    "        shuffle=True,  # This is crucial for map-style datasets\\n",
    "        drop_last=True,\\n",
    "    )\\n",
    "\\n",
    "    # === END: MODIFICATION TO FIX SLOWDOWN ===\\n",
    "    \\n",
    "    val_loader = DataLoader(\\n",
    "        val_ds,\\n",
    "        batch_size=config.BATCH_SIZE,\\n",
    "        num_workers=4,\\n",
    "        shuffle=False,\\n",
    "        collate_fn=build_batch,\\n",
    "        pin_memory=False,\\n",
    "    )\\n",
    "\\n",
    "    # --- Step 3: Train the model (no change here) ---\\n",
    "    new_ckpt = train_diamond_model(\\n",
    "        train_loader,\\n",
    "        val_loader,\\n",
    "        len(fresh_ds),\\n",
    "        start_checkpoint=os.path.join(config.CHECKPOINT_DIR, 'denoiser_model_best_val_loss.pth')\\n",
    "    )\\n",
    "    print(\\"Training Complete\\")\\n",
    "\\n",
    "    # Step 3: compare old best with the newly trained checkpoint\\n",
    "    if os.path.exists(ckpt_path):\\n",
    "        sampler_a = load_sampler(ckpt_path, config.DEVICE)\\n",
    "        sampler_b = load_sampler(new_ckpt, config.DEVICE)\\n",
    "        dataset_holdout = JetbotDataset(\\n",
    "            config.HOLDOUT_CSV_PATH,\\n",
    "            config.HOLDOUT_DATA_DIR,\\n",
    "            config.IMAGE_SIZE,\\n",
    "            config.NUM_PREV_FRAMES,\\n",
    "            transform=config.TRANSFORM,\\n",
    "        )\\n",
    "        if MAX_HOLDOUT and MAX_HOLDOUT < len(dataset_holdout):\\n",
    "            rng = np.random.RandomState(EVAL_SEED)\\n",
    "            subset_idx = rng.choice(len(dataset_holdout), size=MAX_HOLDOUT, replace=False)\\n",
    "            dataset_holdout = Subset(dataset_holdout, subset_idx.tolist())\\n",
    "        dl_holdout = DataLoader(dataset_holdout, batch_size=1, shuffle=False)\\n",
    "        results = evaluate_models_alternating(\\n",
    "            sampler_a, sampler_b, dl_holdout, config.DEVICE, config.NUM_PREV_FRAMES\\n",
    "        )\\n",
    "        promoted = False\\n",
    "        incumbent_stats = {\\n",
    "            'mse': results['A']['avg_mse'],\\n",
    "            'ssim': results['A']['avg_ssim'],\\n",
    "        }\\n",
    "        candidate_stats = {\\n",
    "            'mse': results['B']['avg_mse'],\\n",
    "            'ssim': results['B']['avg_ssim'],\\n",
    "        }\\n",
    "        mse_better = candidate_stats['mse'] < incumbent_stats['mse'] * EPS_MSE\\n",
    "        ssim_better = candidate_stats['ssim'] > incumbent_stats['ssim'] + EPS_SSIM\\n",
    "\\n",
    "        if mse_better and ssim_better:\\n",
    "            os.replace(new_ckpt, ckpt_path)\\n",
    "            promoted = True\\n",
    "            logger.info(f\\"✅ Promoted new model: MSE {incumbent_stats['mse']:.4f} → {candidate_stats['mse']:.4f}, SSIM {incumbent_stats['ssim']:.4f} → {candidate_stats['ssim']:.4f}\\")\\n",
    "        else:\\n",
    "            os.remove(new_ckpt)\\n",
    "            promoted = False\\n",
    "            logger.info(f\\"🛑 Rejected: ΔMSE={candidate_stats['mse'] - incumbent_stats['mse']:.4e}, ΔSSIM={candidate_stats['ssim'] - incumbent_stats['ssim']:.4e}\\")\\n",
    "    else:\\n",
    "        os.replace(new_ckpt, ckpt_path)\\n",
    "        promoted = True\\n",
    "\\n",
    "    # After training, permanently add new sessions to the full dataset\\n",
    "    # The ReplayBuffer's dataset (`train_ds`) is a Subset. To correctly update \\n",
    "    # the underlying full dataset and add new indices, we need to handle this carefully.\\n",
    "    # For now, we'll assume that `combine_sessions_append` updates the source from which `split_dataset` reads.\\n",
    "    # A more robust solution might involve updating the `full_dataset` object used by `split_dataset` \\n",
    "    # and then re-splitting, or carefully managing indices if `train_ds` is a subset of a global dataset.\\n",
    "\\n",
    "    # Assuming `split_dataset` will pick up new data on next run after `combine_sessions_append`\\n",
    "    # The current `replay_ds.dataset` (which is `train_ds`) will not reflect these new sessions until the script is rerun\\n",
    "    # and `split_dataset` is called again. This behavior is kept as is for now.\\n",
    "    # To add *newly collected* data (from `fresh_ds`) to the replay buffer for the *current* training run, that's handled by `MixedDataset`.\\n",
    "    # The logic below primarily concerns adding to the *persistent* dataset for future runs.\\n",
    "\\n",
    "    # Ensure all new data (including fresh_ds from this run) is combined into the main persistent dataset.\\n",
    "    # This makes it available for the next execution of incremental_training.ipynb, \\n",
    "    # where split_dataset will create new train/val splits from the complete data.\\n",
    "    if promoted:\\n",
    "        print(\\"Combining all session data into the main dataset for future runs...\\")\\n",
    "        combine_sessions_append(config.SESSION_DATA_DIR, config.IMAGE_DIR, config.CSV_PATH)\\n",
    "        print(\\"Session data combined.\\")\\n",
    "\\n",
    "        # Delete the dataset split file to ensure a fresh split on the next run\\n",
    "        split_file_path = os.path.join(config.OUTPUT_DIR, getattr(config, 'SPLIT_DATASET_FILENAME', 'dataset_split.pth'))\\n",
    "        if os.path.exists(split_file_path):\\n",
    "            try:\\n",
    "                os.remove(split_file_path)\\n",
    "                print(f\\"Deleted dataset split file: {split_file_path}\\")\\n",
    "            except OSError as e:\\n",
    "                print(f\\"Error deleting dataset split file {split_file_path}: {e}\\")\\n",
    "    else:\\n",
    "        print(\\"Skipping session combine since model was not promoted.\\")\\n",
    "\\n",
    "    # No need to update the current run's replay_ds instance further, as it's ephemeral and will be rebuilt on the next run.\\n",
    "\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\n",
    "if __name__ == '__main__':\\n",
    "    wandb.init()\\n",
    "    start_time = time.time()\\n",
    "    main()\\n",
    "    duration = time.time() - start_time\\n",
    "    formatted_duration = str(datetime.timedelta(seconds=duration))\\n",
    "    print(f\\"Incremental training took : {formatted_duration}\\")\\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf071f68-306c-4b69-ab4e-29a9133c8498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603b95e-0f60-45cb-ab29-c42ffc9e2929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
