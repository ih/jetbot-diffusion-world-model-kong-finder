{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee5d92",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b39856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importnb import Notebook\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset, Subset\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"IncrementalTraining\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018a46c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n",
      "Visualization helpers defined.\n",
      "Training and validation epoch functions adapted for Batch object and Denoiser.forward.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "with Notebook():\n",
    "    from jetbot_dataset import JetbotDataset\n",
    "    from combine_session_data import combine_sessions_append, gather_new_sessions_only\n",
    "    from compare_diamond_models import load_sampler, evaluate_models_alternating\n",
    "from diamond_world_model_trainer import train_diamond_model, split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70959a96",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "max-holdout",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HOLDOUT = 250\n",
    "EVAL_SEED = 42\n",
    "EPS_MSE  = 0.995   # ≥ 0.5 % relative MSE improvement\n",
    "EPS_SSIM = 0.002   # ≥ 0.002 absolute SSIM gain (~0.2 %)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b6f6af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(Dataset):\n",
    "    \"\"\"A simple replay buffer storing dataset indices in memory.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, max_size=50000):\n",
    "        self.dataset = dataset\n",
    "        self.max_size = max_size\n",
    "        # Initialize indices in memory. If dataset is large, take a random subset or the latest `max_size` items.\n",
    "        # For simplicity, taking the first `max_size` indices, assuming newer data is appended.\n",
    "        # If dataset can be shorter than max_size, list slicing handles it.\n",
    "        if len(dataset) > max_size:\n",
    "            # If you want to prioritize newest data (assuming it's at the end of `dataset` after updates):\n",
    "            # self.indices = list(range(len(dataset) - max_size, len(dataset)))\n",
    "            # Or, for random sampling from a large dataset initially:\n",
    "            # self.indices = random.sample(range(len(dataset)), max_size)\n",
    "            # Current: take the first max_size, new data added to front via add_episode\n",
    "            self.indices = list(range(max_size)) \n",
    "        else:\n",
    "            self.indices = list(range(len(dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def sample(self, k):\n",
    "        idxs = random.sample(self.indices, min(k, len(self.indices)))\n",
    "        return [self.dataset[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79f37ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MixedDataset(IterableDataset):\n",
    "    \"\"\"Yields samples from fresh data with probability ``alpha`` and from the\n",
    "    replay buffer otherwise.\"\"\"\n",
    "\n",
    "    def __init__(self, fresh_ds, replay_buffer, alpha=0.2):\n",
    "        self.fresh_ds = fresh_ds\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if random.random() < self.alpha and len(self.fresh_ds) > 0:\n",
    "                idx = random.randint(0, len(self.fresh_ds) - 1)\n",
    "                yield self.fresh_ds[idx]\n",
    "            else:\n",
    "                yield self.replay_buffer.sample(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aeea1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(samples):\n",
    "    \"\"\"Collate function building a ``models.Batch`` from dataset samples.\"\"\"\n",
    "    imgs, acts, prevs = zip(*samples)\n",
    "    imgs  = torch.stack(imgs, 0)\n",
    "    acts  = torch.stack(acts, 0)\n",
    "    prevs = torch.stack(prevs, 0)\n",
    "\n",
    "    b        = len(samples)\n",
    "    num_prev = config.NUM_PREV_FRAMES\n",
    "    c, h, w  = config.DM_IMG_CHANNELS, config.IMAGE_SIZE, config.IMAGE_SIZE\n",
    "    prev_seq = prevs.view(b, num_prev, c, h, w)\n",
    "    obs      = torch.cat((prev_seq, imgs.unsqueeze(1)), dim=1)\n",
    "    act_seq  = acts.repeat(1, num_prev).long()\n",
    "    mask     = torch.ones(b, num_prev + 1, dtype=torch.bool, device=imgs.device)\n",
    "    return models.Batch(obs=obs, act=act_seq, mask_padding=mask, info=[{}] * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff659086",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step 1: gather only new sessions into a temporary dataset\n",
    "    gather_new_sessions_only(\n",
    "        config.SESSION_DATA_DIR,\n",
    "        config.CSV_PATH,\n",
    "        config.NEW_IMAGE_DIR,\n",
    "        config.NEW_CSV_PATH,\n",
    "    )\n",
    "\n",
    "    fresh_ds = JetbotDataset(\n",
    "        config.NEW_CSV_PATH,\n",
    "        config.NEW_DATA_DIR,\n",
    "        config.IMAGE_SIZE,\n",
    "        config.NUM_PREV_FRAMES,\n",
    "        transform=config.TRANSFORM,\n",
    "    ) if os.path.exists(config.NEW_CSV_PATH) else []\n",
    "\n",
    "    # Use split_dataset from diamond_world_model_trainer\n",
    "    train_ds, val_ds = split_dataset() # train_ds replaces full_ds, val_ds replaces val_dataset\n",
    "\n",
    "    replay_ds = ReplayBuffer(train_ds, max_size=50000) # Removed index_path\n",
    "\n",
    "    mixed_dataset = MixedDataset(fresh_ds, replay_ds, alpha=0.2)\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        collate_fn=build_batch,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, # Use val_ds from split_dataset\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=build_batch,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Step 2: train a new model starting from the last best checkpoint\n",
    "    ckpt_path = os.path.join(config.CHECKPOINT_DIR, 'denoiser_model_best_val_loss.pth')\n",
    "    print(\"Starting training\")\n",
    "    new_ckpt = train_diamond_model(\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        start_checkpoint=ckpt_path,\n",
    "        max_steps=config.NUM_TRAIN_STEPS,\n",
    "    )\n",
    "    print(\"Training Complete\")\n",
    "\n",
    "    # Step 3: compare old best with the newly trained checkpoint\n",
    "    if os.path.exists(ckpt_path):\n",
    "        sampler_a = load_sampler(ckpt_path, config.DEVICE)\n",
    "        sampler_b = load_sampler(new_ckpt, config.DEVICE)\n",
    "        dataset_holdout = JetbotDataset(\n",
    "            config.HOLDOUT_CSV_PATH,\n",
    "            config.HOLDOUT_DATA_DIR,\n",
    "            config.IMAGE_SIZE,\n",
    "            config.NUM_PREV_FRAMES,\n",
    "            transform=config.TRANSFORM,\n",
    "        )\n",
    "        if MAX_HOLDOUT and MAX_HOLDOUT < len(dataset_holdout):\n",
    "            rng = np.random.RandomState(EVAL_SEED)\n",
    "            subset_idx = rng.choice(len(dataset_holdout), size=MAX_HOLDOUT, replace=False)\n",
    "            dataset_holdout = Subset(dataset_holdout, subset_idx.tolist())\n",
    "        dl_holdout = DataLoader(dataset_holdout, batch_size=1, shuffle=False)\n",
    "        results = evaluate_models_alternating(\n",
    "            sampler_a, sampler_b, dl_holdout, config.DEVICE, config.NUM_PREV_FRAMES\n",
    "        )\n",
    "        promoted = False\n",
    "        incumbent_stats = {\n",
    "            'mse': results['A']['avg_mse'],\n",
    "            'ssim': results['A']['avg_ssim'],\n",
    "        }\n",
    "        candidate_stats = {\n",
    "            'mse': results['B']['avg_mse'],\n",
    "            'ssim': results['B']['avg_ssim'],\n",
    "        }\n",
    "        mse_better = candidate_stats['mse'] < incumbent_stats['mse'] * EPS_MSE\n",
    "        ssim_better = candidate_stats['ssim'] > incumbent_stats['ssim'] + EPS_SSIM\n",
    "\n",
    "        if mse_better and ssim_better:\n",
    "            os.replace(new_ckpt, ckpt_path)\n",
    "            promoted = True\n",
    "            logger.info(f\"✅ Promoted new model: MSE {incumbent_stats['mse']:.4f} → {candidate_stats['mse']:.4f}, SSIM {incumbent_stats['ssim']:.4f} → {candidate_stats['ssim']:.4f}\")\n",
    "        else:\n",
    "            os.remove(new_ckpt)\n",
    "            promoted = False\n",
    "            logger.info(f\"🛑 Rejected: ΔMSE={candidate_stats['mse'] - incumbent_stats['mse']:.4e}, ΔSSIM={candidate_stats['ssim'] - incumbent_stats['ssim']:.4e}\")\n",
    "    else:\n",
    "        os.replace(new_ckpt, ckpt_path)\n",
    "        promoted = True\n",
    "\n",
    "    # After training, permanently add new sessions to the full dataset\n",
    "    # The ReplayBuffer's dataset (`train_ds`) is a Subset. To correctly update \n",
    "    # the underlying full dataset and add new indices, we need to handle this carefully.\n",
    "    # For now, we'll assume that `combine_sessions_append` updates the source from which `split_dataset` reads.\n",
    "    # A more robust solution might involve updating the `full_dataset` object used by `split_dataset` \n",
    "    # and then re-splitting, or carefully managing indices if `train_ds` is a subset of a global dataset.\n",
    "\n",
    "    # Assuming `split_dataset` will pick up new data on next run after `combine_sessions_append`\n",
    "    # The current `replay_ds.dataset` (which is `train_ds`) will not reflect these new sessions until the script is rerun\n",
    "    # and `split_dataset` is called again. This behavior is kept as is for now.\n",
    "    # To add *newly collected* data (from `fresh_ds`) to the replay buffer for the *current* training run, that's handled by `MixedDataset`.\n",
    "    # The logic below primarily concerns adding to the *persistent* dataset for future runs.\n",
    "\n",
    "    # Ensure all new data (including fresh_ds from this run) is combined into the main persistent dataset.\n",
    "    # This makes it available for the next execution of incremental_training.ipynb, \n",
    "    # where split_dataset will create new train/val splits from the complete data.\n",
    "    if promoted:\n",
    "        print(\"Combining all session data into the main dataset for future runs...\")\n",
    "        combine_sessions_append(config.SESSION_DATA_DIR, config.IMAGE_DIR, config.CSV_PATH)\n",
    "        print(\"Session data combined.\")\n",
    "\n",
    "        # Delete the dataset split file to ensure a fresh split on the next run\n",
    "        split_file_path = os.path.join(config.OUTPUT_DIR, getattr(config, 'SPLIT_DATASET_FILENAME', 'dataset_split.pth'))\n",
    "        if os.path.exists(split_file_path):\n",
    "            try:\n",
    "                os.remove(split_file_path)\n",
    "                print(f\"Deleted dataset split file: {split_file_path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error deleting dataset split file {split_file_path}: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping session combine since model was not promoted.\")\n",
    "\n",
    "    # No need to update the current run's replay_ds instance further, as it's ephemeral and will be rebuilt on the next run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c9df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: irvin-hwang (irvin-hwang-simulacra-systems) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\jetbot-diffusion-world-model-kong-finder\\wandb\\run-20250703_131612-zekev45o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder/runs/zekev45o' target=\"_blank\">visionary-shape-20</a></strong> to <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder/runs/zekev45o' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder/runs/zekev45o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 new sessions to collect.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8077ed1d6434a61bb1dbe6b7ba17bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting New Sessions:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 8790 entries to C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\jetbot_new_data\\new.csv\n",
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n",
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n",
      "Full dataset size: 25180\n",
      "Loading dataset split from C:\\Projects\\jetbot-diffusion-world-model-kong-finder-aux\\output_model_5hz_DIAMOND_laundry_incremental\\dataset_split.pth\n",
      "Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1bc530ce804fe9993097b3c49dff14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Incremental Training Steps:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataset with 2518 samples for actions: [0.0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94058cf2f1d4376bc535ca6daf48115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering Dataset:   0%|          | 0/2518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered down to 1332 samples.\n",
      "Filtering dataset with 2518 samples for actions: [0.13]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3c301a0eaa438eae1ca5fc34893105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering Dataset:   0%|          | 0/2518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered down to 1186 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56aacc9d6f9f4bea8cf0b8486616820b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 100 [Valid]:   0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca7138b27d8459b8575b192248a785c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 200 [Valid]:   0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365a2484891b4555824c50948a76dbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 300 [Valid]:   0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59424c3473b4478aa8bd4edfce14269c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 400 [Valid]:   0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823ac7da193f4612affbc16e139a9b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 500 [Valid]:   0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be483fe8d37f4836807cf0322db56b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 600 [Valid]:   0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Early stopping triggered: No improvement in 400 steps.\n",
      "✅ Restoring model to best validation loss: 0.0238\n",
      "Training Complete\n",
      "Loaded combined CSV with columns: ['session_id', 'image_path', 'timestamp', 'action']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2929a029e66e40fbbe99b255c21b0115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\pythonenv-deeprl\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "INFO:IncrementalTraining:🛑 Rejected: ΔMSE=-4.2177e-04, ΔSSIM=8.0688e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping session combine since model was not promoted.\n",
      "Incremental training took : 0:55:04.605103\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>A/avg_mse</td><td>▅▆▆▄▇█▇▅▅▄█▆▆▆▆▆▅▅▄▄▄▄▃▃▃▄▄▄▄▄▃▃▃▃▂▂▂▁▁▂</td></tr><tr><td>A/avg_ssim</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>A/mse</td><td>▂▁▂▁▂▄▁▁▁█▃▁▁▁▄▃▁▁▄▂▁▂▂▁▄▂▂▁▁▁▁▄▁▂▁▂▁▆▁▁</td></tr><tr><td>A/ssim</td><td>▇▇▆▁▆▆▆▇▄▄▅█▇▃▆▆▅▇▆▆█▆▇▇▂▆▄▆▇▇▆▄▇█▇▇▁▇▂▆</td></tr><tr><td>B/avg_mse</td><td>▃▄▅▄█▅▆▆▆▆▇▇▅▅▅▃▃▃▄▄▄▄▄▄▄▄▄▄▃▃▂▂▂▂▂▁▁▂▂▂</td></tr><tr><td>B/avg_ssim</td><td>█▄▅▆▄▃▃▃▁▁▂▂▂▂▃▃▃▃▃▄▃▃▃▃▂▂▂▃▃▃▃▃▃▃▃▄▃▃▃▄</td></tr><tr><td>B/mse</td><td>▂▁▂▁▃▂▁▂▁▁▄▁▁▁█▃▁▁▃▄▁█▂▁▂▃▁▂▁▂▁▅▃▂▁▂▂▄▃▁</td></tr><tr><td>B/ssim</td><td>▇▇▆▇▃▃▆▇▇▅▇▅▂█▂█▆▁▇▇▇▇▄█▇▇▄▅▇▅███▇▇▆▇▆▆█</td></tr><tr><td>early_stop_step</td><td>▁</td></tr><tr><td>incremental_eval_val_loss</td><td>▁▁▃██▆</td></tr><tr><td>incremental_step_denoising_loss</td><td>▄▂█▁▄▅▃▄▄▃▂▃▄▂▃▁▃▃▂▃▂▃▅▆▆▂▄▆▅▃▅█▅▅▃▂▂▂▁▄</td></tr><tr><td>incremental_step_learning_rate</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>incremental_step_train_loss</td><td>▃▂▅█▁▃▃▂▃▁▃▁▂▂▁▂▂▁▂▂▂▂▃▂▂▂▃▁▂▃▃▃▂▃▂▂▁▁▂▃</td></tr><tr><td>sample_idx</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_batch_denoising_loss</td><td>▄▄▂▁▁▃▂▃▂▃▃▄▂▂▃▂▃▃▄▃▂▂▂█▃▃▂▂▂▅▄▂▄▅▃▅▃▃▂▂</td></tr><tr><td>val_batch_loss</td><td>▅▅▄▆▆▂▆▃▇▁▃▂▃▄▃▃▄▃▃█▅▆▁▄▆▄▂▄▅▃▃▄▅▅▁▄▂▇▅▄</td></tr><tr><td>val_step</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>A/avg_mse</td><td>0.00968</td></tr><tr><td>A/avg_ssim</td><td>0.86736</td></tr><tr><td>A/mse</td><td>0.00442</td></tr><tr><td>A/ssim</td><td>0.89324</td></tr><tr><td>B/avg_mse</td><td>0.00926</td></tr><tr><td>B/avg_ssim</td><td>0.86817</td></tr><tr><td>B/mse</td><td>0.00449</td></tr><tr><td>B/ssim</td><td>0.89408</td></tr><tr><td>early_stop_reason</td><td>patience_met</td></tr><tr><td>early_stop_step</td><td>600</td></tr><tr><td>incremental_eval_val_loss</td><td>0.02546</td></tr><tr><td>incremental_step_denoising_loss</td><td>0.03337</td></tr><tr><td>incremental_step_learning_rate</td><td>1e-05</td></tr><tr><td>incremental_step_train_loss</td><td>0.03337</td></tr><tr><td>sample_idx</td><td>249</td></tr><tr><td>train_step</td><td>7299</td></tr><tr><td>val_batch_denoising_loss</td><td>0.01413</td></tr><tr><td>val_batch_loss</td><td>0.01413</td></tr><tr><td>val_step</td><td>6295</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-shape-20</strong> at: <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder/runs/zekev45o' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder/runs/zekev45o</a><br> View project at: <a href='https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder' target=\"_blank\">https://wandb.ai/irvin-hwang-simulacra-systems/jetbot-diffusion-world-model-kong-finder</a><br>Synced 5 W&B file(s), 18 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250703_131612-zekev45o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    wandb.init()\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    duration = time.time() - start_time\n",
    "    formatted_duration = str(datetime.timedelta(seconds=duration))\n",
    "    print(f\"Incremental training took : {formatted_duration}\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf071f68-306c-4b69-ab4e-29a9133c8498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
