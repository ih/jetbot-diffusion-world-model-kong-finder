{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from importnb import Notebook\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import config\n",
        "import models\n",
        "with Notebook():\n",
        "    from jetbot_dataset import JetbotDataset\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Paths to the model checkpoints (update these as needed)\n",
        "MODEL_A_PATH = 'model_a.pth'\n",
        "MODEL_B_PATH = 'model_b.pth'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "dataset = JetbotDataset(config.HOLDOUT_CSV_PATH, config.HOLDOUT_DATA_DIR, config.IMAGE_SIZE, config.NUM_PREV_FRAMES, transform=config.TRANSFORM)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "print(f'Loaded holdout dataset with {len(dataset)} samples.')\n",
        "wandb.init(project=\"model_comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_sampler(checkpoint_path, device):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    inner_cfg = models.InnerModelConfig(\n",
        "        img_channels=config.DM_IMG_CHANNELS,\n",
        "        num_steps_conditioning=config.DM_NUM_STEPS_CONDITIONING,\n",
        "        cond_channels=config.DM_COND_CHANNELS,\n",
        "        depths=config.DM_UNET_DEPTHS,\n",
        "        channels=config.DM_UNET_CHANNELS,\n",
        "        attn_depths=config.DM_UNET_ATTN_DEPTHS,\n",
        "        num_actions=config.DM_NUM_ACTIONS,\n",
        "        is_upsampler=config.DM_IS_UPSAMPLER\n",
        "    )\n",
        "    denoiser_cfg = models.DenoiserConfig(\n",
        "        inner_model=inner_cfg,\n",
        "        sigma_data=config.DM_SIGMA_DATA,\n",
        "        sigma_offset_noise=config.DM_SIGMA_OFFSET_NOISE,\n",
        "        noise_previous_obs=config.DM_NOISE_PREVIOUS_OBS,\n",
        "        upsampling_factor=config.DM_UPSAMPLING_FACTOR\n",
        "    )\n",
        "    denoiser = models.Denoiser(cfg=denoiser_cfg).to(device)\n",
        "    denoiser.load_state_dict(checkpoint['model_state_dict'])\n",
        "    denoiser.eval()\n",
        "    sampler_cfg = models.DiffusionSamplerConfig(\n",
        "        num_steps_denoising=config.SAMPLER_NUM_STEPS,\n",
        "        sigma_min=config.SAMPLER_SIGMA_MIN,\n",
        "        sigma_max=config.SAMPLER_SIGMA_MAX,\n",
        "        rho=config.SAMPLER_RHO,\n",
        "        order=getattr(config, 'SAMPLER_ORDER', 1),\n",
        "        s_churn=getattr(config, 'SAMPLER_S_CHURN', 0.0)\n",
        "    )\n",
        "    return models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate_model_quantitative(sampler, dataloader, device, num_prev_frames, action_tolerance=1e-6):\n",
        "    sampler.denoiser.eval()\n",
        "    metrics={'overall':{'mse':[], 'ssim':[]}}\n",
        "    with torch.no_grad():\n",
        "        for current_img, action_tensor, prev_frames_tensor in tqdm(dataloader, desc='Evaluating'):\n",
        "            current_img=current_img.to(device)\n",
        "            action_tensor=action_tensor.to(device)\n",
        "            prev_frames_tensor=prev_frames_tensor.to(device)\n",
        "            prev_obs=prev_frames_tensor.view(1, num_prev_frames, current_img.shape[1], current_img.shape[2], current_img.shape[3])\n",
        "            prev_act=action_tensor.long().repeat(1, num_prev_frames)\n",
        "            pred,_=sampler.sample(prev_obs=prev_obs, prev_act=prev_act)\n",
        "            mse=F.mse_loss(pred, current_img).item()\n",
        "            pred_normalized=(pred.clamp(-1,1)+1)/2\n",
        "            gt_normalized=(current_img.clamp(-1,1)+1)/2\n",
        "            ssim_val=ssim(pred_normalized, gt_normalized, data_range=1.0).item()\n",
        "            metrics['overall']['mse'].append(mse)\n",
        "            metrics['overall']['ssim'].append(ssim_val)\n",
        "            key=0.0 if abs(action_tensor.item())<action_tolerance else float(action_tensor.item())\n",
        "            if key not in metrics:\n",
        "                metrics[key]={'mse':[], 'ssim':[]}\n",
        "            metrics[key]['mse'].append(mse)\n",
        "            metrics[key]['ssim'].append(ssim_val)\n",
        "    avg_metrics={}\n",
        "    for k,v in metrics.items():\n",
        "        if v['mse']:\n",
        "            avg_metrics[k]={\n",
        "                'avg_mse': float(np.mean(v['mse'])),\n",
        "                'avg_ssim': float(np.mean(v['ssim'])),\n",
        "                'count': len(v['mse'])\n",
        "            }\n",
        "        else:\n",
        "            avg_metrics[k]={'avg_mse': float('nan'), 'avg_ssim': float('nan'), 'count':0}\n",
        "    return avg_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate_models_alternating(sampler_a, sampler_b, dataloader, device, num_prev_frames):\n",
        "    metrics = {'A': {'mse': [], 'ssim': []}, 'B': {'mse': [], 'ssim': []}}\n",
        "    with torch.no_grad():\n",
        "        for idx, (current_img, action_tensor, prev_frames_tensor) in enumerate(tqdm(dataloader, desc='Evaluating')):\n",
        "            current_img = current_img.to(device)\n",
        "            action_tensor = action_tensor.to(device)\n",
        "            prev_frames_tensor = prev_frames_tensor.to(device)\n",
        "            prev_obs = prev_frames_tensor.view(1, num_prev_frames, current_img.shape[1], current_img.shape[2], current_img.shape[3])\n",
        "            prev_act = action_tensor.long().repeat(1, num_prev_frames)\n",
        "            pred_a, _ = sampler_a.sample(prev_obs=prev_obs, prev_act=prev_act)\n",
        "            pred_b, _ = sampler_b.sample(prev_obs=prev_obs, prev_act=prev_act)\n",
        "            for key, pred in [('A', pred_a), ('B', pred_b)]:\n",
        "                mse = F.mse_loss(pred, current_img).item()\n",
        "                pred_norm = (pred.clamp(-1,1)+1)/2\n",
        "                gt_norm = (current_img.clamp(-1,1)+1)/2\n",
        "                ssim_val = ssim(pred_norm, gt_norm, data_range=1.0).item()\n",
        "                metrics[key]['mse'].append(mse)\n",
        "                metrics[key]['ssim'].append(ssim_val)\n",
        "                avg_mse = float(np.mean(metrics[key]['mse']))\n",
        "                avg_ssim = float(np.mean(metrics[key]['ssim']))\n",
        "                print(f'Sample {idx} Model {key} -> MSE: {mse:.4f}, SSIM: {ssim_val:.4f}, Avg MSE: {avg_mse:.4f}, Avg SSIM: {avg_ssim:.4f}')\n",
        "                wandb.log({f\"{key}/mse\": mse, f\"{key}/ssim\": ssim_val, f\"{key}/avg_mse\": avg_mse, f\"{key}/avg_ssim\": avg_ssim, \"sample_idx\": idx})\n",
        "    results = {}\n",
        "    for key, vals in metrics.items():\n",
        "        if vals['mse']:\n",
        "            results[key] = {\n",
        "                'avg_mse': float(np.mean(vals['mse'])),\n",
        "                'avg_ssim': float(np.mean(vals['ssim'])),\n",
        "                'count': len(vals['mse'])\n",
        "            }\n",
        "        else:\n",
        "            results[key] = {'avg_mse': float('nan'), 'avg_ssim': float('nan'), 'count': 0}\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "device = config.DEVICE\n",
        "sampler_a = load_sampler(MODEL_A_PATH, device)\n",
        "sampler_b = load_sampler(MODEL_B_PATH, device)\n",
        "results = evaluate_models_alternating(sampler_a, sampler_b, dataloader, device, config.NUM_PREV_FRAMES)\n",
        "print('Model A Overall:', results.get('A'))\n",
        "print('Model B Overall:', results.get('B'))\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
