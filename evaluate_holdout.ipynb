{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "from importnb import Notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "import models\n",
    "\n",
    "with Notebook():\n",
    "    from jetbot_dataset import JetbotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sampler(checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    inner_cfg = models.InnerModelConfig(\n",
    "        img_channels=config.DM_IMG_CHANNELS,\n",
    "        num_steps_conditioning=config.DM_NUM_STEPS_CONDITIONING,\n",
    "        cond_channels=config.DM_COND_CHANNELS,\n",
    "        depths=config.DM_UNET_DEPTHS,\n",
    "        channels=config.DM_UNET_CHANNELS,\n",
    "        attn_depths=config.DM_UNET_ATTN_DEPTHS,\n",
    "        num_actions=config.DM_NUM_ACTIONS,\n",
    "        is_upsampler=config.DM_IS_UPSAMPLER,\n",
    "    )\n",
    "    denoiser_cfg = models.DenoiserConfig(\n",
    "        inner_model=inner_cfg,\n",
    "        sigma_data=config.DM_SIGMA_DATA,\n",
    "        sigma_offset_noise=config.DM_SIGMA_OFFSET_NOISE,\n",
    "        noise_previous_obs=config.DM_NOISE_PREVIOUS_OBS,\n",
    "        upsampling_factor=config.DM_UPSAMPLING_FACTOR,\n",
    "    )\n",
    "    denoiser = models.Denoiser(cfg=denoiser_cfg).to(device)\n",
    "    denoiser.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    denoiser.eval()\n",
    "    sampler_cfg = models.DiffusionSamplerConfig(\n",
    "        num_steps_denoising=config.SAMPLER_NUM_STEPS,\n",
    "        sigma_min=config.SAMPLER_SIGMA_MIN,\n",
    "        sigma_max=config.SAMPLER_SIGMA_MAX,\n",
    "        rho=config.SAMPLER_RHO,\n",
    "        order=getattr(config, \"SAMPLER_ORDER\", 1),\n",
    "        s_churn=getattr(config, \"SAMPLER_S_CHURN\", 0.0),\n",
    "    )\n",
    "    return models.DiffusionSampler(denoiser=denoiser, cfg=sampler_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "\n",
    "def tensor_to_pil(tensor_img):\n",
    "    tensor_img = (tensor_img.clamp(-1, 1) + 1) / 2\n",
    "    tensor_img = tensor_img.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    if tensor_img.shape[2] == 1:\n",
    "        tensor_img = tensor_img.squeeze(2)\n",
    "    if not tensor_img.flags.writeable:\n",
    "        tensor_img = np.ascontiguousarray(tensor_img)\n",
    "    if tensor_img.dtype != np.uint8:\n",
    "        pil_img_array = (tensor_img * 255).astype(np.uint8)\n",
    "    else:\n",
    "        pil_img_array = tensor_img\n",
    "    return PILImage.fromarray(pil_img_array)\n",
    "\n",
    "def save_visualization_samples(gen, gt_current, gt_prev, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    num_prev = config.NUM_PREV_FRAMES\n",
    "    fig, axs = plt.subplots(2, num_prev + 1, figsize=((num_prev + 1) * 3, 6))\n",
    "    for i in range(num_prev):\n",
    "        axs[0, i].imshow(tensor_to_pil(gt_prev[i]))\n",
    "        axs[0, i].set_title(f'GT Prev {i+1}')\n",
    "        axs[0, i].axis('off')\n",
    "        axs[1, i].axis('off')\n",
    "    axs[0, num_prev].imshow(tensor_to_pil(gt_current))\n",
    "    axs[0, num_prev].set_title('GT Current')\n",
    "    axs[0, num_prev].axis('off')\n",
    "    axs[1, num_prev].imshow(tensor_to_pil(gen))\n",
    "    axs[1, num_prev].set_title('Generated')\n",
    "    axs[1, num_prev].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sampler(sampler, dataloader, device, num_prev_frames, tol=1e-6):\n",
    "    sampler.denoiser.eval()\n",
    "    metrics = {'overall': {'mse': [], 'ssim': []}}\n",
    "    examples = {'still_best': None, 'still_worst': None, 'move_best': None, 'move_worst': None}\n",
    "    with torch.no_grad():\n",
    "        for current_img, action_tensor, prev_frames_tensor in dataloader:\n",
    "            current_img = current_img.to(device)\n",
    "            action_tensor = action_tensor.to(device)\n",
    "            prev_frames_tensor = prev_frames_tensor.to(device)\n",
    "            prev_obs = prev_frames_tensor.view(1, num_prev_frames, current_img.shape[1], current_img.shape[2], current_img.shape[3])\n",
    "            prev_act = action_tensor.long().repeat(1, num_prev_frames)\n",
    "            pred, _ = sampler.sample(prev_obs=prev_obs, prev_act=prev_act)\n",
    "            mse = F.mse_loss(pred, current_img).item()\n",
    "            pred_n = (pred.clamp(-1,1)+1)/2\n",
    "            gt_n = (current_img.clamp(-1,1)+1)/2\n",
    "            ssim_val = ssim(pred_n, gt_n, data_range=1.0).item()\n",
    "            metrics['overall']['mse'].append(mse)\n",
    "            metrics['overall']['ssim'].append(ssim_val)\n",
    "            key = 'move' if abs(action_tensor.item()) > tol else 'still'\n",
    "            info = {'ssim': ssim_val, 'mse': mse, 'pred': pred.squeeze(0).cpu(), 'gt': current_img.squeeze(0).cpu(), 'prev': prev_frames_tensor.squeeze(0).view(num_prev_frames, pred.shape[1], pred.shape[2], pred.shape[3]).cpu()}\n",
    "            if examples[key+'_best'] is None or ssim_val > examples[key+'_best']['ssim']:\n",
    "                examples[key+'_best'] = info\n",
    "            if examples[key+'_worst'] is None or ssim_val < examples[key+'_worst']['ssim']:\n",
    "                examples[key+'_worst'] = info\n",
    "    avg_metrics = {k: {'avg_mse': float(np.mean(v['mse'])) if v['mse'] else float('nan'), 'avg_ssim': float(np.mean(v['ssim'])) if v['ssim'] else float('nan'), 'count': len(v['mse'])} for k,v in metrics.items()}\n",
    "    return avg_metrics, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JetbotDataset(config.HOLDOUT_CSV_PATH, config.HOLDOUT_DATA_DIR, config.IMAGE_SIZE, config.NUM_PREV_FRAMES, transform=config.TRANSFORM)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "checkpoint = os.path.join(config.CHECKPOINT_DIR, 'denoiser_model_best_val_loss.pth')\n",
    "sampler = load_sampler(checkpoint, config.DEVICE)\n",
    "metrics, examples = evaluate_sampler(sampler, dataloader, config.DEVICE, config.NUM_PREV_FRAMES)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(config.OUTPUT_DIR, 'holdout_examples')\n",
    "if examples['still_best']:\n",
    "    save_visualization_samples(examples['still_best']['pred'], examples['still_best']['gt'], examples['still_best']['prev'], os.path.join(save_dir, 'still_best.png'))\n",
    "if examples['still_worst']:\n",
    "    save_visualization_samples(examples['still_worst']['pred'], examples['still_worst']['gt'], examples['still_worst']['prev'], os.path.join(save_dir, 'still_worst.png'))\n",
    "if examples['move_best']:\n",
    "    save_visualization_samples(examples['move_best']['pred'], examples['move_best']['gt'], examples['move_best']['prev'], os.path.join(save_dir, 'move_best.png'))\n",
    "if examples['move_worst']:\n",
    "    save_visualization_samples(examples['move_worst']['pred'], examples['move_worst']['gt'], examples['move_worst']['prev'], os.path.join(save_dir, 'move_worst.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
